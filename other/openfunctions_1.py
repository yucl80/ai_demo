import pprint
from llama_cpp import Llama
import json
import uuid
from openfunctions_utils import strip_function_calls, parse_function_call
from transformers import AutoTokenizer


tokenizer = AutoTokenizer.from_pretrained(
    'gorilla-llm/gorilla-openfunctions-v2', use_fast=True
)

print(tokenizer.chat_template)


def get_prompt(user_query: str, functions: list = []) -> str:
    """
    Generates a conversation prompt based on the user's query and a list of functions.

    Parameters:
    - user_query (str): The user's query.
    - functions (list): A list of functions to include in the prompt.

    Returns:
    - str: The formatted conversation prompt.
    """
    system = "You are an AI programming assistant, utilizing the Gorilla LLM model, developed by Gorilla LLM, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer."
    if len(functions) == 0:
        return f"{system}\n### Instruction: <<question>> {user_query}\n### Response: "
    functions_string = json.dumps(functions)
    return f"{system}\n### Instruction: <<function>>{functions_string}\n<<question>>{user_query}\n### Response: "

def get_prompt2(user_query: str, functions: list) -> str:
    """
    Generates a conversation prompt based on the user's query and a list of functions.

    Parameters:
    - user_query (str): The user's query.
    - functions (list): A list of functions to include in the prompt.

    Returns:
    - str: The formatted conversation prompt.
    """
    if len(functions) == 0:
        return f"USER: <<question>> {user_query}\nASSISTANT: "
    functions_string = json.dumps(functions)
    return f"USER: <<question>> {user_query} <<function>> {functions_string}\nASSISTANT: "

def format_response(response: str):
    """
    Formats the response from the OpenFunctions model.

    Parameters:
    - response (str): The response generated by the LLM.

    Returns:
    - str: The formatted response.
    - dict: The function call(s) extracted from the response.

    """
    function_call_dicts = None
    try:
        response = strip_function_calls(response)
        # Parallel function calls returned as a str, list[dict]
        if len(response) > 1:
            function_call_dicts = []
            for function_call in response:
                function_call_dicts.append(
                    {"id": "call_"+uuid.uuid4().hex,
                     "function": parse_function_call(function_call),
                     "type": "function"})

            response = ", ".join(response)
        # Single function call returned as a str, dict
        else:
            function_call_dicts = []
            function_call_dicts.append(
                {"id": "call_"+uuid.uuid4().hex,
                 "function": parse_function_call(response[0]),
                 "type": "function"})
            response = response[0]
    except Exception as e:
        # Just faithfully return the generated response str to the user
        print(e)
        pass
    return response, function_call_dicts


def format_response2(response: str):
    """
    Formats the response from the OpenFunctions model.

    Parameters:
    - response (str): The response generated by the LLM.

    Returns:
    - str: The formatted response.
    - dict: The function call(s) extracted from the response.

    """
    function_call_dicts = None
    try:
        response = strip_function_calls(response)
        # Parallel function calls returned as a str, list[dict]
        if len(response) > 1:
            function_call_dicts = []
            for function_call in response:
                function_call_dicts.append(parse_function_call(function_call))
            response = ", ".join(response)
        # Single function call returned as a str, dict
        else:
            function_call_dicts = parse_function_call(response[0])
            response = response[0]
    except Exception as e:
        # Just faithfully return the generated response str to the user
        pass
    return response, function_call_dicts


query = "What's the weather like in the two cities of Boston and San Francisco?"
functions = [
    {
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "The city and state, e.g. San Francisco, CA",
                },
                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
            },
            "required": ["location"],
        },
    }
]



llm = Llama(model_path="/home/test/llm-models/gorilla-openfunctions-v2-q4_K_M.gguf",
            n_threads=12, n_gpu_layers=0,  verbose=False,)

import time
start_time = time.time()

user_prompt = get_prompt(query, functions)
output = llm(user_prompt,
             max_tokens=512,  # Generate up to 512 tokens
             stop=["<|EOT|>"],
             echo=False,       # Whether to echo the prompt
             stream=False
             )
end_time = time.time()
print("--- %s seconds ---" % (end_time - start_time))


start_time = time.time()

user_prompt = get_prompt2(query, functions)
output = llm(user_prompt,
             max_tokens=512,  # Generate up to 512 tokens
             stop=["<|EOT|>"],
             echo=False,       # Whether to echo the prompt
             stream=False
             )
end_time = time.time()
print("--- %s seconds ---" % (end_time - start_time))

# for m in output:
#     print(m["choices"][0]["text"],end="")

pprint.pprint(output)

# fn_call_string, function_call_dict = format_response(
#     output["choices"][0]["text"])
# print("--------------------")
# print(f"Function call strings 1(s): {fn_call_string}")
# print("--------------------")
# print(f"OpenAI compatible `function_call`: {function_call_dict}")
# print("--------------------")
