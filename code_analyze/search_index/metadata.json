{"snippets": [{"content": "from abc import ABC, abstractmethod\nimport asyncio\nimport json\nfrom typing import Any, Dict\n\n# \u5728EnhancedLLMApi\u7c7b\u4e2d\u66f4\u65b0\u65b9\u6cd5\n\nclass ExpertModel(ABC):\n    def __init__(self, api_key: str, model: str = \"gpt-3.5-turbo\"):\n        self.api_key = api_key\n        self.model = model\n\n    @abstractmethod\n    async def analyze(self, code: str, context: Dict[str, Any]) -> str:\n        pass", "file_path": "analyze_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from abc import ABC, abstractmethod\nimport asyncio\nimport json\n", "symbols": ["ArchitectureExpert", "str", "api_key", "preprocess_data", "code_summary", "context", "train_model", "ExpertModel", "__name__", "split_data", "enhanced_context", "_run_expert_analyses", "plot_results", "indent", "r2", "y", "enhanced_analysis", "df", "load_data", "final_analysis", "integration_prompt", "code", "_integrate_analyses", "CodeQualityExpert", "analyze", "main", "prompt", "large_file_path", "drop_first", "random_state", "llm_api", "y_pred", "summary", "X", "__init__", "PerformanceExpert", "EnhancedLLMApi", "mse", "basic_analysis", "model", "analysis", "analysis_result", "CodeSummaryExpert", "SecurityExpert", "_run_expert_analysis", "TestingExpert", "evaluate_model", "main2", "results", "expert_analyses", "experts", "test_size", "y_test", "code_to_analyze", "inplace", "visualization", "tasks", "axis"]}, {"content": "class ArchitectureExpert(ExpertModel):\n    async def analyze(self, code: str, context: Dict[str, Any]) -> str:\n        prompt = \"\"\"\n        As an architecture expert, analyze the given code and context. Focus on:\n        1. Overall code structure and design patterns\n        2. Modularity and component interactions\n        3. Scalability and maintainability of the architecture\n        4. Suggestions for architectural improvements\n\n        Code:\n        {code}\n\n        Context:\n        {context}\n\n        Provide a detailed analysis of the code's architecture.\n        \"\"\"\n        return await self._call_openai_api(prompt.format(code=code, context=json.dumps(context)))", "file_path": "analyze_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from abc import ABC, abstractmethod\nimport asyncio\nimport json\n", "symbols": ["ArchitectureExpert", "str", "api_key", "preprocess_data", "code_summary", "context", "train_model", "ExpertModel", "__name__", "split_data", "enhanced_context", "_run_expert_analyses", "plot_results", "indent", "r2", "y", "enhanced_analysis", "df", "load_data", "final_analysis", "integration_prompt", "code", "_integrate_analyses", "CodeQualityExpert", "analyze", "main", "prompt", "large_file_path", "drop_first", "random_state", "llm_api", "y_pred", "summary", "X", "__init__", "PerformanceExpert", "EnhancedLLMApi", "mse", "basic_analysis", "model", "analysis", "analysis_result", "CodeSummaryExpert", "SecurityExpert", "_run_expert_analysis", "TestingExpert", "evaluate_model", "main2", "results", "expert_analyses", "experts", "test_size", "y_test", "code_to_analyze", "inplace", "visualization", "tasks", "axis"]}, {"content": "class PerformanceExpert(ExpertModel):\n    async def analyze(self, code: str, context: Dict[str, Any]) -> str:\n        prompt = \"\"\"\n        As a performance expert, analyze the given code and context. Focus on:\n        1. Algorithmic efficiency\n        2. Resource usage (CPU, memory, I/O)\n        3. Potential bottlenecks and performance hotspots\n        4. Optimization suggestions\n\n        Code:\n        {code}\n\n        Context:\n        {context}\n\n        Provide a detailed analysis of the code's performance characteristics.\n        \"\"\"\n        return await self._call_openai_api(prompt.format(code=code, context=json.dumps(context)))", "file_path": "analyze_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from abc import ABC, abstractmethod\nimport asyncio\nimport json\n", "symbols": ["ArchitectureExpert", "str", "api_key", "preprocess_data", "code_summary", "context", "train_model", "ExpertModel", "__name__", "split_data", "enhanced_context", "_run_expert_analyses", "plot_results", "indent", "r2", "y", "enhanced_analysis", "df", "load_data", "final_analysis", "integration_prompt", "code", "_integrate_analyses", "CodeQualityExpert", "analyze", "main", "prompt", "large_file_path", "drop_first", "random_state", "llm_api", "y_pred", "summary", "X", "__init__", "PerformanceExpert", "EnhancedLLMApi", "mse", "basic_analysis", "model", "analysis", "analysis_result", "CodeSummaryExpert", "SecurityExpert", "_run_expert_analysis", "TestingExpert", "evaluate_model", "main2", "results", "expert_analyses", "experts", "test_size", "y_test", "code_to_analyze", "inplace", "visualization", "tasks", "axis"]}, {"content": "class SecurityExpert(ExpertModel):\n    async def analyze(self, code: str, context: Dict[str, Any]) -> str:\n        prompt = \"\"\"\n        As a security expert, analyze the given code and context. Focus on:\n        1. Potential security vulnerabilities\n        2. Adherence to security best practices\n        3. Data handling and privacy concerns\n        4. Recommendations for security improvements\n\n        Code:\n        {code}\n\n        Context:\n        {context}\n\n        Provide a detailed security analysis of the code.\n        \"\"\"\n        return await self._call_openai_api(prompt.format(code=code, context=json.dumps(context)))", "file_path": "analyze_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from abc import ABC, abstractmethod\nimport asyncio\nimport json\n", "symbols": ["ArchitectureExpert", "str", "api_key", "preprocess_data", "code_summary", "context", "train_model", "ExpertModel", "__name__", "split_data", "enhanced_context", "_run_expert_analyses", "plot_results", "indent", "r2", "y", "enhanced_analysis", "df", "load_data", "final_analysis", "integration_prompt", "code", "_integrate_analyses", "CodeQualityExpert", "analyze", "main", "prompt", "large_file_path", "drop_first", "random_state", "llm_api", "y_pred", "summary", "X", "__init__", "PerformanceExpert", "EnhancedLLMApi", "mse", "basic_analysis", "model", "analysis", "analysis_result", "CodeSummaryExpert", "SecurityExpert", "_run_expert_analysis", "TestingExpert", "evaluate_model", "main2", "results", "expert_analyses", "experts", "test_size", "y_test", "code_to_analyze", "inplace", "visualization", "tasks", "axis"]}, {"content": "class CodeQualityExpert(ExpertModel):\n    async def analyze(self, code: str, context: Dict[str, Any]) -> str:\n        prompt = \"\"\"\n        As a code quality expert, analyze the given code and context. Focus on:\n        1. Adherence to coding standards and best practices\n        2. Code readability and maintainability\n        3. Proper use of comments and documentation\n        4. Suggestions for improving code quality\n\n        Code:\n        {code}\n\n        Context:\n        {context}\n\n        Provide a detailed analysis of the code's quality.\n        \"\"\"\n        return await self._call_openai_api(prompt.format(code=code, context=json.dumps(context)))", "file_path": "analyze_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from abc import ABC, abstractmethod\nimport asyncio\nimport json\n", "symbols": ["ArchitectureExpert", "str", "api_key", "preprocess_data", "code_summary", "context", "train_model", "ExpertModel", "__name__", "split_data", "enhanced_context", "_run_expert_analyses", "plot_results", "indent", "r2", "y", "enhanced_analysis", "df", "load_data", "final_analysis", "integration_prompt", "code", "_integrate_analyses", "CodeQualityExpert", "analyze", "main", "prompt", "large_file_path", "drop_first", "random_state", "llm_api", "y_pred", "summary", "X", "__init__", "PerformanceExpert", "EnhancedLLMApi", "mse", "basic_analysis", "model", "analysis", "analysis_result", "CodeSummaryExpert", "SecurityExpert", "_run_expert_analysis", "TestingExpert", "evaluate_model", "main2", "results", "expert_analyses", "experts", "test_size", "y_test", "code_to_analyze", "inplace", "visualization", "tasks", "axis"]}, {"content": "class TestingExpert(ExpertModel):\n    async def analyze(self, code: str, context: Dict[str, Any]) -> str:\n        prompt = \"\"\"\n        As a testing expert, analyze the given code and context. Focus on:\n        1. Test coverage and quality\n        2. Potential edge cases and error scenarios\n        3. Testability of the code\n        4. Suggestions for improving test suite\n\n        Code:\n        {code}\n\n        Context:\n        {context}\n\n        Provide a detailed analysis of the code's testing aspects.\n        \"\"\"\n        return await self._call_openai_api(prompt.format(code=code, context=json.dumps(context)))", "file_path": "analyze_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from abc import ABC, abstractmethod\nimport asyncio\nimport json\n", "symbols": ["ArchitectureExpert", "str", "api_key", "preprocess_data", "code_summary", "context", "train_model", "ExpertModel", "__name__", "split_data", "enhanced_context", "_run_expert_analyses", "plot_results", "indent", "r2", "y", "enhanced_analysis", "df", "load_data", "final_analysis", "integration_prompt", "code", "_integrate_analyses", "CodeQualityExpert", "analyze", "main", "prompt", "large_file_path", "drop_first", "random_state", "llm_api", "y_pred", "summary", "X", "__init__", "PerformanceExpert", "EnhancedLLMApi", "mse", "basic_analysis", "model", "analysis", "analysis_result", "CodeSummaryExpert", "SecurityExpert", "_run_expert_analysis", "TestingExpert", "evaluate_model", "main2", "results", "expert_analyses", "experts", "test_size", "y_test", "code_to_analyze", "inplace", "visualization", "tasks", "axis"]}, {"content": "class CodeSummaryExpert(ExpertModel):\n    async def analyze(self, code: str, context: Dict[str, Any]) -> str:\n        prompt = \"\"\"\n        As a code summarization expert, analyze the given code and context. Your task is to generate a high-level summary of the code that captures its main functionality, structure, and purpose. Focus on:\n\n        1. The overall purpose of the code\n        2. Key components or modules and their roles\n        3. Main algorithms or processes implemented\n        4. Important data structures used\n        5. External dependencies and their purposes\n        6. Any notable design patterns or architectural choices\n\n        Provide a concise yet informative summary that would help a developer quickly understand the essence of this code without delving into every detail.\n\n        Code:\n        {code}\n\n        Context:\n        {context}", "file_path": "analyze_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from abc import ABC, abstractmethod\nimport asyncio\nimport json\n", "symbols": ["ArchitectureExpert", "str", "api_key", "preprocess_data", "code_summary", "context", "train_model", "ExpertModel", "__name__", "split_data", "enhanced_context", "_run_expert_analyses", "plot_results", "indent", "r2", "y", "enhanced_analysis", "df", "load_data", "final_analysis", "integration_prompt", "code", "_integrate_analyses", "CodeQualityExpert", "analyze", "main", "prompt", "large_file_path", "drop_first", "random_state", "llm_api", "y_pred", "summary", "X", "__init__", "PerformanceExpert", "EnhancedLLMApi", "mse", "basic_analysis", "model", "analysis", "analysis_result", "CodeSummaryExpert", "SecurityExpert", "_run_expert_analysis", "TestingExpert", "evaluate_model", "main2", "results", "expert_analyses", "experts", "test_size", "y_test", "code_to_analyze", "inplace", "visualization", "tasks", "axis"]}, {"content": "Code:\n        {code}\n\n        Context:\n        {context}\n\n        Generate a comprehensive summary of the code in about 200-300 words.\n        \"\"\"\n        return await self._call_openai_api(prompt.format(code=code, context=json.dumps(context)))", "file_path": "analyze_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from abc import ABC, abstractmethod\nimport asyncio\nimport json\n", "symbols": ["ArchitectureExpert", "str", "api_key", "preprocess_data", "code_summary", "context", "train_model", "ExpertModel", "__name__", "split_data", "enhanced_context", "_run_expert_analyses", "plot_results", "indent", "r2", "y", "enhanced_analysis", "df", "load_data", "final_analysis", "integration_prompt", "code", "_integrate_analyses", "CodeQualityExpert", "analyze", "main", "prompt", "large_file_path", "drop_first", "random_state", "llm_api", "y_pred", "summary", "X", "__init__", "PerformanceExpert", "EnhancedLLMApi", "mse", "basic_analysis", "model", "analysis", "analysis_result", "CodeSummaryExpert", "SecurityExpert", "_run_expert_analysis", "TestingExpert", "evaluate_model", "main2", "results", "expert_analyses", "experts", "test_size", "y_test", "code_to_analyze", "inplace", "visualization", "tasks", "axis"]}, {"content": "class EnhancedLLMApi(LLMApi):\n    def __init__(self, api_key: str = None, model: str = \"gpt-3.5-turbo\"):\n        super().__init__(api_key, model)\n        self.experts = {\n            \"architecture\": ArchitectureExpert(api_key),\n            \"performance\": PerformanceExpert(api_key),\n            \"security\": SecurityExpert(api_key),\n            \"code_quality\": CodeQualityExpert(api_key),\n            \"testing\": TestingExpert(api_key),\n            \"code_summary\": CodeSummaryExpert(api_key)  # \u6dfb\u52a0\u65b0\u7684\u4e13\u5bb6\u6a21\u578b\n        }\n        # ... (\u5176\u4ed6\u521d\u59cb\u5316\u4ee3\u7801\u4fdd\u6301\u4e0d\u53d8)", "file_path": "analyze_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from abc import ABC, abstractmethod\nimport asyncio\nimport json\n", "symbols": ["ArchitectureExpert", "str", "api_key", "preprocess_data", "code_summary", "context", "train_model", "ExpertModel", "__name__", "split_data", "enhanced_context", "_run_expert_analyses", "plot_results", "indent", "r2", "y", "enhanced_analysis", "df", "load_data", "final_analysis", "integration_prompt", "code", "_integrate_analyses", "CodeQualityExpert", "analyze", "main", "prompt", "large_file_path", "drop_first", "random_state", "llm_api", "y_pred", "summary", "X", "__init__", "PerformanceExpert", "EnhancedLLMApi", "mse", "basic_analysis", "model", "analysis", "analysis_result", "CodeSummaryExpert", "SecurityExpert", "_run_expert_analysis", "TestingExpert", "evaluate_model", "main2", "results", "expert_analyses", "experts", "test_size", "y_test", "code_to_analyze", "inplace", "visualization", "tasks", "axis"]}, {"content": "async def enhanced_analysis(self, code: str, repo_path: str = None) -> Dict[str, Any]:\n        basic_analysis = await self.analyze_with_global_context(code)\n        \n        enhanced_context = {\n            \"ast_analysis\": self.ast_analyzer.analyze(code),\n            \"git_analysis\": self.git_analyzer.analyze(repo_path) if repo_path else None,\n            \"static_analysis\": self.static_analyzer.analyze(code),\n            \"test_analysis\": self.test_analyzer.analyze(repo_path) if repo_path else None,\n            \"dependency_analysis\": self.dependency_analyzer.analyze(repo_path) if repo_path else None,\n            \"pattern_analysis\": self.pattern_recognizer.recognize(code),\n            \"performance_analysis\": self.performance_analyzer.analyze(code),\n            \"security_analysis\": self.security_scanner.scan(code),\n            \"domain_insights\": self.domain_knowledge.get_insights(code),\n            \"comments_analysis\": self.comment_extractor.extract(code),", "file_path": "analyze_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from abc import ABC, abstractmethod\nimport asyncio\nimport json\n", "symbols": ["ArchitectureExpert", "str", "api_key", "preprocess_data", "code_summary", "context", "train_model", "ExpertModel", "__name__", "split_data", "enhanced_context", "_run_expert_analyses", "plot_results", "indent", "r2", "y", "enhanced_analysis", "df", "load_data", "final_analysis", "integration_prompt", "code", "_integrate_analyses", "CodeQualityExpert", "analyze", "main", "prompt", "large_file_path", "drop_first", "random_state", "llm_api", "y_pred", "summary", "X", "__init__", "PerformanceExpert", "EnhancedLLMApi", "mse", "basic_analysis", "model", "analysis", "analysis_result", "CodeSummaryExpert", "SecurityExpert", "_run_expert_analysis", "TestingExpert", "evaluate_model", "main2", "results", "expert_analyses", "experts", "test_size", "y_test", "code_to_analyze", "inplace", "visualization", "tasks", "axis"]}, {"content": "\"domain_insights\": self.domain_knowledge.get_insights(code),\n            \"comments_analysis\": self.comment_extractor.extract(code),\n            \"metaprogramming_analysis\": self.metaprogramming_analyzer.analyze(code),\n            \"environment_analysis\": self.environment_analyzer.analyze(repo_path) if repo_path else None\n        }", "file_path": "analyze_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from abc import ABC, abstractmethod\nimport asyncio\nimport json\n", "symbols": ["ArchitectureExpert", "str", "api_key", "preprocess_data", "code_summary", "context", "train_model", "ExpertModel", "__name__", "split_data", "enhanced_context", "_run_expert_analyses", "plot_results", "indent", "r2", "y", "enhanced_analysis", "df", "load_data", "final_analysis", "integration_prompt", "code", "_integrate_analyses", "CodeQualityExpert", "analyze", "main", "prompt", "large_file_path", "drop_first", "random_state", "llm_api", "y_pred", "summary", "X", "__init__", "PerformanceExpert", "EnhancedLLMApi", "mse", "basic_analysis", "model", "analysis", "analysis_result", "CodeSummaryExpert", "SecurityExpert", "_run_expert_analysis", "TestingExpert", "evaluate_model", "main2", "results", "expert_analyses", "experts", "test_size", "y_test", "code_to_analyze", "inplace", "visualization", "tasks", "axis"]}, {"content": "# \u4f7f\u7528\u4e13\u5bb6\u6a21\u578b\u8fdb\u884c\u5206\u6790\n        expert_analyses = await self._run_expert_analyses(code, enhanced_context)\n        enhanced_context.update(expert_analyses)\n        \n        # \u751f\u6210\u4ee3\u7801\u6458\u8981\n        code_summary = expert_analyses['code_summary_analysis']\n\n        # \u4f7f\u7528\u4e3b\u6a21\u578b\u6574\u5408\u6240\u6709\u5206\u6790\u7ed3\u679c\n        final_analysis = await self._integrate_analyses(enhanced_context)\n\n        visualization = self.generate_visualization(enhanced_context)\n\n        return {\n            **basic_analysis, \n            \"enhanced_analysis\": final_analysis,\n            \"expert_analyses\": expert_analyses,\n            \"code_summary\": code_summary,  # \u6dfb\u52a0\u4ee3\u7801\u6458\u8981\u5230\u8fd4\u56de\u7ed3\u679c\n            \"visualization\": visualization\n        }", "file_path": "analyze_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from abc import ABC, abstractmethod\nimport asyncio\nimport json\n", "symbols": ["ArchitectureExpert", "str", "api_key", "preprocess_data", "code_summary", "context", "train_model", "ExpertModel", "__name__", "split_data", "enhanced_context", "_run_expert_analyses", "plot_results", "indent", "r2", "y", "enhanced_analysis", "df", "load_data", "final_analysis", "integration_prompt", "code", "_integrate_analyses", "CodeQualityExpert", "analyze", "main", "prompt", "large_file_path", "drop_first", "random_state", "llm_api", "y_pred", "summary", "X", "__init__", "PerformanceExpert", "EnhancedLLMApi", "mse", "basic_analysis", "model", "analysis", "analysis_result", "CodeSummaryExpert", "SecurityExpert", "_run_expert_analysis", "TestingExpert", "evaluate_model", "main2", "results", "expert_analyses", "experts", "test_size", "y_test", "code_to_analyze", "inplace", "visualization", "tasks", "axis"]}, {"content": "async def _run_expert_analyses(self, code: str, context: Dict[str, Any]) -> Dict[str, str]:\n        expert_analyses = {}\n        tasks = []\n        for expert_name, expert in self.experts.items():\n            tasks.append(self._run_expert_analysis(expert_name, expert, code, context))\n        results = await asyncio.gather(*tasks)\n        for expert_name, analysis in results:\n            expert_analyses[f\"{expert_name}_analysis\"] = analysis\n        return expert_analyses\n\n    async def _run_expert_analysis(self, expert_name: str, expert: ExpertModel, code: str, context: Dict[str, Any]) -> Tuple[str, str]:\n        analysis = await expert.analyze(code, context)\n        return expert_name, analysis\n\n    async def _integrate_analyses(self, context: Dict[str, Any]) -> str:\n        integration_prompt = \"\"\"\n        As a senior software architect and code analyst, review and integrate the following analyses of a codebase:\n\n        {context}", "file_path": "analyze_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from abc import ABC, abstractmethod\nimport asyncio\nimport json\n", "symbols": ["ArchitectureExpert", "str", "api_key", "preprocess_data", "code_summary", "context", "train_model", "ExpertModel", "__name__", "split_data", "enhanced_context", "_run_expert_analyses", "plot_results", "indent", "r2", "y", "enhanced_analysis", "df", "load_data", "final_analysis", "integration_prompt", "code", "_integrate_analyses", "CodeQualityExpert", "analyze", "main", "prompt", "large_file_path", "drop_first", "random_state", "llm_api", "y_pred", "summary", "X", "__init__", "PerformanceExpert", "EnhancedLLMApi", "mse", "basic_analysis", "model", "analysis", "analysis_result", "CodeSummaryExpert", "SecurityExpert", "_run_expert_analysis", "TestingExpert", "evaluate_model", "main2", "results", "expert_analyses", "experts", "test_size", "y_test", "code_to_analyze", "inplace", "visualization", "tasks", "axis"]}, {"content": "{context}\n\n        Provide a comprehensive, high-level summary of the codebase, addressing:\n        1. Overall architecture and design\n        2. Code quality and maintainability\n        3. Performance characteristics\n        4. Security considerations\n        5. Testing and reliability\n        6. Areas for improvement and recommendations\n        \n        Also, consider the provided code summary and how it relates to the detailed analyses.\n\n        Your summary should synthesize insights from all the expert analyses and provide a holistic view of the codebase.\n        \"\"\"\n        return await self.analyze(\"\", integration_prompt.format(context=json.dumps(context)))", "file_path": "analyze_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from abc import ABC, abstractmethod\nimport asyncio\nimport json\n", "symbols": ["ArchitectureExpert", "str", "api_key", "preprocess_data", "code_summary", "context", "train_model", "ExpertModel", "__name__", "split_data", "enhanced_context", "_run_expert_analyses", "plot_results", "indent", "r2", "y", "enhanced_analysis", "df", "load_data", "final_analysis", "integration_prompt", "code", "_integrate_analyses", "CodeQualityExpert", "analyze", "main", "prompt", "large_file_path", "drop_first", "random_state", "llm_api", "y_pred", "summary", "X", "__init__", "PerformanceExpert", "EnhancedLLMApi", "mse", "basic_analysis", "model", "analysis", "analysis_result", "CodeSummaryExpert", "SecurityExpert", "_run_expert_analysis", "TestingExpert", "evaluate_model", "main2", "results", "expert_analyses", "experts", "test_size", "y_test", "code_to_analyze", "inplace", "visualization", "tasks", "axis"]}, {"content": "# ... (\u5176\u4ed6\u65b9\u6cd5\u4fdd\u6301\u4e0d\u53d8)\nasync def main():\n    api_key = \"your-api-key-here\"\n    llm_api = EnhancedLLMApi(api_key)\n    \n    code_to_analyze = \"\"\"\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from sklearn.model_selection import train_test_split\n    from sklearn.linear_model import LinearRegression\n    from sklearn.metrics import mean_squared_error, r2_score\n\n    def load_data(file_path):\n        return pd.read_csv(file_path)\n\n    def preprocess_data(df):\n        # Handle missing values\n        df.dropna(inplace=True)\n        # Convert categorical variables to numeric\n        df = pd.get_dummies(df, drop_first=True)\n        return df\n\n    def split_data(X, y, test_size=0.2, random_state=42):\n        return train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    def train_model(X_train, y_train):\n        model = LinearRegression()\n        model.fit(X_train, y_train)\n        return model", "file_path": "analyze_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from abc import ABC, abstractmethod\nimport asyncio\nimport json\n", "symbols": ["ArchitectureExpert", "str", "api_key", "preprocess_data", "code_summary", "context", "train_model", "ExpertModel", "__name__", "split_data", "enhanced_context", "_run_expert_analyses", "plot_results", "indent", "r2", "y", "enhanced_analysis", "df", "load_data", "final_analysis", "integration_prompt", "code", "_integrate_analyses", "CodeQualityExpert", "analyze", "main", "prompt", "large_file_path", "drop_first", "random_state", "llm_api", "y_pred", "summary", "X", "__init__", "PerformanceExpert", "EnhancedLLMApi", "mse", "basic_analysis", "model", "analysis", "analysis_result", "CodeSummaryExpert", "SecurityExpert", "_run_expert_analysis", "TestingExpert", "evaluate_model", "main2", "results", "expert_analyses", "experts", "test_size", "y_test", "code_to_analyze", "inplace", "visualization", "tasks", "axis"]}, {"content": "def train_model(X_train, y_train):\n        model = LinearRegression()\n        model.fit(X_train, y_train)\n        return model\n\n    def evaluate_model(model, X_test, y_test):\n        y_pred = model.predict(X_test)\n        mse = mean_squared_error(y_test, y_pred)\n        r2 = r2_score(y_test, y_pred)\n        return mse, r2\n\n    def plot_results(y_test, y_pred):\n        plt.scatter(y_test, y_pred)\n        plt.xlabel('Actual Values')\n        plt.ylabel('Predicted Values')\n        plt.title('Actual vs Predicted Values')\n        plt.show()\n\n    def main():\n        # Load and preprocess data\n        df = load_data('data.csv')\n        df = preprocess_data(df)\n\n        # Split features and target\n        X = df.drop('target', axis=1)\n        y = df['target']\n\n        # Split data into train and test sets\n        X_train, X_test, y_train, y_test = split_data(X, y)\n\n        # Train the model\n        model = train_model(X_train, y_train)", "file_path": "analyze_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from abc import ABC, abstractmethod\nimport asyncio\nimport json\n", "symbols": ["ArchitectureExpert", "str", "api_key", "preprocess_data", "code_summary", "context", "train_model", "ExpertModel", "__name__", "split_data", "enhanced_context", "_run_expert_analyses", "plot_results", "indent", "r2", "y", "enhanced_analysis", "df", "load_data", "final_analysis", "integration_prompt", "code", "_integrate_analyses", "CodeQualityExpert", "analyze", "main", "prompt", "large_file_path", "drop_first", "random_state", "llm_api", "y_pred", "summary", "X", "__init__", "PerformanceExpert", "EnhancedLLMApi", "mse", "basic_analysis", "model", "analysis", "analysis_result", "CodeSummaryExpert", "SecurityExpert", "_run_expert_analysis", "TestingExpert", "evaluate_model", "main2", "results", "expert_analyses", "experts", "test_size", "y_test", "code_to_analyze", "inplace", "visualization", "tasks", "axis"]}, {"content": "# Split data into train and test sets\n        X_train, X_test, y_train, y_test = split_data(X, y)\n\n        # Train the model\n        model = train_model(X_train, y_train)\n\n        # Evaluate the model\n        mse, r2 = evaluate_model(model, X_test, y_test)\n        print(f\"Mean Squared Error: {mse}\")\n        print(f\"R-squared Score: {r2}\")\n\n        # Plot results\n        y_pred = model.predict(X_test)\n        plot_results(y_test, y_pred)\n\n    if __name__ == \"__main__\":\n        main()\n    \"\"\"\n    \n    # \u751f\u6210\u4ee3\u7801\u6458\u8981\n    summary = await llm_api.generate_code_summary(code_to_analyze)\n    print(\"Code Summary:\")\n    print(summary)\n    \n    # \u6267\u884c\u5b8c\u6574\u7684\u589e\u5f3a\u5206\u6790\n    analysis_result = await llm_api.enhanced_analysis(code_to_analyze)\n    print(\"\\nFull Analysis:\")\n    print(json.dumps(analysis_result, indent=2))", "file_path": "analyze_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from abc import ABC, abstractmethod\nimport asyncio\nimport json\n", "symbols": ["ArchitectureExpert", "str", "api_key", "preprocess_data", "code_summary", "context", "train_model", "ExpertModel", "__name__", "split_data", "enhanced_context", "_run_expert_analyses", "plot_results", "indent", "r2", "y", "enhanced_analysis", "df", "load_data", "final_analysis", "integration_prompt", "code", "_integrate_analyses", "CodeQualityExpert", "analyze", "main", "prompt", "large_file_path", "drop_first", "random_state", "llm_api", "y_pred", "summary", "X", "__init__", "PerformanceExpert", "EnhancedLLMApi", "mse", "basic_analysis", "model", "analysis", "analysis_result", "CodeSummaryExpert", "SecurityExpert", "_run_expert_analysis", "TestingExpert", "evaluate_model", "main2", "results", "expert_analyses", "experts", "test_size", "y_test", "code_to_analyze", "inplace", "visualization", "tasks", "axis"]}, {"content": "if __name__ == \"__main__\":\n    asyncio.run(main())\n    \n    \n# \u4f7f\u7528\u793a\u4f8b\nasync def main2():\n    api_key = \"your-api-key-here\"\n    llm_api = EnhancedLLMApi(api_key)\n    \n    large_file_path = \"/path/to/your/large/file.py\"\n    analysis_result = await llm_api.analyze_large_file(large_file_path)\n    \n    print(\"File Summary:\")\n    print(analysis_result['summary'])\n    \n    print(\"\\nBlock Analyses:\")\n    for block_analysis in analysis_result['block_analyses']:\n        print(f\"\\n{block_analysis['type']} {block_analysis['name']}:\")\n        print(block_analysis['analysis'])\n    \n    print(\"\\nFinal Integrated Analysis:\")\n    print(analysis_result['final_analysis'])\n    \n    print(\"\\nGlobal Context:\")\n    print(analysis_result['global_context'])", "file_path": "analyze_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from abc import ABC, abstractmethod\nimport asyncio\nimport json\n", "symbols": ["ArchitectureExpert", "str", "api_key", "preprocess_data", "code_summary", "context", "train_model", "ExpertModel", "__name__", "split_data", "enhanced_context", "_run_expert_analyses", "plot_results", "indent", "r2", "y", "enhanced_analysis", "df", "load_data", "final_analysis", "integration_prompt", "code", "_integrate_analyses", "CodeQualityExpert", "analyze", "main", "prompt", "large_file_path", "drop_first", "random_state", "llm_api", "y_pred", "summary", "X", "__init__", "PerformanceExpert", "EnhancedLLMApi", "mse", "basic_analysis", "model", "analysis", "analysis_result", "CodeSummaryExpert", "SecurityExpert", "_run_expert_analysis", "TestingExpert", "evaluate_model", "main2", "results", "expert_analyses", "experts", "test_size", "y_test", "code_to_analyze", "inplace", "visualization", "tasks", "axis"]}, {"content": "import ast\nimport importlib\nimport inspect\nimport git\nfrom pylint import epylint as lint\nfrom radon.complexity import cc_visit\nfrom bandit import manager as bandit_manager\nimport cProfile\nimport io\nimport pstats\nimport re\nimport json\nimport asyncio\nfrom typing import List, Dict, Any\nimport networkx as nx\nfrom sentence_transformers import SentenceTransformer, util\nfrom LLMApi_4 import LLMApi", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "class EnhancedLLMApi(LLMApi):\n    def __init__(self, api_key: str = None, model: str = \"gpt-3.5-turbo\"):\n        super().__init__(api_key, model)\n        self.ast_analyzer = ASTAnalyzer()\n        self.git_analyzer = GitAnalyzer()\n        self.static_analyzer = StaticCodeAnalyzer()\n        self.test_analyzer = TestAnalyzer()\n        self.dependency_analyzer = DependencyAnalyzer()\n        self.pattern_recognizer = DesignPatternRecognizer()\n        self.performance_analyzer = PerformanceAnalyzer()\n        self.security_scanner = SecurityScanner()\n        self.domain_knowledge = DomainKnowledgeBase()\n        self.comment_extractor = CommentExtractor()\n        self.metaprogramming_analyzer = MetaprogrammingAnalyzer()\n        self.environment_analyzer = EnvironmentAnalyzer()", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "async def enhanced_analysis(self, code: str, repo_path: str = None) -> Dict[str, Any]:\n        basic_analysis = await self.analyze_with_global_context(code)\n        \n        enhanced_context = {\n            \"ast_analysis\": self.ast_analyzer.analyze(code),\n            \"git_analysis\": self.git_analyzer.analyze(repo_path) if repo_path else None,\n            \"static_analysis\": self.static_analyzer.analyze(code),\n            \"test_analysis\": self.test_analyzer.analyze(repo_path) if repo_path else None,\n            \"dependency_analysis\": self.dependency_analyzer.analyze(repo_path) if repo_path else None,\n            \"pattern_analysis\": self.pattern_recognizer.recognize(code),\n            \"performance_analysis\": self.performance_analyzer.analyze(code),\n            \"security_analysis\": self.security_scanner.scan(code),\n            \"domain_insights\": self.domain_knowledge.get_insights(code),\n            \"comments_analysis\": self.comment_extractor.extract(code),", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "\"domain_insights\": self.domain_knowledge.get_insights(code),\n            \"comments_analysis\": self.comment_extractor.extract(code),\n            \"metaprogramming_analysis\": self.metaprogramming_analyzer.analyze(code),\n            \"environment_analysis\": self.environment_analyzer.analyze(repo_path) if repo_path else None\n        }", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "final_analysis = await self.analyze(\n            json.dumps(enhanced_context),\n            self.prompts['enhanced_analysis']\n        )\n\n        visualization = self.generate_visualization(enhanced_context)\n\n        return {\n            **basic_analysis, \n            \"enhanced_analysis\": final_analysis,\n            \"visualization\": visualization\n        }\n\n    def generate_visualization(self, context: Dict[str, Any]) -> str:\n        # \u5b9e\u73b0\u53ef\u89c6\u5316\u903b\u8f91\n        # \u8fd9\u91cc\u53ef\u4ee5\u4f7f\u7528 graphviz \u6216\u5176\u4ed6\u53ef\u89c6\u5316\u5e93\n        # \u8fd4\u56de\u53ef\u89c6\u5316\u7ed3\u679c\u7684\u6587\u4ef6\u8def\u5f84\u6216 base64 \u7f16\u7801\u7684\u56fe\u50cf\n        pass", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "def generate_visualization(self, context: Dict[str, Any]) -> str:\n        # \u5b9e\u73b0\u53ef\u89c6\u5316\u903b\u8f91\n        # \u8fd9\u91cc\u53ef\u4ee5\u4f7f\u7528 graphviz \u6216\u5176\u4ed6\u53ef\u89c6\u5316\u5e93\n        # \u8fd4\u56de\u53ef\u89c6\u5316\u7ed3\u679c\u7684\u6587\u4ef6\u8def\u5f84\u6216 base64 \u7f16\u7801\u7684\u56fe\u50cf\n        pass\n\n    async def interactive_enhanced_analysis(self, code: str, repo_path: str = None):\n        print(\"Performing enhanced analysis...\")\n        analysis_result = await self.enhanced_analysis(code, repo_path)\n        print(\"Enhanced analysis complete. You can now ask detailed questions about the code.\")\n        \n        while True:\n            query = input(\"\\nEnter your question (or 'quit' to exit): \")\n            if query.lower() == 'quit':\n                break\n\n            context = json.dumps({\n                \"basic_analysis\": analysis_result['overview'],\n                \"enhanced_analysis\": analysis_result['enhanced_analysis'],\n                \"visualization\": analysis_result['visualization']\n            })", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "answer = await self.analyze(context, self.prompts['interactive_enhanced_query'].format(query=query))\n            print(\"\\nAnswer:\", answer)\n\n    async def adaptive_learning(self, feedback: str):\n        learning_prompt = self.prompts['adaptive_learning'].format(feedback=feedback)\n        learning_result = await self.analyze(\"\", learning_prompt)\n        \n        # \u8fd9\u91cc\u53ef\u4ee5\u5b9e\u73b0\u57fa\u4e8e\u5b66\u4e60\u7ed3\u679c\u66f4\u65b0\u5206\u6790\u7b56\u7565\u6216\u77e5\u8bc6\u5e93\u7684\u903b\u8f91\n        # \u4f8b\u5982\uff0c\u53ef\u4ee5\u66f4\u65b0 DomainKnowledgeBase \u4e2d\u7684\u5173\u952e\u8bcd\n        # \u6216\u8005\u8c03\u6574 DesignPatternRecognizer \u4e2d\u7684\u6a21\u5f0f\u5339\u914d\u89c4\u5219\n        print(\"Learning result:\", learning_result)\n        # TODO: Implement logic to update analysis strategies based on learning_result\n\n# \u5176\u4ed6\u7c7b\u7684\u5b9e\u73b0\u4fdd\u6301\u4e0d\u53d8", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "class ASTAnalyzer:\n    def analyze(self, code: str) -> Dict[str, Any]:\n        tree = ast.parse(code)\n        return {\n            \"imports\": [node.names[0].name for node in ast.walk(tree) if isinstance(node, ast.Import)],\n            \"functions\": [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)],\n            \"classes\": [node.name for node in ast.walk(tree) if isinstance(node, ast.ClassDef)],\n            \"assignments\": [node.targets[0].id for node in ast.walk(tree) if isinstance(node, ast.Assign) and isinstance(node.targets[0], ast.Name)],\n            \"control_structures\": self._get_control_structures(tree)\n        }\n\n    def _get_control_structures(self, tree: ast.AST) -> List[str]:\n        control_structures = []\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.If, ast.For, ast.While, ast.Try)):\n                control_structures.append(type(node).__name__)\n        return control_structures", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "class GitAnalyzer:\n    def analyze(self, repo_path: str) -> Dict[str, Any]:\n        repo = git.Repo(repo_path)\n        return {\n            \"recent_commits\": [{\"message\": c.message, \"author\": str(c.author), \"date\": str(c.committed_datetime)} for c in list(repo.iter_commits(max_count=5))],\n            \"branches\": [str(b) for b in repo.branches],\n            \"tags\": [str(t) for t in repo.tags],\n            \"contributors\": self._get_contributors(repo)\n        }\n\n    def _get_contributors(self, repo: git.Repo) -> List[Dict[str, Any]]:\n        return [{\"name\": c.name, \"email\": c.email, \"commits\": c.count} for c in repo.get_contributors()]", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "class StaticCodeAnalyzer:\n    def analyze(self, code: str) -> Dict[str, Any]:\n        pylint_stdout, pylint_stderr = lint.py_run(code, return_std=True)\n        complexity = cc_visit(code)\n        return {\n            \"pylint_output\": pylint_stdout.getvalue(),\n            \"complexity\": [{\"name\": item.name, \"complexity\": item.complexity} for item in complexity],\n            \"maintainability_index\": self._calculate_maintainability_index(code)\n        }\n\n    def _calculate_maintainability_index(self, code: str) -> float:\n        # \u5b9e\u73b0\u7ef4\u62a4\u6027\u6307\u6570\u7684\u8ba1\u7b97\n        # \u8fd9\u53ef\u80fd\u9700\u8981\u4f7f\u7528\u989d\u5916\u7684\u5e93\u6216\u81ea\u5b9a\u4e49\u7b97\u6cd5\n        pass", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "class TestAnalyzer:\n    def analyze(self, repo_path: str) -> Dict[str, Any]:\n        import unittest\n        loader = unittest.TestLoader()\n        suite = loader.discover(repo_path)\n        result = unittest.TextTestRunner().run(suite)\n        return {\n            \"tests_run\": result.testsRun,\n            \"errors\": len(result.errors),\n            \"failures\": len(result.failures),\n            \"coverage\": self._get_test_coverage(repo_path)\n        }\n\n    def _get_test_coverage(self, repo_path: str) -> Dict[str, float]:\n        # \u5b9e\u73b0\u6d4b\u8bd5\u8986\u76d6\u7387\u7684\u8ba1\u7b97\n        # \u53ef\u4ee5\u4f7f\u7528 coverage.py \u5e93\n        pass", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "class DependencyAnalyzer:\n    def analyze(self, repo_path: str) -> Dict[str, Any]:\n        with open(f\"{repo_path}/requirements.txt\", \"r\") as f:\n            requirements = f.read().splitlines()\n        return {\n            \"dependencies\": requirements,\n            \"dependency_graph\": self._build_dependency_graph(requirements)\n        }\n\n    def _build_dependency_graph(self, requirements: List[str]) -> Dict[str, List[str]]:\n        # \u6784\u5efa\u4f9d\u8d56\u5173\u7cfb\u56fe\n        # \u53ef\u80fd\u9700\u8981\u4f7f\u7528 pip \u6216\u5176\u4ed6\u5de5\u5177\u6765\u89e3\u6790\u4f9d\u8d56\u5173\u7cfb\n        pass", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "class DesignPatternRecognizer:\n    def recognize(self, code: str) -> Dict[str, bool]:\n        patterns = {\n            \"Singleton\": r\"class.*?:\\s*_instance\\s*=\\s*None.*?@classmethod.*?def\\s+getInstance\",\n            \"Factory\": r\"class.*?Factory.*?:.*?def\\s+create\",\n            \"Observer\": r\"class.*?Observer.*?:.*?def\\s+update\",\n            \"Strategy\": r\"class.*?Strategy.*?:.*?def\\s+execute\",\n            \"Decorator\": r\"class.*?Decorator.*?:.*?def\\s+__init__.*?def\\s+__call__\"\n        }\n        recognized = {}\n        for pattern, regex in patterns.items():\n            recognized[pattern] = bool(re.search(regex, code, re.DOTALL))\n        return recognized", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "class PerformanceAnalyzer:\n    def analyze(self, code: str) -> Dict[str, Any]:\n        pr = cProfile.Profile()\n        pr.enable()\n        \n        # Execute the code\n        exec(code)\n        \n        pr.disable()\n        s = io.StringIO()\n        sortby = 'cumulative'\n        ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n        ps.print_stats()\n        \n        return {\n            \"profile\": s.getvalue(),\n            \"hotspots\": self._identify_hotspots(ps)\n        }\n\n    def _identify_hotspots(self, stats: pstats.Stats) -> List[Dict[str, Any]]:\n        # \u8bc6\u522b\u6027\u80fd\u70ed\u70b9\n        # \u8fd4\u56de\u8017\u65f6\u6700\u591a\u7684\u51fd\u6570\u5217\u8868\n        pass", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "class SecurityScanner:\n    def scan(self, code: str) -> List[Dict[str, Any]]:\n        b_mgr = bandit_manager.BanditManager()\n        b_mgr.discover_files([code])\n        b_mgr.run_tests()\n        return [\n            {\n                \"severity\": issue.severity,\n                \"confidence\": issue.confidence,\n                \"text\": issue.text\n            } for issue in b_mgr.get_issue_list()\n        ]", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "class DomainKnowledgeBase:\n    def __init__(self):\n        self.knowledge_base = {\n            \"finance\": [\"calculate_interest\", \"compound_interest\", \"amortization\"],\n            \"web_development\": [\"route\", \"middleware\", \"session\", \"cookie\"],\n            \"machine_learning\": [\"train\", \"predict\", \"feature_extraction\", \"model\"],\n            \"data_processing\": [\"parse\", \"transform\", \"clean\", \"aggregate\"],\n            \"networking\": [\"socket\", \"protocol\", \"packet\", \"request\", \"response\"]\n        }\n\n    def get_insights(self, code: str) -> Dict[str, str]:\n        insights = {}\n        for domain, keywords in self.knowledge_base.items():\n            if any(keyword in code for keyword in keywords):\n                insights[domain] = f\"This code appears to be related to {domain}. Consider reviewing relevant {domain} best practices and patterns.\"\n        return insights", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "class CommentExtractor:\n    def extract(self, code: str) -> Dict[str, List[str]]:\n        tree = ast.parse(code)\n        return {\n            \"module_docstring\": ast.get_docstring(tree),\n            \"function_comments\": self._get_function_comments(tree),\n            \"class_comments\": self._get_class_comments(tree),\n            \"inline_comments\": self._get_inline_comments(code)\n        }\n\n    def _get_function_comments(self, tree: ast.AST) -> List[Dict[str, str]]:\n        return [\n            {\"function\": node.name, \"docstring\": ast.get_docstring(node)}\n            for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)\n        ]\n\n    def _get_class_comments(self, tree: ast.AST) -> List[Dict[str, str]]:\n        return [\n            {\"class\": node.name, \"docstring\": ast.get_docstring(node)}\n            for node in ast.walk(tree) if isinstance(node, ast.ClassDef)\n        ]", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "def _get_inline_comments(self, code: str) -> List[str]:\n        return re.findall(r'#.*$', code, re.MULTILINE)", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "class MetaprogrammingAnalyzer:\n    def analyze(self, code: str) -> Dict[str, Any]:\n        tree = ast.parse(code)\n        return {\n            \"decorators\": self._get_decorators(tree),\n            \"metaclasses\": self._get_metaclasses(tree),\n            \"dynamic_imports\": self._get_dynamic_imports(tree)\n        }\n\n    def _get_decorators(self, tree: ast.AST) -> List[str]:\n        return [\n            decorator.id if isinstance(decorator, ast.Name) else decorator.attr\n            for node in ast.walk(tree)\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef))\n            for decorator in node.decorator_list\n        ]\n\n    def _get_metaclasses(self, tree: ast.AST) -> List[str]:\n        return [\n            base.id for node in ast.walk(tree)\n            if isinstance(node, ast.ClassDef)\n            for base in node.bases\n            if isinstance(base, ast.Name) and base.id == 'type'\n        ]", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "def _get_dynamic_imports(self, tree: ast.AST) -> List[str]:\n        return [\n            node.names[0].name\n            for node in ast.walk(tree)\n            if isinstance(node, ast.ImportFrom) and node.level > 0\n        ]", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "class EnvironmentAnalyzer:\n    def analyze(self, repo_path: str) -> Dict[str, Any]:\n        return {\n            \"python_version\": self._get_python_version(),\n            \"environment_variables\": self._get_environment_variables(repo_path),\n            \"configuration_files\": self._get_configuration_files(repo_path)\n        }\n\n    def _get_python_version(self) -> str:\n        import sys\n        return sys.version\n\n    def _get_environment_variables(self, repo_path: str) -> Dict[str, str]:\n        # \u8bfb\u53d6 .env \u6587\u4ef6\u6216\u5176\u4ed6\u73af\u5883\u53d8\u91cf\u914d\u7f6e\n        # \u6ce8\u610f\u4e0d\u8981\u8fd4\u56de\u654f\u611f\u4fe1\u606f\n        pass\n\n    def _get_configuration_files(self, repo_path: str) -> List[str]:\n        import os\n        config_files = []\n        for root, dirs, files in os.walk(repo_path):\n            for file in files:\n                if file.endswith(('.ini', '.yaml', '.json', '.toml')):\n                    config_files.append(os.path.join(root, file))\n        return config_files", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "async def main():\n    api_key = \"your-api-key-here\"\n    llm_api = EnhancedLLMApi(api_key)\n    \n    code_to_analyze = \"\"\"\n    # Your code here\n    \"\"\"\n    \n    repo_path = \"/path/to/your/repo\"  # \u5982\u679c\u6709\u5bf9\u5e94\u7684 Git \u4ed3\u5e93\n    \n    analysis_result = await llm_api.enhanced_analysis(code_to_analyze, repo_path)\n    print(json.dumps(analysis_result, indent=2))\n    \n    # \u542f\u52a8\u4ea4\u4e92\u5f0f\u5206\u6790\u4f1a\u8bdd\n    await llm_api.interactive_enhanced_analysis(code_to_analyze, repo_path)\n    \n    # \u63d0\u4f9b\u53cd\u9988\u4ee5\u6539\u8fdb\u5206\u6790\n    feedback = \"The analysis missed some important design patterns in the code.\"\n    await llm_api.adaptive_learning(feedback)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())", "file_path": "EnhancedLLMApi.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport importlib\nimport inspect\n", "symbols": ["api_key", "__name__", "performance_analyzer", "loader", "GitAnalyzer", "final_analysis", "_get_control_structures", "learning_result", "return_std", "analyze", "domain_knowledge", "metaprogramming_analyzer", "_get_contributors", "pr", "CommentExtractor", "get_insights", "pattern_recognizer", "repo_path", "DomainKnowledgeBase", "ps", "_get_dynamic_imports", "recognize", "DesignPatternRecognizer", "_get_class_comments", "knowledge_base", "_get_configuration_files", "_calculate_maintainability_index", "enhanced_context", "requirements", "security_scanner", "learning_prompt", "SecurityScanner", "generate_visualization", "main", "stream", "b_mgr", "llm_api", "scan", "max_count", "__init__", "tree", "_get_decorators", "EnhancedLLMApi", "answer", "extract", "analysis_result", "_get_test_coverage", "code_to_analyze", "ast_analyzer", "visualization", "str", "config_files", "StaticCodeAnalyzer", "MetaprogrammingAnalyzer", "DependencyAnalyzer", "pylint_stderr", "_build_dependency_graph", "insights", "_get_python_version", "git_analyzer", "test_analyzer", "environment_analyzer", "dependency_analyzer", "_get_inline_comments", "ASTAnalyzer", "suite", "basic_analysis", "patterns", "recognized", "TestAnalyzer", "complexity", "static_analyzer", "feedback", "id", "context", "indent", "s", "control_structures", "enhanced_analysis", "_get_metaclasses", "PerformanceAnalyzer", "result", "interactive_enhanced_analysis", "_get_function_comments", "adaptive_learning", "query", "_identify_hotspots", "_get_environment_variables", "comment_extractor", "sortby", "repo", "EnvironmentAnalyzer"]}, {"content": "import asyncio\nimport json\nfrom typing import Dict, Any, Tuple\nfrom experts import (\n    ExpertModel,\n    ArchitectureExpert,\n    PerformanceExpert,\n    SecurityExpert,\n    CodeQualityExpert,\n    TestingExpert,\n    CodeSummaryExpert\n)", "file_path": "enhanced_llm_api.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import asyncio\nimport json\nfrom typing import Dict, Any, Tuple\n", "symbols": ["str", "api_key", "code_summary", "context", "enhanced_context", "_run_expert_analyses", "enhanced_analysis", "final_analysis", "integration_prompt", "_integrate_analyses", "__init__", "EnhancedLLMApi", "basic_analysis", "model", "analysis", "_run_expert_analysis", "results", "expert_analyses", "experts", "visualization", "tasks"]}, {"content": "class EnhancedLLMApi:\n    def __init__(self, api_key: str = None, model: str = \"gpt-3.5-turbo\"):\n        self.api_key = api_key\n        self.model = model\n        self.experts = {\n            \"architecture\": ArchitectureExpert(api_key),\n            \"performance\": PerformanceExpert(api_key),\n            \"security\": SecurityExpert(api_key),\n            \"code_quality\": CodeQualityExpert(api_key),\n            \"testing\": TestingExpert(api_key),\n            \"code_summary\": CodeSummaryExpert(api_key)\n        }", "file_path": "enhanced_llm_api.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import asyncio\nimport json\nfrom typing import Dict, Any, Tuple\n", "symbols": ["str", "api_key", "code_summary", "context", "enhanced_context", "_run_expert_analyses", "enhanced_analysis", "final_analysis", "integration_prompt", "_integrate_analyses", "__init__", "EnhancedLLMApi", "basic_analysis", "model", "analysis", "_run_expert_analysis", "results", "expert_analyses", "experts", "visualization", "tasks"]}, {"content": "async def enhanced_analysis(self, code: str, repo_path: str = None) -> Dict[str, Any]:\n        \"\"\"Perform enhanced analysis using multiple expert models.\"\"\"\n        # Get basic analysis results\n        basic_analysis = await self.analyze_with_global_context(code)\n        \n        # Prepare enhanced context with various analyses\n        enhanced_context = {\n            \"ast_analysis\": self.ast_analyzer.analyze(code),\n            \"git_analysis\": self.git_analyzer.analyze(repo_path) if repo_path else None,\n            \"static_analysis\": self.static_analyzer.analyze(code),\n            \"test_analysis\": self.test_analyzer.analyze(repo_path) if repo_path else None,\n            \"dependency_analysis\": self.dependency_analyzer.analyze(repo_path) if repo_path else None,\n            \"pattern_analysis\": self.pattern_recognizer.recognize(code),\n            \"performance_analysis\": self.performance_analyzer.analyze(code),\n            \"security_analysis\": self.security_scanner.scan(code),", "file_path": "enhanced_llm_api.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import asyncio\nimport json\nfrom typing import Dict, Any, Tuple\n", "symbols": ["str", "api_key", "code_summary", "context", "enhanced_context", "_run_expert_analyses", "enhanced_analysis", "final_analysis", "integration_prompt", "_integrate_analyses", "__init__", "EnhancedLLMApi", "basic_analysis", "model", "analysis", "_run_expert_analysis", "results", "expert_analyses", "experts", "visualization", "tasks"]}, {"content": "\"performance_analysis\": self.performance_analyzer.analyze(code),\n            \"security_analysis\": self.security_scanner.scan(code),\n            \"domain_insights\": self.domain_knowledge.get_insights(code),\n            \"comments_analysis\": self.comment_extractor.extract(code),\n            \"metaprogramming_analysis\": self.metaprogramming_analyzer.analyze(code),\n            \"environment_analysis\": self.environment_analyzer.analyze(repo_path) if repo_path else None\n        }", "file_path": "enhanced_llm_api.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import asyncio\nimport json\nfrom typing import Dict, Any, Tuple\n", "symbols": ["str", "api_key", "code_summary", "context", "enhanced_context", "_run_expert_analyses", "enhanced_analysis", "final_analysis", "integration_prompt", "_integrate_analyses", "__init__", "EnhancedLLMApi", "basic_analysis", "model", "analysis", "_run_expert_analysis", "results", "expert_analyses", "experts", "visualization", "tasks"]}, {"content": "# Run expert analyses\n        expert_analyses = await self._run_expert_analyses(code, enhanced_context)\n        enhanced_context.update(expert_analyses)\n        \n        # Generate code summary\n        code_summary = expert_analyses['code_summary_analysis']\n\n        # Integrate all analyses\n        final_analysis = await self._integrate_analyses(enhanced_context)\n\n        # Generate visualization\n        visualization = self.generate_visualization(enhanced_context)\n\n        return {\n            **basic_analysis, \n            \"enhanced_analysis\": final_analysis,\n            \"expert_analyses\": expert_analyses,\n            \"code_summary\": code_summary,\n            \"visualization\": visualization\n        }", "file_path": "enhanced_llm_api.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import asyncio\nimport json\nfrom typing import Dict, Any, Tuple\n", "symbols": ["str", "api_key", "code_summary", "context", "enhanced_context", "_run_expert_analyses", "enhanced_analysis", "final_analysis", "integration_prompt", "_integrate_analyses", "__init__", "EnhancedLLMApi", "basic_analysis", "model", "analysis", "_run_expert_analysis", "results", "expert_analyses", "experts", "visualization", "tasks"]}, {"content": "async def _run_expert_analyses(self, code: str, context: Dict[str, Any]) -> Dict[str, str]:\n        \"\"\"Run analyses using all expert models concurrently.\"\"\"\n        expert_analyses = {}\n        tasks = []\n        for expert_name, expert in self.experts.items():\n            tasks.append(self._run_expert_analysis(expert_name, expert, code, context))\n        results = await asyncio.gather(*tasks)\n        for expert_name, analysis in results:\n            expert_analyses[f\"{expert_name}_analysis\"] = analysis\n        return expert_analyses\n\n    async def _run_expert_analysis(self, expert_name: str, expert: ExpertModel, code: str, context: Dict[str, Any]) -> Tuple[str, str]:\n        \"\"\"Run analysis using a single expert model.\"\"\"\n        analysis = await expert.analyze(code, context)\n        return expert_name, analysis", "file_path": "enhanced_llm_api.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import asyncio\nimport json\nfrom typing import Dict, Any, Tuple\n", "symbols": ["str", "api_key", "code_summary", "context", "enhanced_context", "_run_expert_analyses", "enhanced_analysis", "final_analysis", "integration_prompt", "_integrate_analyses", "__init__", "EnhancedLLMApi", "basic_analysis", "model", "analysis", "_run_expert_analysis", "results", "expert_analyses", "experts", "visualization", "tasks"]}, {"content": "async def _integrate_analyses(self, context: Dict[str, Any]) -> str:\n        \"\"\"Integrate all analyses into a comprehensive report.\"\"\"\n        integration_prompt = \"\"\"\n        As a senior software architect and code analyst, review and integrate the following analyses of a codebase:\n\n        {context}\n\n        Provide a comprehensive, high-level summary of the codebase, addressing:\n        1. Overall architecture and design\n        2. Code quality and maintainability\n        3. Performance characteristics\n        4. Security considerations\n        5. Testing and reliability\n        6. Areas for improvement and recommendations\n        \n        Also, consider the provided code summary and how it relates to the detailed analyses.\n\n        Your summary should synthesize insights from all the expert analyses and provide a holistic view of the codebase.\n        \"\"\"\n        return await self.analyze(\"\", integration_prompt.format(context=json.dumps(context)))", "file_path": "enhanced_llm_api.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import asyncio\nimport json\nfrom typing import Dict, Any, Tuple\n", "symbols": ["str", "api_key", "code_summary", "context", "enhanced_context", "_run_expert_analyses", "enhanced_analysis", "final_analysis", "integration_prompt", "_integrate_analyses", "__init__", "EnhancedLLMApi", "basic_analysis", "model", "analysis", "_run_expert_analysis", "results", "expert_analyses", "experts", "visualization", "tasks"]}, {"content": "import asyncio\nfrom enhanced_llm_api import EnhancedLLMApi\n\nasync def main():\n    # Initialize the enhanced LLM API with your API key\n    api_key = \"your-api-key-here\"\n    llm_api = EnhancedLLMApi(api_key)\n    \n    # Example code to analyze\n    code_to_analyze = \"\"\"\n    def calculate_fibonacci(n: int) -> int:\n        if n <= 0:\n            raise ValueError(\"n must be positive\")\n        if n <= 2:\n            return 1\n        return calculate_fibonacci(n - 1) + calculate_fibonacci(n - 2)\n    \"\"\"\n    \n    # Perform enhanced analysis\n    analysis_result = await llm_api.enhanced_analysis(code_to_analyze)\n    \n    # Print results\n    print(\"\\nCode Summary:\")\n    print(analysis_result[\"code_summary\"])\n    \n    print(\"\\nExpert Analyses:\")\n    for expert_name, analysis in analysis_result[\"expert_analyses\"].items():\n        print(f\"\\n{expert_name.replace('_', ' ').title()}:\")\n        print(analysis)\n    \n    print(\"\\nFinal Analysis:\")\n    print(analysis_result[\"enhanced_analysis\"])", "file_path": "example.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import asyncio\nfrom enhanced_llm_api import EnhancedLLMApi\n\n", "symbols": ["main", "api_key", "analysis_result", "calculate_fibonacci", "llm_api", "code_to_analyze", "__name__"]}, {"content": "if __name__ == \"__main__\":\n    asyncio.run(main())", "file_path": "example.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import asyncio\nfrom enhanced_llm_api import EnhancedLLMApi\n\n", "symbols": ["main", "api_key", "analysis_result", "calculate_fibonacci", "llm_api", "code_to_analyze", "__name__"]}, {"content": "import base64\nimport logging\nimport tempfile\nfrom pathlib import Path\nimport os\nimport subprocess\n\n\ndef get_modified_files(repo_dir, pre_sha, post_sha):\n    logging.info(f\"Getting list of modified files for repo at {repo_dir} between commit shas {pre_sha} and {post_sha} ...\")\n\n    diff_command = f'git --no-index --no-pager diff --name-only \"{pre_sha}\" \"{post_sha}\" --diff-filter=M'\n    result = subprocess.run(\n        diff_command,\n        shell=True,\n        text=True,\n        cwd=repo_dir,\n        capture_output=True,\n    )\n    return result.stdout.splitlines()\n\n\nGIT_ATTRIBUTES_FOR_LANG_SPECIFIC_DIFF = \"\"\"*.cs\\tdiff=csharp\n*.py\\tdiff=python\"\"\"", "file_path": "git_utils.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import base64\nimport logging\nimport tempfile\n", "symbols": ["b64_auth_token", "create_new_branch", "stderr", "attributes_file_path", "commit_changes", "None", "text", "tdiff", "push_changes", "capture_output", "stdout", "cwd", "parents", "shell", "result", "GIT_ATTRIBUTES_FOR_LANG_SPECIFIC_DIFF", "clone_repo", "temp_dir", "username", "get_modified_files", "clone_command", "extraHeader", "files_to_stage", "file_paths", "pat", "filter", "remote_name", "branch_name", "stage_and_commit_changes", "_log_str", "repo_dir", "set_remote_url", "paths_list_str", "ignore_cleanup_errors", "results", "checkout_repo", "file_path", "get_current_branch", "encoding", "bool", "get_diff", "diff_command"]}, {"content": "def get_diff(pre, post, repo_dir = None, file_path = None, file_paths = None, function_context: bool = False, diff_command: str | None = None):\n    if file_path is not None and file_paths is not None:\n        raise ValueError(\"Only one of file_path and file_paths should be specified.\")\n    \n    if repo_dir is not None:\n        _log_str = f\"Getting diff in repo at {repo_dir} between commit shas {pre} and {post}\"\n    else:\n        _log_str = f\"Getting diff between {pre} and {post}\"\n    \n    if file_path:\n        _log_str += f\" for file {file_path}\"\n    elif file_paths:\n        _log_str += f\" for {len(file_paths)} files\"\n\n    if function_context:\n        _log_str += \" with function context\"\n    _log_str += \" ...\"\n    logging.debug(_log_str)", "file_path": "git_utils.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import base64\nimport logging\nimport tempfile\n", "symbols": ["b64_auth_token", "create_new_branch", "stderr", "attributes_file_path", "commit_changes", "None", "text", "tdiff", "push_changes", "capture_output", "stdout", "cwd", "parents", "shell", "result", "GIT_ATTRIBUTES_FOR_LANG_SPECIFIC_DIFF", "clone_repo", "temp_dir", "username", "get_modified_files", "clone_command", "extraHeader", "files_to_stage", "file_paths", "pat", "filter", "remote_name", "branch_name", "stage_and_commit_changes", "_log_str", "repo_dir", "set_remote_url", "paths_list_str", "ignore_cleanup_errors", "results", "checkout_repo", "file_path", "get_current_branch", "encoding", "bool", "get_diff", "diff_command"]}, {"content": "if function_context:\n        _log_str += \" with function context\"\n    _log_str += \" ...\"\n    logging.debug(_log_str)\n\n\n    temp_dir = tempfile.TemporaryDirectory(ignore_cleanup_errors=True)\n    attributes_file_path = Path(temp_dir.name) / \"git/attributes\"\n    attributes_file_path.parent.mkdir(parents=True)\n    attributes_file_path.write_text(GIT_ATTRIBUTES_FOR_LANG_SPECIFIC_DIFF)\n    os.environ[\"XDG_CONFIG_HOME\"] = str(temp_dir.name)\n\n    if diff_command is None:\n        diff_command = f\"git --no-pager diff --no-index \"\n\n    if function_context:\n        diff_command += \"-U0 --function-context \"\n\n    diff_command += f'\"{pre}\" \"{post}\"'\n\n    if file_path:\n        diff_command += f' -- \"{file_path}\"'\n    elif file_paths:\n        paths_list_str = \" \".join([str(p) for p in file_paths])\n        diff_command += f' -- {paths_list_str}'", "file_path": "git_utils.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import base64\nimport logging\nimport tempfile\n", "symbols": ["b64_auth_token", "create_new_branch", "stderr", "attributes_file_path", "commit_changes", "None", "text", "tdiff", "push_changes", "capture_output", "stdout", "cwd", "parents", "shell", "result", "GIT_ATTRIBUTES_FOR_LANG_SPECIFIC_DIFF", "clone_repo", "temp_dir", "username", "get_modified_files", "clone_command", "extraHeader", "files_to_stage", "file_paths", "pat", "filter", "remote_name", "branch_name", "stage_and_commit_changes", "_log_str", "repo_dir", "set_remote_url", "paths_list_str", "ignore_cleanup_errors", "results", "checkout_repo", "file_path", "get_current_branch", "encoding", "bool", "get_diff", "diff_command"]}, {"content": "if file_path:\n        diff_command += f' -- \"{file_path}\"'\n    elif file_paths:\n        paths_list_str = \" \".join([str(p) for p in file_paths])\n        diff_command += f' -- {paths_list_str}'\n\n    logging.debug(diff_command)\n    if repo_dir is not None:\n        result =  subprocess.run(\n            diff_command,\n            shell=True,\n            text=True,\n            cwd=repo_dir,\n            capture_output=True,\n            encoding=\"utf-8\",\n        )\n    else:\n        result =  subprocess.run(\n            diff_command,\n            shell=True,\n            text=True,\n            capture_output=True,\n            encoding=\"utf-8\",\n        )\n\n    temp_dir.cleanup()\n    os.environ.pop(\"XDG_CONFIG_HOME\", None)", "file_path": "git_utils.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import base64\nimport logging\nimport tempfile\n", "symbols": ["b64_auth_token", "create_new_branch", "stderr", "attributes_file_path", "commit_changes", "None", "text", "tdiff", "push_changes", "capture_output", "stdout", "cwd", "parents", "shell", "result", "GIT_ATTRIBUTES_FOR_LANG_SPECIFIC_DIFF", "clone_repo", "temp_dir", "username", "get_modified_files", "clone_command", "extraHeader", "files_to_stage", "file_paths", "pat", "filter", "remote_name", "branch_name", "stage_and_commit_changes", "_log_str", "repo_dir", "set_remote_url", "paths_list_str", "ignore_cleanup_errors", "results", "checkout_repo", "file_path", "get_current_branch", "encoding", "bool", "get_diff", "diff_command"]}, {"content": "temp_dir.cleanup()\n    os.environ.pop(\"XDG_CONFIG_HOME\", None)\n\n    if result.returncode != 0 and result.stderr != \"\" and not result.stderr.startswith(\"warning:\"):\n        raise RuntimeError(f\"Failed to get diff in repo at {repo_dir} between {pre} and {post} using the following diff command:\\n{diff_command}\\n\\n resulting in the following error:\\n{result.stderr}\")\n\n    return result.stdout", "file_path": "git_utils.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import base64\nimport logging\nimport tempfile\n", "symbols": ["b64_auth_token", "create_new_branch", "stderr", "attributes_file_path", "commit_changes", "None", "text", "tdiff", "push_changes", "capture_output", "stdout", "cwd", "parents", "shell", "result", "GIT_ATTRIBUTES_FOR_LANG_SPECIFIC_DIFF", "clone_repo", "temp_dir", "username", "get_modified_files", "clone_command", "extraHeader", "files_to_stage", "file_paths", "pat", "filter", "remote_name", "branch_name", "stage_and_commit_changes", "_log_str", "repo_dir", "set_remote_url", "paths_list_str", "ignore_cleanup_errors", "results", "checkout_repo", "file_path", "get_current_branch", "encoding", "bool", "get_diff", "diff_command"]}, {"content": "def clone_repo(repo_url, repo_dir, pat = None, username = None):\n    if pat is not None:\n        username = username if username is not None else \"\"\n        b64_auth_token = base64.b64encode(bytes(f\"{username}:{pat.strip()}\", 'utf-8')).decode('utf-8')\n        clone_command = f'git -c http.extraHeader=\"Authorization: Basic {b64_auth_token}\" clone {repo_url} {repo_dir}'\n    else:\n        clone_command = f'git clone {repo_url} {repo_dir}'\n\n    logging.info(f\"Cloning repo from URL {repo_url} into directory {repo_dir} ...\")\n    result = subprocess.run(clone_command, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    if result.returncode != 0:\n        raise RuntimeError(f\"Failed to clone repo from URL {repo_url} into directory {repo_dir}.\")", "file_path": "git_utils.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import base64\nimport logging\nimport tempfile\n", "symbols": ["b64_auth_token", "create_new_branch", "stderr", "attributes_file_path", "commit_changes", "None", "text", "tdiff", "push_changes", "capture_output", "stdout", "cwd", "parents", "shell", "result", "GIT_ATTRIBUTES_FOR_LANG_SPECIFIC_DIFF", "clone_repo", "temp_dir", "username", "get_modified_files", "clone_command", "extraHeader", "files_to_stage", "file_paths", "pat", "filter", "remote_name", "branch_name", "stage_and_commit_changes", "_log_str", "repo_dir", "set_remote_url", "paths_list_str", "ignore_cleanup_errors", "results", "checkout_repo", "file_path", "get_current_branch", "encoding", "bool", "get_diff", "diff_command"]}, {"content": "def checkout_repo(repo_dir, commit):\n    logging.info(f\"Checking out repo at {repo_dir} at {commit} ...\")\n    result = subprocess.run(f'git checkout {commit}', shell=True, cwd=repo_dir, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    if result.returncode != 0:\n        raise RuntimeError(f\"Failed to checkout repo at {repo_dir} at {commit}.\")\n\n\ndef create_new_branch(repo_dir, new_branch_name, old_commit):\n    logging.info(f\"Creating new branch {new_branch_name} from {old_commit} in repo at {repo_dir} ...\")\n    result = subprocess.run(f'git checkout -b {new_branch_name} {old_commit}', shell=True, cwd=repo_dir, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    if result.returncode != 0:\n        raise RuntimeError(f\"Failed to create new branch {new_branch_name} from {old_commit} in repo at {repo_dir}.\")", "file_path": "git_utils.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import base64\nimport logging\nimport tempfile\n", "symbols": ["b64_auth_token", "create_new_branch", "stderr", "attributes_file_path", "commit_changes", "None", "text", "tdiff", "push_changes", "capture_output", "stdout", "cwd", "parents", "shell", "result", "GIT_ATTRIBUTES_FOR_LANG_SPECIFIC_DIFF", "clone_repo", "temp_dir", "username", "get_modified_files", "clone_command", "extraHeader", "files_to_stage", "file_paths", "pat", "filter", "remote_name", "branch_name", "stage_and_commit_changes", "_log_str", "repo_dir", "set_remote_url", "paths_list_str", "ignore_cleanup_errors", "results", "checkout_repo", "file_path", "get_current_branch", "encoding", "bool", "get_diff", "diff_command"]}, {"content": "def commit_changes(repo_dir, commit_message):\n    logging.info(f\"Committing changes in repo at {repo_dir} with message {commit_message} ...\")\n    result = subprocess.run(f'git commit -m \"{commit_message}\"', shell=True, cwd=repo_dir, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    if result.returncode != 0:\n        raise RuntimeError(f\"Failed to commit changes in repo at {repo_dir} with message {commit_message}.\")", "file_path": "git_utils.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import base64\nimport logging\nimport tempfile\n", "symbols": ["b64_auth_token", "create_new_branch", "stderr", "attributes_file_path", "commit_changes", "None", "text", "tdiff", "push_changes", "capture_output", "stdout", "cwd", "parents", "shell", "result", "GIT_ATTRIBUTES_FOR_LANG_SPECIFIC_DIFF", "clone_repo", "temp_dir", "username", "get_modified_files", "clone_command", "extraHeader", "files_to_stage", "file_paths", "pat", "filter", "remote_name", "branch_name", "stage_and_commit_changes", "_log_str", "repo_dir", "set_remote_url", "paths_list_str", "ignore_cleanup_errors", "results", "checkout_repo", "file_path", "get_current_branch", "encoding", "bool", "get_diff", "diff_command"]}, {"content": "def stage_and_commit_changes(repo_dir, commit_message, files_to_stage = None):\n    if files_to_stage is None:\n        logging.info(f\"Committing all changes in repo at {repo_dir} with message {commit_message} ...\")\n        result = subprocess.run(f'git commit -am \"{commit_message}\"', shell=True, cwd=repo_dir, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Failed to commit changes in repo at {repo_dir} with message {commit_message}.\")\n    else:\n        logging.info(f\"Committing changes in repo at {repo_dir} with message {commit_message} for files {files_to_stage} ...\")\n        results = subprocess.run(f'git commit -m \"{commit_message}\" {\" \".join(files_to_stage)}', shell=True, cwd=repo_dir, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n        if results.returncode != 0:", "file_path": "git_utils.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import base64\nimport logging\nimport tempfile\n", "symbols": ["b64_auth_token", "create_new_branch", "stderr", "attributes_file_path", "commit_changes", "None", "text", "tdiff", "push_changes", "capture_output", "stdout", "cwd", "parents", "shell", "result", "GIT_ATTRIBUTES_FOR_LANG_SPECIFIC_DIFF", "clone_repo", "temp_dir", "username", "get_modified_files", "clone_command", "extraHeader", "files_to_stage", "file_paths", "pat", "filter", "remote_name", "branch_name", "stage_and_commit_changes", "_log_str", "repo_dir", "set_remote_url", "paths_list_str", "ignore_cleanup_errors", "results", "checkout_repo", "file_path", "get_current_branch", "encoding", "bool", "get_diff", "diff_command"]}, {"content": "if results.returncode != 0:\n            raise RuntimeError(f\"Failed to commit changes in repo at {repo_dir} with message {commit_message} for files {files_to_stage}.\")", "file_path": "git_utils.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import base64\nimport logging\nimport tempfile\n", "symbols": ["b64_auth_token", "create_new_branch", "stderr", "attributes_file_path", "commit_changes", "None", "text", "tdiff", "push_changes", "capture_output", "stdout", "cwd", "parents", "shell", "result", "GIT_ATTRIBUTES_FOR_LANG_SPECIFIC_DIFF", "clone_repo", "temp_dir", "username", "get_modified_files", "clone_command", "extraHeader", "files_to_stage", "file_paths", "pat", "filter", "remote_name", "branch_name", "stage_and_commit_changes", "_log_str", "repo_dir", "set_remote_url", "paths_list_str", "ignore_cleanup_errors", "results", "checkout_repo", "file_path", "get_current_branch", "encoding", "bool", "get_diff", "diff_command"]}, {"content": "def push_changes(repo_dir, remote_name=\"\", branch_name=\"\"):\n    logging.info(f\"Pushing changes in repo at {repo_dir} to remote {remote_name} branch {branch_name} ...\")\n    result = subprocess.run(f'git push {remote_name} {branch_name}', shell=True, cwd=repo_dir, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    if result.returncode != 0:\n        raise RuntimeError(f\"Failed to push changes in repo at {repo_dir} to remote {remote_name} branch {branch_name}.\")\n\ndef set_remote_url(repo_dir, remote_name, remote_url):\n    logging.info(f\"Setting remote URL for remote {remote_name} to {remote_url} for repo at {repo_dir} ...\")\n    results = subprocess.run(f'git remote set-url {remote_name} {remote_url}', shell=True, cwd=repo_dir, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    if results.returncode != 0:\n        raise RuntimeError(f\"Failed to set remote URL for remote {remote_name} to {remote_url} for repo at {repo_dir}.\")", "file_path": "git_utils.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import base64\nimport logging\nimport tempfile\n", "symbols": ["b64_auth_token", "create_new_branch", "stderr", "attributes_file_path", "commit_changes", "None", "text", "tdiff", "push_changes", "capture_output", "stdout", "cwd", "parents", "shell", "result", "GIT_ATTRIBUTES_FOR_LANG_SPECIFIC_DIFF", "clone_repo", "temp_dir", "username", "get_modified_files", "clone_command", "extraHeader", "files_to_stage", "file_paths", "pat", "filter", "remote_name", "branch_name", "stage_and_commit_changes", "_log_str", "repo_dir", "set_remote_url", "paths_list_str", "ignore_cleanup_errors", "results", "checkout_repo", "file_path", "get_current_branch", "encoding", "bool", "get_diff", "diff_command"]}, {"content": "def get_current_branch(repo_dir):\n    logging.info(f\"Getting current branch for repo at {repo_dir} ...\")\n    result = subprocess.run(f'git branch --show-current', shell=True, text=True, capture_output=True, cwd=repo_dir)\n    if result.returncode != 0:\n        raise RuntimeError(f\"Failed to get current branch for repo at {repo_dir}.\")\n    return result.stdout", "file_path": "git_utils.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import base64\nimport logging\nimport tempfile\n", "symbols": ["b64_auth_token", "create_new_branch", "stderr", "attributes_file_path", "commit_changes", "None", "text", "tdiff", "push_changes", "capture_output", "stdout", "cwd", "parents", "shell", "result", "GIT_ATTRIBUTES_FOR_LANG_SPECIFIC_DIFF", "clone_repo", "temp_dir", "username", "get_modified_files", "clone_command", "extraHeader", "files_to_stage", "file_paths", "pat", "filter", "remote_name", "branch_name", "stage_and_commit_changes", "_log_str", "repo_dir", "set_remote_url", "paths_list_str", "ignore_cleanup_errors", "results", "checkout_repo", "file_path", "get_current_branch", "encoding", "bool", "get_diff", "diff_command"]}, {"content": "import javalang\n\ndef parse_java_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        java_code = file.read()\n    \n    tree = javalang.parse.parse(java_code)\n    return tree\n\ndef extract_method_calls(tree):\n    method_calls = []\n    for path, node in tree.filter(javalang.tree.MethodInvocation):\n        method_calls.append({\n            'method_name': node.member,\n            'arguments': [arg.name for arg in node.arguments] if node.arguments else []\n        })\n    return method_calls", "file_path": "java_parse.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import javalang\n\ndef parse_java_file(file_path):\n", "symbols": ["parse_java_file", "extract_method_calls", "extract_lambda_method_calls", "java_file_path", "new_code", "lambda_method_calls", "method_calls", "orderList", "java_code", "OrderDao", "method_name", "encoding", "__name__", "method_call_chains", "extract_method_call_chains", "tree"]}, {"content": "def extract_lambda_method_calls(tree):\n    lambda_method_calls = []\n    for path, node in tree.filter(javalang.tree.LambdaExpression):\n        for call_path, call_node in tree.filter(javalang.tree.MethodInvocation):\n            print(call_node.arguments)\n            lambda_method_calls.append({\n                'lambda_expression': node,\n                'method_name': call_node.member,\n                # 'arguments': [arg.name for arg in call_node.arguments] if call_node.arguments else []\n            })\n    return lambda_method_calls", "file_path": "java_parse.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import javalang\n\ndef parse_java_file(file_path):\n", "symbols": ["parse_java_file", "extract_method_calls", "extract_lambda_method_calls", "java_file_path", "new_code", "lambda_method_calls", "method_calls", "orderList", "java_code", "OrderDao", "method_name", "encoding", "__name__", "method_call_chains", "extract_method_call_chains", "tree"]}, {"content": "def extract_method_call_chains(tree):\n    method_call_chains = {}\n    for path, node in tree.filter(javalang.tree.MethodDeclaration):\n        method_name = node.name\n        method_call_chains[method_name] = []\n        for call_path, call_node in tree.filter(javalang.tree.MethodInvocation, path):\n            method_call_chains[method_name].append(call_node.member)\n    return method_call_chains", "file_path": "java_parse.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import javalang\n\ndef parse_java_file(file_path):\n", "symbols": ["parse_java_file", "extract_method_calls", "extract_lambda_method_calls", "java_file_path", "new_code", "lambda_method_calls", "method_calls", "orderList", "java_code", "OrderDao", "method_name", "encoding", "__name__", "method_call_chains", "extract_method_call_chains", "tree"]}, {"content": "if __name__ == \"__main__\":\n    # java_file_path = 'path/to/your/JavaFile.java'\n    # tree = parse_java_file(java_file_path)\n    \n    new_code = \"\"\"\n    public class OrderDao{\n    private double queryOrder(Order order){\n        List<Order> orderList = jdbcTemplate.query(order);\n        orderList = orderList.stream().filter(o -> o.getOrderStatus() == OrderStatus.Paid ).collect(Collectors.toList());\n        return order;\n    }    \n    }\n    \"\"\"\n    \n    tree = javalang.parse.parse(new_code)\n    \n    lambda_method_calls = extract_lambda_method_calls(tree)\n    \n    print(\"Lambda Method Calls:\", lambda_method_calls)", "file_path": "java_parse.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import javalang\n\ndef parse_java_file(file_path):\n", "symbols": ["parse_java_file", "extract_method_calls", "extract_lambda_method_calls", "java_file_path", "new_code", "lambda_method_calls", "method_calls", "orderList", "java_code", "OrderDao", "method_name", "encoding", "__name__", "method_call_chains", "extract_method_call_chains", "tree"]}, {"content": "import ast\nimport json\nimport networkx as nx\nfrom typing import List, Dict, Any\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nimport pickle\nfrom collections import deque", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "class CodeIndexer:\n    def __init__(self):\n        self.index = {}\n        self.tfidf_vectorizer = TfidfVectorizer()\n\n    def add_to_index(self, name: str, content: str, type: str):\n        self.index[name] = {\n            \"content\": content,\n            \"type\": type,\n            \"keywords\": self.extract_keywords(content)\n        }\n\n    def extract_keywords(self, content: str, top_n: int = 5) -> List[str]:\n        tfidf_matrix = self.tfidf_vectorizer.fit_transform([content])\n        feature_names = self.tfidf_vectorizer.get_feature_names_out()\n        sorted_items = sorted(zip(tfidf_matrix.tocsc().data, feature_names), key=lambda x: x[0], reverse=True)\n        return [item[1] for item in sorted_items[:top_n]]", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "def search(self, query: str) -> List[Dict[str, Any]]:\n        query_vector = self.tfidf_vectorizer.transform([query])\n        results = []\n        for name, data in self.index.items():\n            content_vector = self.tfidf_vectorizer.transform([data['content']])\n            similarity = np.dot(query_vector.toarray(), content_vector.toarray().T)[0][0]\n            results.append({\"name\": name, \"type\": data['type'], \"similarity\": similarity})\n        return sorted(results, key=lambda x: x['similarity'], reverse=True)[:5]", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "class LargeCodeAnalyzer:\n    def __init__(self, llm_api):\n        self.llm_api = llm_api\n        self.max_tokens = 7500\n        self.dependency_graph = nx.DiGraph()\n        self.context_snapshots = deque(maxlen=10)\n        self.global_context = {}\n        self.context_size_threshold = 1000\n        self.block_summaries = {}\n        self.indexer = CodeIndexer()\n        self.project_summary = \"\"", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "async def analyze_large_file(self, file_path: str) -> Dict[str, Any]:\n        with open(file_path, 'r') as file:\n            content = file.read()\n        \n        summary = await self.generate_summary(content)\n        self.build_dependency_graph(content)\n        code_blocks = self.split_code(content)\n        \n        block_analyses = []\n        for i, block in enumerate(code_blocks):\n            block_analysis = await self.analyze_block_with_context(block, summary, i)\n            block_analyses.append(block_analysis)\n            self.save_context_snapshot(i, block_analysis)\n            await self.update_global_context(block_analysis)\n            self.block_summaries[block['name']] = block_analysis['analysis'][:200]\n            self.indexer.add_to_index(block['name'], block['code'], block['type'])\n        \n        self.project_summary = await self.generate_project_summary(summary, block_analyses)\n        final_analysis = await self.integrate_analyses(summary, block_analyses)", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "self.project_summary = await self.generate_project_summary(summary, block_analyses)\n        final_analysis = await self.integrate_analyses(summary, block_analyses)\n        \n        return {\n            \"summary\": summary,\n            \"project_summary\": self.project_summary,\n            \"block_analyses\": block_analyses,\n            \"final_analysis\": final_analysis,\n            \"dependency_graph\": nx.node_link_data(self.dependency_graph),\n            \"global_context\": self.global_context,\n            \"index\": self.indexer.index\n        }", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "async def generate_project_summary(self, file_summary: str, block_analyses: List[Dict[str, Any]]) -> str:\n        context = {\n            \"file_summary\": file_summary,\n            \"block_summaries\": [{\"name\": ba['name'], \"type\": ba['type'], \"summary\": ba['analysis'][:200]} for ba in block_analyses],\n            \"dependency_graph\": nx.node_link_data(self.dependency_graph)\n        }\n        prompt = \"\"\"\n        Based on the provided information, generate a comprehensive project summary that includes:\n        1. An overview of the project's purpose and main functionality\n        2. Key components and their roles\n        3. Important relationships and dependencies between components\n        4. Main algorithms or processes implemented\n        5. Notable design patterns or architectural choices\n        6. Potential areas of complexity or importance", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "This summary should serve as a high-level guide to understanding the project structure and functionality.\n        \"\"\"\n        return await self.llm_api.analyze(json.dumps(context), prompt)\n        \n    async def generate_summary(self, content: str) -> str:\n        tree = ast.parse(content)\n        summary = {\n            \"imports\": [node.names[0].name for node in ast.walk(tree) if isinstance(node, ast.Import)],\n            \"functions\": [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)],\n            \"classes\": [node.name for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]\n        }\n        return await self.llm_api.analyze(str(summary), \"Generate a high-level summary of this code structure.\")", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "def build_dependency_graph(self, content: str):\n        tree = ast.parse(content)\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n                self.dependency_graph.add_node(node.name)\n                for child_node in ast.walk(node):\n                    if isinstance(child_node, ast.Name) and isinstance(child_node.ctx, ast.Load):\n                        if child_node.id in self.dependency_graph:\n                            self.dependency_graph.add_edge(node.name, child_node.id)", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "def split_code(self, content: str) -> List[Dict[str, Any]]:\n        tree = ast.parse(content)\n        blocks = []\n        for node in ast.iter_child_nodes(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n                block = ast.get_source_segment(content, node)\n                if block:\n                    blocks.append({\n                        \"name\": node.name,\n                        \"type\": type(node).__name__,\n                        \"code\": block\n                    })\n        return blocks", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "async def analyze_block_with_context(self, block: Dict[str, Any], summary: str, block_index: int) -> Dict[str, Any]:\n        local_context = self.get_block_context(block['name'])\n        compressed_context = self.compress_context(local_context)\n        relevant_previous_contexts = self.get_relevant_previous_contexts(block['name'])\n        relevant_global_context = self.get_relevant_global_context(block['name'])\n        relevant_indexed_info = self.get_relevant_indexed_info(block['code'])\n        \n        prompt = f\"\"\"\n        Analyze this code block in the context of the following information:\n\n        Overall Summary:\n        {summary}\n\n        Block Type: {block['type']}\n        Block Name: {block['name']}\n\n        Local Context:\n        {compressed_context}\n\n        Relevant Previous Contexts:\n        {relevant_previous_contexts}\n\n        Relevant Global Context:\n        {relevant_global_context}\n\n        Relevant Indexed Information:\n        {relevant_indexed_info}", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "Relevant Global Context:\n        {relevant_global_context}\n\n        Relevant Indexed Information:\n        {relevant_indexed_info}\n\n        Code block:\n        {block['code']}\n\n        Provide a detailed analysis of this block, focusing on:\n        1. Its specific role and functionality\n        2. How it relates to its immediate dependencies (local context)\n        3. Its place in the overall structure (global context)\n        4. How it connects to or builds upon previously analyzed relevant blocks\n        5. Any notable patterns or potential issues\n        6. How it relates to the indexed information provided", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "When referencing previous blocks, global context, or indexed information, be explicit about how this current block relates to or differs from them.\n        Limit your analysis to the provided contexts and avoid speculating about parts of the code not mentioned here.\n        \"\"\"\n        analysis = await self.llm_api.analyze(block['code'], prompt)\n        return {\"name\": block['name'], \"type\": block['type'], \"code\": block['code'], \"analysis\": analysis}\n\n    def get_relevant_indexed_info(self, code: str) -> str:\n        search_results = self.indexer.search(code)\n        return json.dumps([{\"name\": r['name'], \"type\": r['type']} for r in search_results])", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "def get_block_context(self, block_name: str) -> str:\n        predecessors = list(self.dependency_graph.predecessors(block_name))\n        successors = list(self.dependency_graph.successors(block_name))\n        context = f\"This {block_name} is used by: {', '.join(predecessors)}\\n\" if predecessors else \"\"\n        context += f\"This {block_name} uses: {', '.join(successors)}\\n\" if successors else \"\"\n        return context\n        \n    \n    def compress_context(self, context: str) -> str:\n        # \u4f7f\u7528TF-IDF\u538b\u7f29\u4e0a\u4e0b\u6587\n        if not hasattr(self, 'tfidf_matrix'):\n            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform([context])\n        else:\n            self.tfidf_matrix = self.tfidf_vectorizer.transform([context])\n        \n        feature_names = self.tfidf_vectorizer.get_feature_names_out()\n        important_words = [feature_names[i] for i in self.tfidf_matrix.indices]\n        \n        # \u53ea\u4fdd\u7559TF-IDF\u503c\u6700\u9ad8\u7684\u524d10\u4e2a\u8bcd\n        compressed_context = ' '.join(important_words[:10])", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "important_words = [feature_names[i] for i in self.tfidf_matrix.indices]\n        \n        # \u53ea\u4fdd\u7559TF-IDF\u503c\u6700\u9ad8\u7684\u524d10\u4e2a\u8bcd\n        compressed_context = ' '.join(important_words[:10])\n        return compressed_context", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "def get_previous_context(self, current_block_index: int) -> str:\n        if not self.context_snapshots:\n            return \"No previous context available.\"\n        previous_analysis = self.context_snapshots[-1]\n        return f\"Previous block ({previous_analysis['name']}) analysis summary: {previous_analysis['analysis'][:200]}...\"\n\n    def save_context_snapshot(self, block_index: int, block_analysis: Dict[str, Any]):\n        self.context_snapshots.append(block_analysis)\n\n    async def update_global_context(self, block_analysis: Dict[str, Any]):\n        self.global_context[block_analysis['name']] = block_analysis['analysis'][:200]  # \u4fdd\u5b58\u7b80\u77ed\u6458\u8981\n        \n        if len(json.dumps(self.global_context)) > self.context_size_threshold:\n            await self.restructure_global_context()", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "def save_context_snapshot(self, block_index: int, block_analysis: Dict[str, Any]):\n        self.context_snapshots.append(block_analysis)\n        \n        # \u5982\u679c\u5feb\u7167\u6570\u91cf\u8fc7\u591a\uff0c\u53ef\u4ee5\u8003\u8651\u53ea\u4fdd\u7559\u6700\u8fd1\u7684\u51e0\u4e2a\n        if len(self.context_snapshots) > 5:\n            self.context_snapshots.pop(0)\n        \n        # \u5c06\u5feb\u7167\u4fdd\u5b58\u5230\u6587\u4ef6\uff08\u53ef\u9009\uff09\n        with open(f'context_snapshot_{block_index}.pkl', 'wb') as f:\n            pickle.dump(block_analysis, f)\n\n    def load_context_snapshot(self, block_index: int) -> Dict[str, Any]:\n        # \u4ece\u6587\u4ef6\u52a0\u8f7d\u5feb\u7167\uff08\u5982\u679c\u9700\u8981\uff09\n        with open(f'context_snapshot_{block_index}.pkl', 'rb') as f:\n            return pickle.load(f)", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "async def update_global_context(self, block_analysis: Dict[str, Any]):\n        self.global_context[block_analysis['name']] = block_analysis['analysis'][:200]  # \u4fdd\u5b58\u7b80\u77ed\u6458\u8981\n        \n        if len(json.dumps(self.global_context)) > self.context_size_threshold:\n            await self.restructure_global_context()\n            \n    async def analyze_block_with_context(self, block: Dict[str, Any], summary: str, block_index: int) -> Dict[str, Any]:\n        local_context = self.get_block_context(block['name'])\n        compressed_context = self.compress_context(local_context)\n        relevant_previous_contexts = self.get_relevant_previous_contexts(block['name'])\n        relevant_global_context = self.get_relevant_global_context(block['name'])\n        \n        prompt = f\"\"\"\n        Analyze this code block in the context of the following information:\n\n        Overall Summary:\n        {summary}\n\n        Block Type: {block['type']}\n        Block Name: {block['name']}", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "Overall Summary:\n        {summary}\n\n        Block Type: {block['type']}\n        Block Name: {block['name']}\n\n        Local Context:\n        {compressed_context}\n\n        Relevant Previous Contexts:\n        {relevant_previous_contexts}\n\n        Relevant Global Context:\n        {relevant_global_context}\n\n        Code block:\n        {block['code']}\n\n        Provide a detailed analysis of this block, focusing on:\n        1. Its specific role and functionality\n        2. How it relates to its immediate dependencies (local context)\n        3. Its place in the overall structure (global context)\n        4. How it connects to or builds upon previously analyzed relevant blocks\n        5. Any notable patterns or potential issues", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "When referencing previous blocks or global context, be explicit about how this current block relates to or differs from them.\n        Limit your analysis to the provided contexts and avoid speculating about parts of the code not mentioned here.\n        \"\"\"\n        analysis = await self.llm_api.analyze(block['code'], prompt)\n        return {\"name\": block['name'], \"type\": block['type'], \"code\": block['code'], \"analysis\": analysis}", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "def get_relevant_previous_contexts(self, block_name: str) -> str:\n        relevant_contexts = []\n        for predecessor in self.dependency_graph.predecessors(block_name):\n            if predecessor in self.block_summaries:\n                relevant_contexts.append(f\"{predecessor}: {self.block_summaries[predecessor]}\")\n        \n        # \u4e5f\u8003\u8651\u6700\u8fd1\u5206\u6790\u7684\u51e0\u4e2a\u5757\n        recent_blocks = list(self.context_snapshots)[-3:]  # \u83b7\u53d6\u6700\u8fd1\u76843\u4e2a\u5757\n        for block in recent_blocks:\n            if block['name'] != block_name and block['name'] not in relevant_contexts:\n                relevant_contexts.append(f\"{block['name']}: {block['analysis'][:200]}\")\n        \n        return \"\\n\".join(relevant_contexts)", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "def get_relevant_global_context(self, block_name: str) -> str:\n        if \"restructured_context\" in self.global_context:\n            return self.global_context[\"restructured_context\"]\n        \n        relevant_context = {}\n        for name, summary in self.global_context.items():\n            if name == block_name or name in self.dependency_graph.predecessors(block_name) or name in self.dependency_graph.successors(block_name):\n                relevant_context[name] = summary\n        \n        return json.dumps(relevant_context)\n\n    async def update_global_context(self, block_analysis: Dict[str, Any]):\n        self.global_context[block_analysis['name']] = block_analysis['analysis'][:200]\n        \n        if len(json.dumps(self.global_context)) > self.context_size_threshold:\n            await self.restructure_global_context()", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "async def restructure_global_context(self):\n        context_str = json.dumps(self.global_context)\n        prompt = f\"\"\"\n        The following is a collection of summaries for different parts of a large codebase:\n\n        {context_str}\n\n        Please restructure and consolidate this information into a more concise global context. \n        Focus on key components, their main purposes, and critical relationships. \n        Highlight how different parts of the code relate to each other and any overarching patterns or principles.\n        The restructured context should be about 50% of the original length.\n        \"\"\"\n        restructured_context = await self.llm_api.analyze(context_str, prompt)\n        self.global_context = {\"restructured_context\": restructured_context}", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "async def integrate_analyses(self, summary: str, block_analyses: List[Dict[str, Any]]) -> str:\n        context = {\n            \"summary\": summary,\n            \"project_summary\": self.project_summary,\n            \"block_analyses\": [{\"name\": ba['name'], \"type\": ba['type'], \"analysis\": ba['analysis']} for ba in block_analyses],\n            \"dependency_graph\": nx.node_link_data(self.dependency_graph),\n            \"global_context\": self.global_context,\n            \"index\": self.indexer.index\n        }\n        prompt = \"\"\"\n        Provide an integrated analysis of the entire codebase based on the following information:\n        \n        1. Overall summary\n        2. Project summary\n        3. Individual block analyses\n        4. Dependency relationships between different parts of the code\n        5. Global context\n        6. Code index", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "Focus on:\n        - The overall structure and architecture of the code\n        - Key components and their roles\n        - How different parts of the code interact with each other\n        - Any patterns or design principles evident in the code structure\n        - How the understanding of the code evolved as different blocks were analyzed\n        - Potential areas for improvement or refactoring\n        - How the indexed information contributes to understanding the codebase", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "Synthesize all this information to give a comprehensive understanding of the codebase, \n        highlighting how the different pieces fit together and any notable transitions or developments in the code structure.\n        Use the project summary and index to provide a high-level perspective on the codebase.\n        \"\"\"\n        return await self.llm_api.analyze(json.dumps(context), prompt)\n   \n    \n    \nasync def main():\n    api_key = \"your-api-key-here\"\n    llm_api = EnhancedLLMApi(api_key)\n    \n    large_file_path = \"/path/to/your/large/file.py\"\n    analyzer = LargeCodeAnalyzer(llm_api)\n    analysis_result = await analyzer.analyze_large_file(large_file_path)\n    \n    print(\"Project Summary:\")\n    print(analysis_result['project_summary'])\n    \n    print(\"\\nBlock Analyses:\")\n    for block_analysis in analysis_result['block_analyses']:\n        print(f\"\\n{block_analysis['type']} {block_analysis['name']}:\")\n        print(block_analysis['analysis'])", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "for block_analysis in analysis_result['block_analyses']:\n        print(f\"\\n{block_analysis['type']} {block_analysis['name']}:\")\n        print(block_analysis['analysis'])\n    \n    print(\"\\nFinal Integrated Analysis:\")\n    print(analysis_result['final_analysis'])\n    \n    print(\"\\nCode Index:\")\n    print(json.dumps(analysis_result['index'], indent=2))", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "if __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())", "file_path": "large_code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["api_key", "analyzer", "get_relevant_indexed_info", "key", "analyze_block_with_context", "__name__", "search", "block_summaries", "relevant_contexts", "final_analysis", "CodeIndexer", "load_context_snapshot", "update_global_context", "sorted_items", "successors", "analysis", "tfidf_vectorizer", "block_analyses", "recent_blocks", "query_vector", "similarity", "context_snapshots", "build_dependency_graph", "split_code", "previous_analysis", "maxlen", "global_context", "main", "int", "llm_api", "__init__", "search_results", "tree", "blocks", "analysis_result", "predecessors", "relevant_previous_contexts", "analyze_large_file", "dependency_graph", "reverse", "indexer", "relevant_indexed_info", "generate_project_summary", "block_analysis", "context_size_threshold", "restructured_context", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "add_to_index", "tfidf_matrix", "compressed_context", "context_str", "block", "results", "get_relevant_global_context", "relevant_global_context", "get_relevant_previous_contexts", "important_words", "name", "code_blocks", "get_block_context", "context", "local_context", "restructure_global_context", "index", "indent", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "prompt", "get_previous_context", "max_tokens", "large_file_path", "relevant_context", "extract_keywords", "compress_context"]}, {"content": "import ast\nfrom typing import List, Dict, Any\nfrom collections import deque\nfrom analyzers import CodeIndexer, BusinessLogicAnalyzer", "file_path": "Large_code_analyzer4.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Any\nfrom collections import deque\n", "symbols": ["api_key", "context_snapshots", "code_indexer", "analyzer", "__name__", "code_files", "maxlen", "business_logic_analysis", "content", "global_context", "main", "max_tokens", "int", "LargeCodeAnalyzer", "load_context_snapshot", "llm_api", "__init__", "search_code", "business_logic_analyzer", "analysis_result", "analyze_code", "encoding"]}, {"content": "class LargeCodeAnalyzer:\n    def __init__(self, llm_api):\n        self.llm_api = llm_api\n        self.max_tokens = 7500\n        self.context_snapshots = deque(maxlen=10)\n        self.global_context = {}\n        self.code_indexer = CodeIndexer()\n        self.business_logic_analyzer = BusinessLogicAnalyzer(llm_api)\n\n    async def analyze_code(self, code_files: List[str]) -> Dict[str, Any]:\n        \"\"\"\n        Analyze code files and extract insights.\n        \"\"\"\n        # Index all files for searching\n        for file_path in code_files:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                content = f.read()\n            self.code_indexer.add_to_index(file_path, content, \"file\")", "file_path": "Large_code_analyzer4.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Any\nfrom collections import deque\n", "symbols": ["api_key", "context_snapshots", "code_indexer", "analyzer", "__name__", "code_files", "maxlen", "business_logic_analysis", "content", "global_context", "main", "max_tokens", "int", "LargeCodeAnalyzer", "load_context_snapshot", "llm_api", "__init__", "search_code", "business_logic_analyzer", "analysis_result", "analyze_code", "encoding"]}, {"content": "# Analyze business logic across files\n        business_logic_analysis = await self.business_logic_analyzer.analyze_business_logic(code_files)\n        \n        # Store context snapshot\n        self.context_snapshots.append({\n            \"files_analyzed\": code_files,\n            \"business_logic\": business_logic_analysis\n        })\n\n        return {\n            \"business_logic_analysis\": business_logic_analysis,\n            \"indexed_files\": list(self.code_indexer.index.keys())\n        }\n\n    def search_code(self, query: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Search through indexed code using the query.\n        \"\"\"\n        return self.code_indexer.search(query)\n\n    def load_context_snapshot(self, snapshot_index: int = -1) -> Dict[str, Any]:\n        \"\"\"\n        Load a previous context snapshot.\n        \"\"\"\n        if not self.context_snapshots:\n            return {}\n        return list(self.context_snapshots)[snapshot_index]", "file_path": "Large_code_analyzer4.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Any\nfrom collections import deque\n", "symbols": ["api_key", "context_snapshots", "code_indexer", "analyzer", "__name__", "code_files", "maxlen", "business_logic_analysis", "content", "global_context", "main", "max_tokens", "int", "LargeCodeAnalyzer", "load_context_snapshot", "llm_api", "__init__", "search_code", "business_logic_analyzer", "analysis_result", "analyze_code", "encoding"]}, {"content": "# Example usage\nasync def main():\n    api_key = \"your-api-key-here\"\n    from LLMApi_2 import EnhancedLLMApi\n    \n    llm_api = EnhancedLLMApi(api_key)\n    analyzer = LargeCodeAnalyzer(llm_api)\n    \n    code_files = [\n        \"path/to/your/file1.py\",\n        \"path/to/your/file2.py\"\n    ]\n    \n    analysis_result = await analyzer.analyze_code(code_files)\n    print(analysis_result)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())", "file_path": "Large_code_analyzer4.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Any\nfrom collections import deque\n", "symbols": ["api_key", "context_snapshots", "code_indexer", "analyzer", "__name__", "code_files", "maxlen", "business_logic_analysis", "content", "global_context", "main", "max_tokens", "int", "LargeCodeAnalyzer", "load_context_snapshot", "llm_api", "__init__", "search_code", "business_logic_analyzer", "analysis_result", "analyze_code", "encoding"]}, {"content": "import ast\nimport json\nimport networkx as nx\nfrom typing import List, Dict, Any\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nimport pickle\nfrom collections import deque\n\nclass ContextAnchor:\n    def __init__(self, name: str, type: str, importance: float, summary: str):\n        self.name = name\n        self.type = type\n        self.importance = importance\n        self.summary = summary", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "class CodeIndexer:\n    def __init__(self):\n        self.index = {}\n        self.tfidf_vectorizer = TfidfVectorizer()\n\n    def add_to_index(self, name: str, content: str, type: str):\n        self.index[name] = {\n            \"content\": content,\n            \"type\": type,\n            \"keywords\": self.extract_keywords(content)\n        }\n\n    def extract_keywords(self, content: str, top_n: int = 5) -> List[str]:\n        tfidf_matrix = self.tfidf_vectorizer.fit_transform([content])\n        feature_names = self.tfidf_vectorizer.get_feature_names_out()\n        sorted_items = sorted(zip(tfidf_matrix.tocsc().data, feature_names), key=lambda x: x[0], reverse=True)\n        return [item[1] for item in sorted_items[:top_n]]", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "def search(self, query: str) -> List[Dict[str, Any]]:\n        query_vector = self.tfidf_vectorizer.transform([query])\n        results = []\n        for name, data in self.index.items():\n            content_vector = self.tfidf_vectorizer.transform([data['content']])\n            similarity = np.dot(query_vector.toarray(), content_vector.toarray().T)[0][0]\n            results.append({\"name\": name, \"type\": data['type'], \"similarity\": similarity})\n        return sorted(results, key=lambda x: x['similarity'], reverse=True)[:5]", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "class LargeCodeAnalyzer:\n    def __init__(self, llm_api):\n        self.llm_api = llm_api\n        self.max_tokens = 7500\n        self.dependency_graph = nx.DiGraph()\n        self.context_snapshots = deque(maxlen=10)\n        self.global_context = {}\n        self.context_size_threshold = 5000\n        self.block_summaries = {}\n        self.indexer = CodeIndexer()\n        self.project_summary = \"\"\n        self.context_anchors = []\n        self.critical_paths = []\n        self.local_contexts = {}", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "async def analyze_large_file(self, file_path: str) -> Dict[str, Any]:\n        with open(file_path, 'r') as file:\n            content = file.read()\n        \n        summary = await self.generate_summary(content)\n        self.build_dependency_graph(content)\n        code_blocks = self.split_code(content)\n        \n        # \u8bc6\u522b\u7a0b\u5e8f\u5165\u53e3\u70b9\n        entry_point = self.identify_entry_point(content)\n        entry_point_analysis = await self.analyze_entry_point(entry_point)\n        \n        # \u8bc6\u522b\u5173\u952e\u8def\u5f84\n        self.identify_critical_paths(entry_point)\n        \n        block_analyses = []\n        for i, block in enumerate(code_blocks):\n            block_analysis = await self.analyze_block_with_context(block, summary, i)\n            block_analyses.append(block_analysis)\n            self.save_context_snapshot(i, block_analysis)\n            await self.update_global_context(block_analysis)\n            self.update_local_context(block['name'], block_analysis)", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "self.save_context_snapshot(i, block_analysis)\n            await self.update_global_context(block_analysis)\n            self.update_local_context(block['name'], block_analysis)\n            self.block_summaries[block['name']] = block_analysis['analysis'][:200]\n            self.indexer.add_to_index(block['name'], block['code'], block['type'])\n            \n             # \u8bc6\u522b\u5e76\u6dfb\u52a0\u4e0a\u4e0b\u6587\u951a\u70b9\n            if self.is_context_anchor(block_analysis):\n                anchor = self.create_context_anchor(block_analysis)\n                self.context_anchors.append(anchor)\n            \n            if self.should_restructure_context():\n                await self.restructure_context()    \n        \n        self.project_summary = await self.generate_project_summary(summary, block_analyses)\n        final_analysis = await self.integrate_analyses(summary, block_analyses, entry_point_analysis)\n        \n        return {\n            \"summary\": summary,\n            \"project_summary\": self.project_summary,", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "return {\n            \"summary\": summary,\n            \"project_summary\": self.project_summary,\n            \"block_analyses\": block_analyses,\n            \"final_analysis\": final_analysis,\n            \"dependency_graph\": nx.node_link_data(self.dependency_graph),\n            \"global_context\": self.global_context,\n            \"index\": self.indexer.index,\n            \"context_anchors\": [vars(anchor) for anchor in self.context_anchors],\n            \"critical_paths\": self.critical_paths\n        }\n        \n    def should_restructure_context(self) -> bool:\n        total_context_size = (\n            len(json.dumps(self.global_context)) +\n            sum(len(json.dumps(ctx)) for ctx in self.local_contexts.values())\n        )\n        return total_context_size > self.context_size_threshold", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "async def restructure_context(self):\n        # \u91cd\u6784\u5168\u5c40\u4e0a\u4e0b\u6587\n        global_context_str = json.dumps(self.global_context)\n        restructured_global = await self.llm_api.analyze(global_context_str, \"\"\"\n        Restructure and consolidate this global context information. \n        Focus on key components, their main purposes, and critical relationships. \n        The restructured context should be about 50% of the original length.\n        \"\"\")\n        self.global_context = {\"restructured_global\": restructured_global}", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "# \u91cd\u6784\u5c40\u90e8\u4e0a\u4e0b\u6587\n        for block_name, local_context in self.local_contexts.items():\n            local_context_str = json.dumps(local_context)\n            restructured_local = await self.llm_api.analyze(local_context_str, f\"\"\"\n            Restructure and consolidate the local context for the block '{block_name}'.\n            Focus on the most important information about this block's functionality and relationships.\n            The restructured context should be about 50% of the original length.\n            \"\"\")\n            self.local_contexts[block_name] = {\"restructured_local\": restructured_local}", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "def update_local_context(self, block_name: str, block_analysis: Dict[str, Any]):\n        if block_name not in self.local_contexts:\n            self.local_contexts[block_name] = {}\n        self.local_contexts[block_name].update({\n            \"analysis\": block_analysis['analysis'][:200],\n            \"type\": block_analysis['type'],\n            \"dependencies\": list(self.dependency_graph.predecessors(block_name)),\n            \"dependents\": list(self.dependency_graph.successors(block_name))\n        })\n\n    \n    def identify_critical_paths(self, entry_point: str):\n        tree = ast.parse(entry_point)\n        main_calls = [node for node in ast.walk(tree) if isinstance(node, ast.Call)]\n        \n        for call in main_calls:\n            if isinstance(call.func, ast.Name):\n                start_node = call.func.id\n                path = self.trace_path(start_node)\n                if path:\n                    self.critical_paths.append(path)", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "def trace_path(self, start_node: str) -> List[str]:\n        path = []\n        visited = set()\n        queue = deque([(start_node, [start_node])])\n        \n        while queue:\n            current, current_path = queue.popleft()\n            if current in visited:\n                continue\n            visited.add(current)\n            path = current_path\n            \n            if current not in self.dependency_graph:\n                break\n            \n            for neighbor in self.dependency_graph.successors(current):\n                if neighbor not in visited:\n                    queue.append((neighbor, current_path + [neighbor]))\n        \n        return path", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "def identify_entry_point(self, content: str) -> str:\n        # \u4f7f\u7528AST\u6216\u6b63\u5219\u8868\u8fbe\u5f0f\u6765\u67e5\u627e if __name__ == \"__main__\": \u8bed\u53e5\n        # \u8fd9\u91cc\u4f7f\u7528\u4e00\u4e2a\u7b80\u5355\u7684\u5b57\u7b26\u4e32\u641c\u7d22\u4f5c\u4e3a\u793a\u4f8b\n        main_block_start = content.find('if __name__ == \"__main__\":')\n        if main_block_start != -1:\n            # \u63d0\u53d6main\u5757\u7684\u5185\u5bb9\n            main_block_end = content.find('\\n\\n', main_block_start)\n            return content[main_block_start:main_block_end if main_block_end != -1 else None]\n        return \"\" \n    \n    async def analyze_entry_point(self, entry_point: str) -> Dict[str, Any]:\n        if not entry_point:\n            return {\"analysis\": \"No clear entry point found in the code.\"}\n        \n        prompt = f\"\"\"\n        Analyze the following entry point of the Python program:\n\n        {entry_point}", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "{entry_point}\n\n        Focus on:\n        1. The main function or code block that is executed\n        2. Any command-line arguments or configuration being processed\n        3. The sequence of operations or function calls in the main execution flow\n        4. Any setup or initialization procedures\n        5. The overall purpose or functionality initiated from this entry point\n\n        Provide a concise analysis of how this entry point sets up and starts the program's execution.\n        \"\"\"\n        analysis = await self.llm_api.analyze(entry_point, prompt)\n        return {\"code\": entry_point, \"analysis\": analysis}\n        \n        \n    def is_context_anchor(self, block_analysis: Dict[str, Any]) -> bool:\n        # \u8fd9\u91cc\u53ef\u4ee5\u5b9e\u73b0\u66f4\u590d\u6742\u7684\u903b\u8f91\u6765\u5224\u65ad\u4e00\u4e2a\u4ee3\u7801\u5757\u662f\u5426\u5e94\u8be5\u6210\u4e3a\u4e0a\u4e0b\u6587\u951a\u70b9\n        # \u4f8b\u5982\uff0c\u57fa\u4e8e\u4ee3\u7801\u5757\u7684\u590d\u6742\u5ea6\u3001\u4f9d\u8d56\u5173\u7cfb\u6570\u91cf\u3001\u6216\u8005\u7279\u5b9a\u7684\u6a21\u5f0f\n        return (block_analysis['type'] == 'class' or \n                block_analysis['type'] == 'function' and len(self.dependency_graph.edges(block_analysis['name'])) > 3)", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "def create_context_anchor(self, block_analysis: Dict[str, Any]) -> ContextAnchor:\n        importance = self.calculate_anchor_importance(block_analysis)\n        return ContextAnchor(\n            name=block_analysis['name'],\n            type=block_analysis['type'],\n            importance=importance,\n            summary=block_analysis['analysis'][:200]\n        )\n\n    def calculate_anchor_importance(self, block_analysis: Dict[str, Any]) -> float:\n        # \u8fd9\u91cc\u53ef\u4ee5\u5b9e\u73b0\u66f4\u590d\u6742\u7684\u903b\u8f91\u6765\u8ba1\u7b97\u951a\u70b9\u7684\u91cd\u8981\u6027\n        # \u4f8b\u5982\uff0c\u57fa\u4e8e\u4f9d\u8d56\u5173\u7cfb\u7684\u6570\u91cf\u3001\u4ee3\u7801\u590d\u6742\u5ea6\u7b49\n        return len(self.dependency_graph.edges(block_analysis['name'])) * 0.1", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "async def generate_project_summary(self, file_summary: str, block_analyses: List[Dict[str, Any]]) -> str:\n        context = {\n            \"file_summary\": file_summary,\n            \"block_summaries\": [{\"name\": ba['name'], \"type\": ba['type'], \"summary\": ba['analysis'][:200]} for ba in block_analyses],\n            \"dependency_graph\": nx.node_link_data(self.dependency_graph)\n        }\n        prompt = \"\"\"\n        Based on the provided information, generate a comprehensive project summary that includes:\n        1. An overview of the project's purpose and main functionality\n        2. Key components and their roles\n        3. Important relationships and dependencies between components\n        4. Main algorithms or processes implemented\n        5. Notable design patterns or architectural choices\n        6. Potential areas of complexity or importance", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "This summary should serve as a high-level guide to understanding the project structure and functionality.\n        \"\"\"\n        return await self.llm_api.analyze(json.dumps(context), prompt)\n        \n    async def generate_summary(self, content: str) -> str:\n        tree = ast.parse(content)\n        summary = {\n            \"imports\": [node.names[0].name for node in ast.walk(tree) if isinstance(node, ast.Import)],\n            \"functions\": [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)],\n            \"classes\": [node.name for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]\n        }\n        return await self.llm_api.analyze(str(summary), \"Generate a high-level summary of this code structure.\")", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "def build_dependency_graph(self, content: str):\n        tree = ast.parse(content)\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n                self.dependency_graph.add_node(node.name)\n                for child_node in ast.walk(node):\n                    if isinstance(child_node, ast.Name) and isinstance(child_node.ctx, ast.Load):\n                        if child_node.id in self.dependency_graph:\n                            self.dependency_graph.add_edge(node.name, child_node.id)", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "def split_code(self, content: str) -> List[Dict[str, Any]]:\n        tree = ast.parse(content)\n        blocks = []\n        for node in ast.iter_child_nodes(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n                block = ast.get_source_segment(content, node)\n                if block:\n                    blocks.append({\n                        \"name\": node.name,\n                        \"type\": type(node).__name__,\n                        \"code\": block\n                    })\n        return blocks\n\n    \n\n    def get_relevant_indexed_info(self, code: str) -> str:\n        search_results = self.indexer.search(code)\n        return json.dumps([{\"name\": r['name'], \"type\": r['type']} for r in search_results])", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "def get_block_context(self, block_name: str) -> str:\n        predecessors = list(self.dependency_graph.predecessors(block_name))\n        successors = list(self.dependency_graph.successors(block_name))\n        context = f\"This {block_name} is used by: {', '.join(predecessors)}\\n\" if predecessors else \"\"\n        context += f\"This {block_name} uses: {', '.join(successors)}\\n\" if successors else \"\"\n        return context\n        \n    \n    def compress_context(self, context: str) -> str:\n        # \u4f7f\u7528TF-IDF\u538b\u7f29\u4e0a\u4e0b\u6587\n        if not hasattr(self, 'tfidf_matrix'):\n            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform([context])\n        else:\n            self.tfidf_matrix = self.tfidf_vectorizer.transform([context])\n        \n        feature_names = self.tfidf_vectorizer.get_feature_names_out()\n        important_words = [feature_names[i] for i in self.tfidf_matrix.indices]\n        \n        # \u53ea\u4fdd\u7559TF-IDF\u503c\u6700\u9ad8\u7684\u524d10\u4e2a\u8bcd\n        compressed_context = ' '.join(important_words[:10])", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "important_words = [feature_names[i] for i in self.tfidf_matrix.indices]\n        \n        # \u53ea\u4fdd\u7559TF-IDF\u503c\u6700\u9ad8\u7684\u524d10\u4e2a\u8bcd\n        compressed_context = ' '.join(important_words[:10])\n        return compressed_context", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "def get_previous_context(self, current_block_index: int) -> str:\n        if not self.context_snapshots:\n            return \"No previous context available.\"\n        previous_analysis = self.context_snapshots[-1]\n        return f\"Previous block ({previous_analysis['name']}) analysis summary: {previous_analysis['analysis'][:200]}...\"\n\n    def save_context_snapshot(self, block_index: int, block_analysis: Dict[str, Any]):\n        self.context_snapshots.append(block_analysis)\n\n    async def update_global_context(self, block_analysis: Dict[str, Any]):\n        self.global_context[block_analysis['name']] = block_analysis['analysis'][:200]  # \u4fdd\u5b58\u7b80\u77ed\u6458\u8981\n        \n        if len(json.dumps(self.global_context)) > self.context_size_threshold:\n            await self.restructure_global_context()", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "def save_context_snapshot(self, block_index: int, block_analysis: Dict[str, Any]):\n        self.context_snapshots.append(block_analysis)\n        \n        # \u5982\u679c\u5feb\u7167\u6570\u91cf\u8fc7\u591a\uff0c\u53ef\u4ee5\u8003\u8651\u53ea\u4fdd\u7559\u6700\u8fd1\u7684\u51e0\u4e2a\n        if len(self.context_snapshots) > 5:\n            self.context_snapshots.pop(0)\n        \n        # \u5c06\u5feb\u7167\u4fdd\u5b58\u5230\u6587\u4ef6\uff08\u53ef\u9009\uff09\n        with open(f'context_snapshot_{block_index}.pkl', 'wb') as f:\n            pickle.dump(block_analysis, f)\n\n    def load_context_snapshot(self, block_index: int) -> Dict[str, Any]:\n        # \u4ece\u6587\u4ef6\u52a0\u8f7d\u5feb\u7167\uff08\u5982\u679c\u9700\u8981\uff09\n        with open(f'context_snapshot_{block_index}.pkl', 'rb') as f:\n            return pickle.load(f)\n\n    async def update_global_context(self, block_analysis: Dict[str, Any]):\n        self.global_context[block_analysis['name']] = block_analysis['analysis'][:200]  # \u4fdd\u5b58\u7b80\u77ed\u6458\u8981\n        \n        if len(json.dumps(self.global_context)) > self.context_size_threshold:\n            await self.restructure_global_context()", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "def get_relevant_previous_contexts(self, block_name: str) -> str:\n        relevant_contexts = []\n        for predecessor in self.dependency_graph.predecessors(block_name):\n            if predecessor in self.block_summaries:\n                relevant_contexts.append(f\"{predecessor}: {self.block_summaries[predecessor]}\")\n        \n        # \u4e5f\u8003\u8651\u6700\u8fd1\u5206\u6790\u7684\u51e0\u4e2a\u5757\n        recent_blocks = list(self.context_snapshots)[-3:]  # \u83b7\u53d6\u6700\u8fd1\u76843\u4e2a\u5757\n        for block in recent_blocks:\n            if block['name'] != block_name and block['name'] not in relevant_contexts:\n                relevant_contexts.append(f\"{block['name']}: {block['analysis'][:200]}\")\n        \n        return \"\\n\".join(relevant_contexts)", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "def get_relevant_global_context(self, block_name: str) -> str:\n        if \"restructured_context\" in self.global_context:\n            return self.global_context[\"restructured_context\"]\n        \n        relevant_context = {}\n        for name, summary in self.global_context.items():\n            if name == block_name or name in self.dependency_graph.predecessors(block_name) or name in self.dependency_graph.successors(block_name):\n                relevant_context[name] = summary\n        \n        return json.dumps(relevant_context)\n    \n    def get_block_context(self, block_name: str) -> str:\n        return json.dumps(self.local_contexts.get(block_name, {}))\n\n    def get_relevant_global_context(self) -> str:\n        return json.dumps(self.global_context)", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "def get_relevant_global_context(self) -> str:\n        return json.dumps(self.global_context)\n\n    async def update_global_context(self, block_analysis: Dict[str, Any]):\n        self.global_context[block_analysis['name']] = block_analysis['analysis'][:200]\n        \n        if len(json.dumps(self.global_context)) > self.context_size_threshold:\n            await self.restructure_global_context()\n\n    async def restructure_global_context(self):\n        context_str = json.dumps(self.global_context)\n        prompt = f\"\"\"\n        The following is a collection of summaries for different parts of a large codebase:\n\n        {context_str}", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "Please restructure and consolidate this information into a more concise global context. \n        Focus on key components, their main purposes, and critical relationships. \n        Highlight how different parts of the code relate to each other and any overarching patterns or principles.\n        The restructured context should be about 50% of the original length.\n        \"\"\"\n        restructured_context = await self.llm_api.analyze(context_str, prompt)\n        self.global_context = {\"restructured_context\": restructured_context}\n        \n    def get_critical_path_info(self, block_name: str) -> str:\n        paths_info = []\n        for i, path in enumerate(self.critical_paths):\n            if block_name in path:\n                position = path.index(block_name)\n                prev = path[position - 1] if position > 0 else \"Start\"\n                next = path[position + 1] if position < len(path) - 1 else \"End\"", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "prev = path[position - 1] if position > 0 else \"Start\"\n                next = path[position + 1] if position < len(path) - 1 else \"End\"\n                paths_info.append(f\"Path {i + 1}: ... -> {prev} -> {block_name} -> {next} -> ...\")\n        \n        if paths_info:\n            return \"This block is part of the following critical paths:\\n\" + \"\\n\".join(paths_info)\n        else:\n            return \"This block is not part of any identified critical paths.\"    \n        \n    async def analyze_block_with_context(self, block: Dict[str, Any], summary: str, block_index: int) -> Dict[str, Any]:\n        local_context = self.get_block_context(block['name'])\n        compressed_context = self.compress_context(local_context)\n        relevant_global_context = self.get_relevant_global_context()\n        critical_path_info = self.get_critical_path_info(block['name'])\n        \n        prompt = f\"\"\"\n        Analyze this code block in the context of the following information:", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "Overall Summary:\n        {summary}\n\n        Block Type: {block['type']}\n        Block Name: {block['name']}\n\n        Local Context:\n        {compressed_context}\n\n        Relevant Global Context:\n        {relevant_global_context}\n\n        Critical Path Information:\n        {critical_path_info}\n\n        Code block:\n        {block['code']}\n\n        Provide a detailed analysis of this block, focusing on:\n        1. Its specific role and functionality\n        2. How it relates to its immediate dependencies and dependents\n        3. Its place in the overall structure (global context)\n        4. Its role in the critical execution paths of the program\n        5. Any notable patterns or potential issues", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "When referencing global context or critical paths, be explicit about how this current block relates to or differs from them.\n        Pay special attention to the block's role in any critical paths it's part of, and how this impacts the overall program flow.\n        Limit your analysis to the provided contexts and avoid speculating about parts of the code not mentioned here.\n        \"\"\"\n        analysis = await self.llm_api.analyze(block['code'], prompt)\n        return {\"name\": block['name'], \"type\": block['type'], \"code\": block['code'], \"analysis\": analysis}", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "def get_relevant_anchors(self, block_name: str) -> str:\n        relevant_anchors = []\n        for anchor in self.context_anchors:\n            if (anchor.name in self.dependency_graph.predecessors(block_name) or\n                anchor.name in self.dependency_graph.successors(block_name)):\n                relevant_anchors.append(f\"{anchor.name} ({anchor.type}): {anchor.summary}\")\n        return \"\\n\".join(relevant_anchors)", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "async def integrate_analyses(self, summary: str, block_analyses: List[Dict[str, Any]], entry_point_analysis: Dict[str, Any]) -> str:\n        context = {\n            \"summary\": summary,\n            \"project_summary\": self.project_summary,\n            \"entry_point_analysis\": entry_point_analysis,\n            \"block_analyses\": [{\"name\": ba['name'], \"type\": ba['type'], \"analysis\": ba['analysis']} for ba in block_analyses],\n            \"dependency_graph\": nx.node_link_data(self.dependency_graph),\n            \"global_context\": self.global_context,\n            \"index\": self.indexer.index,\n            \"context_anchors\": [vars(anchor) for anchor in self.context_anchors],\n            \"critical_paths\": self.critical_paths\n        }\n        prompt = \"\"\"\n        Provide an integrated analysis of the entire codebase based on the following information:\n        \n        1. Overall summary\n        2. Project summary\n        3. Entry point analysis\n        4. Individual block analyses", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "1. Overall summary\n        2. Project summary\n        3. Entry point analysis\n        4. Individual block analyses\n        5. Dependency relationships between different parts of the code\n        6. Global context\n        7. Code index\n        8. Context anchors\n        9. Critical execution paths", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "Focus on:\n        - The overall structure and architecture of the code\n        - The program's entry point and how it initiates the main functionality\n        - Key components and their roles, especially the identified context anchors\n        - How different parts of the code interact with each other\n        - The flow of execution from the entry point through the main components, particularly along critical paths\n        - Any patterns or design principles evident in the code structure\n        - How the understanding of the code evolved as different blocks were analyzed\n        - Potential areas for improvement or refactoring\n        - How the indexed information and context anchors contribute to understanding the codebase\n        - The significance of the identified critical paths and their impact on the program's functionality", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "Synthesize all this information to give a comprehensive understanding of the codebase, \n        highlighting how the different pieces fit together and any notable transitions or developments in the code structure.\n        Use the project summary, entry point analysis, index, context anchors, and critical paths to provide a high-level perspective on the codebase.\n        Pay special attention to how the critical paths reveal the core functionality and execution flow of the program.\n        \"\"\"\n        return await self.llm_api.analyze(json.dumps(context), prompt)\n    \n    async def integrate_analyses2(self, summary: str, block_analyses: List[Dict[str, Any]], entry_point_analysis: Dict[str, Any]) -> str:\n        context = {\n            \"summary\": summary,\n            \"project_summary\": self.project_summary,\n            \"entry_point_analysis\": entry_point_analysis,\n            \"global_context\": self.global_context,\n            \"critical_paths\": self.critical_paths\n        }", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "\"entry_point_analysis\": entry_point_analysis,\n            \"global_context\": self.global_context,\n            \"critical_paths\": self.critical_paths\n        }\n        prompt = \"\"\"\n        Provide an integrated analysis of the entire codebase based on the following information:\n        \n        1. Overall summary\n        2. Project summary\n        3. Entry point analysis\n        4. Global context\n        5. Critical execution paths", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "Focus on:\n        - The overall structure and architecture of the code\n        - The program's entry point and how it initiates the main functionality\n        - Key components and their roles\n        - How different parts of the code interact with each other\n        - The flow of execution from the entry point through the main components, particularly along critical paths\n        - Any patterns or design principles evident in the code structure\n        - Potential areas for improvement or refactoring\n        - The significance of the identified critical paths and their impact on the program's functionality", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "Synthesize all this information to give a comprehensive understanding of the codebase, \n        highlighting how the different pieces fit together and any notable transitions or developments in the code structure.\n        Pay special attention to how the critical paths reveal the core functionality and execution flow of the program.\n        \"\"\"\n        return await self.llm_api.analyze(json.dumps(context), prompt)", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "async def analyze_business_logic(self, file_paths: List[str]) -> Dict[str, Any]:\n        \"\"\"\n        Analyze business logic across multiple files in the codebase.\n        \n        Args:\n            file_paths: List of file paths to analyze\n            \n        Returns:\n            Dict containing business logic analysis results\n        \"\"\"\n        business_components = {}\n        \n        # First pass: Extract individual file business logic\n        for file_path in file_paths:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                content = f.read()\n                \n            # Parse AST to get function and class definitions\n            tree = ast.parse(content)\n            \n            # Extract business logic components from the file\n            file_components = await self._extract_file_business_logic(tree, file_path)\n            business_components[file_path] = file_components\n            \n            # Add to dependency graph", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "file_components = await self._extract_file_business_logic(tree, file_path)\n            business_components[file_path] = file_components\n            \n            # Add to dependency graph\n            self._build_dependency_graph(tree, file_path)\n            \n        # Second pass: Analyze cross-file relationships\n        business_flows = self._analyze_business_flows(business_components)\n        \n        # Use LLM to generate high-level business logic summary\n        summary_prompt = self._create_business_logic_summary_prompt(business_components, business_flows)\n        overall_summary = await self.llm_api.analyze_code(summary_prompt)\n        \n        return {\n            \"components\": business_components,\n            \"flows\": business_flows,\n            \"summary\": overall_summary,\n            \"dependency_graph\": self._serialize_graph()\n        }\n        \n    async def _extract_file_business_logic(self, tree: ast.AST, file_path: str) -> Dict[str, Any]:", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "\"dependency_graph\": self._serialize_graph()\n        }\n        \n    async def _extract_file_business_logic(self, tree: ast.AST, file_path: str) -> Dict[str, Any]:\n        \"\"\"Extract business logic components from a single file.\"\"\"\n        components = {\n            \"functions\": [],\n            \"classes\": [],\n            \"business_rules\": []\n        }\n        \n        for node in ast.walk(tree):\n            if isinstance(node, ast.FunctionDef):\n                # Analyze function\n                function_code = ast.get_source_segment(self._get_file_content(file_path), node)\n                function_prompt = f\"Analyze the business logic in this function:\\n{function_code}\"\n                function_analysis = await self.llm_api.analyze_code(function_prompt)\n                \n                components[\"functions\"].append({\n                    \"name\": node.name,\n                    \"logic\": function_analysis,\n                    \"code\": function_code\n                })", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "components[\"functions\"].append({\n                    \"name\": node.name,\n                    \"logic\": function_analysis,\n                    \"code\": function_code\n                })\n                \n            elif isinstance(node, ast.ClassDef):\n                # Analyze class\n                class_code = ast.get_source_segment(self._get_file_content(file_path), node)\n                class_prompt = f\"Analyze the business logic in this class:\\n{class_code}\"\n                class_analysis = await self.llm_api.analyze_code(class_prompt)\n                \n                components[\"classes\"].append({\n                    \"name\": node.name,\n                    \"logic\": class_analysis,\n                    \"code\": class_code\n                })\n        \n        return components\n    \n    def _build_dependency_graph(self, tree: ast.AST, file_path: str):\n        \"\"\"Build a dependency graph between files based on imports and function calls.\"\"\"\n        for node in ast.walk(tree):", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "def _build_dependency_graph(self, tree: ast.AST, file_path: str):\n        \"\"\"Build a dependency graph between files based on imports and function calls.\"\"\"\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.Import, ast.ImportFrom)):\n                # Add import relationships to graph\n                module_name = node.names[0].name if isinstance(node, ast.Import) else node.module\n                self.dependency_graph.add_edge(file_path, module_name)\n                \n    def _analyze_business_flows(self, components: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Analyze business flows across different files.\"\"\"\n        flows = []\n        visited = set()\n        \n        # Use graph traversal to identify business flows\n        for start_node in self.dependency_graph.nodes():\n            if start_node not in visited:\n                flow = self._trace_business_flow(start_node, components, visited)\n                if flow:", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "if start_node not in visited:\n                flow = self._trace_business_flow(start_node, components, visited)\n                if flow:\n                    flows.append(flow)\n                    \n        return flows\n    \n    def _trace_business_flow(self, start_node: str, components: Dict[str, Any], visited: set) -> Dict[str, Any]:\n        \"\"\"Trace a business flow starting from a specific node.\"\"\"\n        if start_node in visited or start_node not in components:\n            return None\n            \n        visited.add(start_node)\n        flow = {\n            \"start\": start_node,\n            \"steps\": [],\n            \"related_components\": []\n        }\n        \n        # Add components from the current file\n        file_components = components[start_node]\n        flow[\"related_components\"].extend(file_components[\"functions\"])\n        flow[\"related_components\"].extend(file_components[\"classes\"])\n        \n        # Follow dependencies", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "flow[\"related_components\"].extend(file_components[\"functions\"])\n        flow[\"related_components\"].extend(file_components[\"classes\"])\n        \n        # Follow dependencies\n        for next_node in self.dependency_graph.successors(start_node):\n            sub_flow = self._trace_business_flow(next_node, components, visited)\n            if sub_flow:\n                flow[\"steps\"].append(sub_flow)\n                \n        return flow\n    \n    def _create_business_logic_summary_prompt(self, components: Dict[str, Any], flows: List[Dict[str, Any]]) -> str:\n        \"\"\"Create a prompt for LLM to generate overall business logic summary.\"\"\"\n        prompt = \"Analyze the following business components and their relationships:\\n\\n\"\n        \n        # Add component information\n        for file_path, file_components in components.items():\n            prompt += f\"\\nFile: {file_path}\\n\"\n            prompt += \"Functions:\\n\"\n            for func in file_components[\"functions\"]:", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "for file_path, file_components in components.items():\n            prompt += f\"\\nFile: {file_path}\\n\"\n            prompt += \"Functions:\\n\"\n            for func in file_components[\"functions\"]:\n                prompt += f\"- {func['name']}: {func['logic']}\\n\"\n            prompt += \"Classes:\\n\"\n            for cls in file_components[\"classes\"]:\n                prompt += f\"- {cls['name']}: {cls['logic']}\\n\"\n                \n        # Add flow information\n        prompt += \"\\nBusiness Flows:\\n\"\n        for flow in flows:\n            prompt += f\"Flow starting from {flow['start']}:\\n\"\n            prompt += f\"- Related components: {', '.join(c['name'] for c in flow['related_components'])}\\n\"\n            \n        prompt += \"\\nProvide a comprehensive summary of the business logic, including:\\n\"\n        prompt += \"1. Main business components and their responsibilities\\n\"\n        prompt += \"2. Key business flows and their interactions\\n\"", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "prompt += \"1. Main business components and their responsibilities\\n\"\n        prompt += \"2. Key business flows and their interactions\\n\"\n        prompt += \"3. Important business rules and constraints\\n\"\n        \n        return prompt\n    \n    def _serialize_graph(self) -> Dict[str, Any]:\n        \"\"\"Serialize the dependency graph for output.\"\"\"\n        return {\n            \"nodes\": list(self.dependency_graph.nodes()),\n            \"edges\": list(self.dependency_graph.edges())\n        }\n        \n    def _get_file_content(self, file_path: str) -> str:\n        \"\"\"Helper method to get file content.\"\"\"\n        with open(file_path, 'r', encoding='utf-8') as f:\n            return f.read()", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "# \u4f7f\u7528\u793a\u4f8b\nasync def main():\n    api_key = \"your-api-key-here\"\n    llm_api = EnhancedLLMApi(api_key)\n    \n    large_file_path = \"/path/to/your/large/file.py\"\n    analyzer = LargeCodeAnalyzer(llm_api)\n    analysis_result = await analyzer.analyze_large_file(large_file_path)\n    \n    print(\"Project Summary:\")\n    print(analysis_result['project_summary'])\n    \n    print(\"\\nContext Anchors:\")\n    for anchor in analysis_result['context_anchors']:\n        print(f\"{anchor['name']} ({anchor['type']}) - Importance: {anchor['importance']}\")\n        print(f\"Summary: {anchor['summary']}\")\n        print()\n    \n    print(\"\\nFinal Integrated Analysis:\")\n    print(analysis_result['final_analysis'])\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())", "file_path": "large_code_analyzer_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport json\nimport networkx as nx\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "overall_summary", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "__name__", "update_local_context", "main_block_start", "_create_business_logic_summary_prompt", "search", "block_summaries", "entry_point_analysis", "relevant_contexts", "analyze_entry_point", "identify_entry_point", "final_analysis", "_trace_business_flow", "CodeIndexer", "load_context_snapshot", "module_name", "update_global_context", "sorted_items", "components", "successors", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "_get_file_content", "recent_blocks", "query_vector", "similarity", "business_components", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "previous_analysis", "maxlen", "_analyze_business_flows", "global_context", "get_relevant_anchors", "relevant_anchors", "_extract_file_business_logic", "main", "int", "context_anchors", "ContextAnchor", "llm_api", "__init__", "anchor", "tree", "trace_path", "search_results", "blocks", "analysis_result", "path", "predecessors", "entry_point", "analyze_business_logic", "encoding", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "local_contexts", "queue", "reverse", "integrate_analyses2", "summary_prompt", "indexer", "_build_dependency_graph", "class_code", "generate_project_summary", "current_path", "block_analysis", "flows", "calculate_anchor_importance", "context_size_threshold", "restructured_context", "sub_flow", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "flow", "tfidf_matrix", "compressed_context", "function_prompt", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "get_relevant_global_context", "relevant_global_context", "critical_paths", "get_relevant_previous_contexts", "important_words", "visited", "name", "code_blocks", "get_block_context", "context", "file_components", "local_context", "main_calls", "restructure_global_context", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "class_analysis", "prompt", "get_previous_context", "start_node", "function_analysis", "definitions", "identify_critical_paths", "max_tokens", "large_file_path", "total_context_size", "relevant_context", "business_flows", "class_prompt", "paths_info", "extract_keywords", "is_context_anchor", "compress_context", "_serialize_graph", "type", "local_context_str", "function_code"]}, {"content": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\nfrom collections import deque\nimport json\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n\nclass ContextAnchor:\n    def __init__(self, name: str, type: str, importance: float, summary: str):\n        self.name = name\n        self.type = type\n        self.importance = importance\n        self.summary = summary", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "class CodeIndexer:\n    def __init__(self):\n        self.index = {}\n        self.tfidf_vectorizer = TfidfVectorizer()\n\n    def add_to_index(self, name: str, content: str, type: str):\n        self.index[name] = {\n            \"content\": content,\n            \"type\": type,\n            \"keywords\": self.extract_keywords(content)\n        }\n\n    def extract_keywords(self, content: str, top_n: int = 5) -> List[str]:\n        tfidf_matrix = self.tfidf_vectorizer.fit_transform([content])\n        feature_names = self.tfidf_vectorizer.get_feature_names_out()\n        sorted_items = sorted(zip(tfidf_matrix.tocsc().data, feature_names), key=lambda x: x[0], reverse=True)\n        return [item[1] for item in sorted_items[:top_n]]", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "def search(self, query: str) -> List[Dict[str, Any]]:\n        query_vector = self.tfidf_vectorizer.transform([query])\n        results = []\n        for name, data in self.index.items():\n            content_vector = self.tfidf_vectorizer.transform([data['content']])\n            similarity = np.dot(query_vector.toarray(), content_vector.toarray().T)[0][0]\n            results.append({\"name\": name, \"type\": data['type'], \"similarity\": similarity})\n        return sorted(results, key=lambda x: x['similarity'], reverse=True)[:5]", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "class LargeCodeAnalyzer:\n    def __init__(self, llm_api):\n        self.llm_api = llm_api\n        self.max_tokens = 7500\n        self.dependency_graph = nx.DiGraph()\n        self.context_snapshots = deque(maxlen=10)\n        self.global_context = {}\n        self.local_contexts = {}\n        self.context_size_threshold = 5000\n        self.block_summaries = {}\n        self.indexer = CodeIndexer()\n        self.project_summary = \"\"\n        self.context_anchors = []\n        self.critical_paths = []", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "async def analyze_large_file(self, file_path: str) -> Dict[str, Any]:\n        with open(file_path, 'r') as file:\n            content = file.read()\n        \n        summary = await self.generate_summary(content)\n        self.build_dependency_graph(content)\n        code_blocks = self.split_code(content)\n        \n        entry_point = self.identify_entry_point(content)\n        entry_point_analysis = await self.analyze_entry_point(entry_point)\n        \n        self.identify_critical_paths(entry_point)\n        \n        block_analyses = []\n        for i, block in enumerate(code_blocks):\n            block_analysis = await self.analyze_block_with_context(block, summary, i)\n            block_analyses.append(block_analysis)\n            self.save_context_snapshot(i, block_analysis)\n            await self.update_global_context(block_analysis)\n            self.update_local_context(block['name'], block_analysis)\n            self.block_summaries[block['name']] = block_analysis['analysis'][:200]", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "self.update_local_context(block['name'], block_analysis)\n            self.block_summaries[block['name']] = block_analysis['analysis'][:200]\n            self.indexer.add_to_index(block['name'], block['code'], block['type'])\n            \n            if self.is_context_anchor(block_analysis):\n                anchor = self.create_context_anchor(block_analysis)\n                self.context_anchors.append(anchor)\n            \n            if self.should_restructure_context():\n                await self.restructure_context()\n        \n        self.project_summary = await self.generate_project_summary(summary, block_analyses)\n        final_analysis = await self.integrate_analyses(summary, block_analyses, entry_point_analysis)\n        \n        return {\n            \"summary\": summary,\n            \"project_summary\": self.project_summary,\n            \"entry_point_analysis\": entry_point_analysis,\n            \"block_analyses\": block_analyses,\n            \"final_analysis\": final_analysis,", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "\"entry_point_analysis\": entry_point_analysis,\n            \"block_analyses\": block_analyses,\n            \"final_analysis\": final_analysis,\n            \"dependency_graph\": nx.node_link_data(self.dependency_graph),\n            \"global_context\": self.global_context,\n            \"index\": self.indexer.index,\n            \"context_anchors\": [vars(anchor) for anchor in self.context_anchors],\n            \"critical_paths\": self.critical_paths\n        }", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "async def generate_summary(self, content: str) -> str:\n        tree = ast.parse(content)\n        summary = {\n            \"imports\": [node.names[0].name for node in ast.walk(tree) if isinstance(node, ast.Import)],\n            \"functions\": [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)],\n            \"classes\": [node.name for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]\n        }\n        return await self.llm_api.analyze(str(summary), \"Generate a high-level summary of this code structure.\")", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "def build_dependency_graph(self, content: str):\n        tree = ast.parse(content)\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n                self.dependency_graph.add_node(node.name)\n                for child_node in ast.walk(node):\n                    if isinstance(child_node, ast.Name) and isinstance(child_node.ctx, ast.Load):\n                        if child_node.id in self.dependency_graph:\n                            self.dependency_graph.add_edge(node.name, child_node.id)", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "def split_code(self, content: str) -> List[Dict[str, Any]]:\n        tree = ast.parse(content)\n        blocks = []\n        for node in ast.iter_child_nodes(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n                block = ast.get_source_segment(content, node)\n                if block:\n                    blocks.append({\n                        \"name\": node.name,\n                        \"type\": type(node).__name__,\n                        \"code\": block\n                    })\n        return blocks\n\n    def identify_entry_point(self, content: str) -> str:\n        main_block_start = content.find('if __name__ == \"__main__\":')\n        if main_block_start != -1:\n            main_block_end = content.find('\\n\\n', main_block_start)\n            return content[main_block_start:main_block_end if main_block_end != -1 else None]\n        return \"\"", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "async def analyze_entry_point(self, entry_point: str) -> Dict[str, Any]:\n        if not entry_point:\n            return {\"analysis\": \"No clear entry point found in the code.\"}\n        \n        prompt = f\"\"\"\n        Analyze the following entry point of the Python program:\n\n        {entry_point}\n\n        Focus on:\n        1. The main function or code block that is executed\n        2. Any command-line arguments or configuration being processed\n        3. The sequence of operations or function calls in the main execution flow\n        4. Any setup or initialization procedures\n        5. The overall purpose or functionality initiated from this entry point\n\n        Provide a concise analysis of how this entry point sets up and starts the program's execution.\n        \"\"\"\n        analysis = await self.llm_api.analyze(entry_point, prompt)\n        return {\"code\": entry_point, \"analysis\": analysis}", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "def identify_critical_paths(self, entry_point: str):\n        tree = ast.parse(entry_point)\n        main_calls = [node for node in ast.walk(tree) if isinstance(node, ast.Call)]\n        \n        for call in main_calls:\n            if isinstance(call.func, ast.Name):\n                start_node = call.func.id\n                path = self.trace_path(start_node)\n                if path:\n                    self.critical_paths.append(path)", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "def trace_path(self, start_node: str) -> List[str]:\n        path = []\n        visited = set()\n        queue = deque([(start_node, [start_node])])\n        \n        while queue:\n            current, current_path = queue.popleft()\n            if current in visited:\n                continue\n            visited.add(current)\n            path = current_path\n            \n            if current not in self.dependency_graph:\n                break\n            \n            for neighbor in self.dependency_graph.successors(current):\n                if neighbor not in visited:\n                    queue.append((neighbor, current_path + [neighbor]))\n        \n        return path", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "async def analyze_block_with_context(self, block: Dict[str, Any], summary: str, block_index: int) -> Dict[str, Any]:\n        local_context = self.get_block_context(block['name'])\n        compressed_context = self.compress_context(local_context)\n        relevant_global_context = self.get_relevant_global_context()\n        relevant_indexed_info = self.get_relevant_indexed_info(block['code'])\n        relevant_anchors = self.get_relevant_anchors(block['name'])\n        critical_path_info = self.get_critical_path_info(block['name'])\n        \n        prompt = f\"\"\"\n        Analyze this code block in the context of the following information:\n\n        Overall Summary:\n        {summary}\n\n        Block Type: {block['type']}\n        Block Name: {block['name']}\n\n        Local Context:\n        {compressed_context}\n\n        Relevant Global Context:\n        {relevant_global_context}\n\n        Relevant Indexed Information:\n        {relevant_indexed_info}", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "Local Context:\n        {compressed_context}\n\n        Relevant Global Context:\n        {relevant_global_context}\n\n        Relevant Indexed Information:\n        {relevant_indexed_info}\n\n        Relevant Context Anchors:\n        {relevant_anchors}\n\n        Critical Path Information:\n        {critical_path_info}\n\n        Code block:\n        {block['code']}\n\n        Provide a detailed analysis of this block, focusing on:\n        1. Its specific role and functionality\n        2. How it relates to its immediate dependencies and dependents\n        3. Its place in the overall structure (global context)\n        4. Its relationship to the identified context anchors\n        5. Its role in the critical execution paths of the program\n        6. Any notable patterns or potential issues\n        7. How it relates to the indexed information provided", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "When referencing global context, indexed information, context anchors, or critical paths, be explicit about how this current block relates to or differs from them.\n        Pay special attention to the block's role in any critical paths it's part of, and how this impacts the overall program flow.\n        Limit your analysis to the provided contexts and avoid speculating about parts of the code not mentioned here.\n        \"\"\"\n        analysis = await self.llm_api.analyze(block['code'], prompt)\n        return {\"name\": block['name'], \"type\": block['type'], \"code\": block['code'], \"analysis\": analysis}\n\n    def get_block_context(self, block_name: str) -> str:\n        return json.dumps(self.local_contexts.get(block_name, {}))", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "def get_block_context(self, block_name: str) -> str:\n        return json.dumps(self.local_contexts.get(block_name, {}))\n\n    def compress_context(self, context: str) -> str:\n        # \u4f7f\u7528TF-IDF\u538b\u7f29\u4e0a\u4e0b\u6587\n        if not hasattr(self, 'tfidf_matrix'):\n            self.tfidf_matrix = self.indexer.tfidf_vectorizer.fit_transform([context])\n        else:\n            self.tfidf_matrix = self.indexer.tfidf_vectorizer.transform([context])\n        \n        feature_names = self.indexer.tfidf_vectorizer.get_feature_names_out()\n        important_words = [feature_names[i] for i in self.tfidf_matrix.indices]\n        \n        # \u53ea\u4fdd\u7559TF-IDF\u503c\u6700\u9ad8\u7684\u524d20\u4e2a\u8bcd\n        compressed_context = ' '.join(important_words[:20])\n        return compressed_context\n\n    def get_relevant_global_context(self) -> str:\n        return json.dumps(self.global_context)", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "def get_relevant_global_context(self) -> str:\n        return json.dumps(self.global_context)\n\n    def get_relevant_indexed_info(self, code: str) -> str:\n        search_results = self.indexer.search(code)\n        return json.dumps([{\"name\": r['name'], \"type\": r['type']} for r in search_results])\n\n    def get_relevant_anchors(self, block_name: str) -> str:\n        relevant_anchors = []\n        for anchor in self.context_anchors:\n            if (anchor.name in self.dependency_graph.predecessors(block_name) or\n                anchor.name in self.dependency_graph.successors(block_name)):\n                relevant_anchors.append(f\"{anchor.name} ({anchor.type}): {anchor.summary}\")\n        return \"\\n\".join(relevant_anchors)", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "def get_critical_path_info(self, block_name: str) -> str:\n        paths_info = []\n        for i, path in enumerate(self.critical_paths):\n            if block_name in path:\n                position = path.index(block_name)\n                prev = path[position - 1] if position > 0 else \"Start\"\n                next = path[position + 1] if position < len(path) - 1 else \"End\"\n                paths_info.append(f\"Path {i + 1}: ... -> {prev} -> {block_name} -> {next} -> ...\")\n        \n        if paths_info:\n            return \"This block is part of the following critical paths:\\n\" + \"\\n\".join(paths_info)\n        else:\n            return \"This block is not part of any identified critical paths.\"\n\n    def save_context_snapshot(self, block_index: int, block_analysis: Dict[str, Any]):\n        self.context_snapshots.append(block_analysis)", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "def save_context_snapshot(self, block_index: int, block_analysis: Dict[str, Any]):\n        self.context_snapshots.append(block_analysis)\n\n    async def update_global_context(self, block_analysis: Dict[str, Any]):\n        self.global_context[block_analysis['name']] = block_analysis['analysis'][:200]\n\n    def update_local_context(self, block_name: str, block_analysis: Dict[str, Any]):\n        if block_name not in self.local_contexts:\n            self.local_contexts[block_name] = {}\n        self.local_contexts[block_name].update({\n            \"analysis\": block_analysis['analysis'][:200],\n            \"type\": block_analysis['type'],\n            \"dependencies\": list(self.dependency_graph.predecessors(block_name)),\n            \"dependents\": list(self.dependency_graph.successors(block_name))\n        })", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "def should_restructure_context(self) -> bool:\n        total_context_size = (\n            len(json.dumps(self.global_context)) +\n            sum(len(json.dumps(ctx)) for ctx in self.local_contexts.values())\n        )\n        return total_context_size > self.context_size_threshold\n\n    async def restructure_context(self):\n        global_context_str = json.dumps(self.global_context)\n        restructured_global = await self.llm_api.analyze(global_context_str, \"\"\"\n        Restructure and consolidate this global context information. \n        Focus on key components, their main purposes, and critical relationships. \n        The restructured context should be about 50% of the original length.\n        \"\"\")\n        self.global_context = {\"restructured_global\": restructured_global}", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "for block_name, local_context in self.local_contexts.items():\n            local_context_str = json.dumps(local_context)\n            restructured_local = await self.llm_api.analyze(local_context_str, f\"\"\"\n            Restructure and consolidate the local context for the block '{block_name}'.\n            Focus on the most important information about this block's functionality and relationships.\n            The restructured context should be about 50% of the original length.\n            \"\"\")\n            self.local_contexts[block_name] = {\"restructured_local\": restructured_local}\n\n    def is_context_anchor(self, block_analysis: Dict[str, Any]) -> bool:\n        return (block_analysis['type'] == 'ClassDef' or \n                (block_analysis['type'] == 'FunctionDef' and len(self.dependency_graph.edges(block_analysis['name'])) > 3))", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "def create_context_anchor(self, block_analysis: Dict[str, Any]) -> ContextAnchor:\n        importance = self.calculate_anchor_importance(block_analysis)\n        return ContextAnchor(\n            name=block_analysis['name'],\n            type=block_analysis['type'],\n            importance=importance,\n            summary=block_analysis['analysis'][:200]\n        )\n\n    def calculate_anchor_importance(self, block_analysis: Dict[str, Any]) -> float:\n        return len(self.dependency_graph.edges(block_analysis['name'])) * 0.1", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "async def generate_project_summary(self, file_summary: str, block_analyses: List[Dict[str, Any]]) -> str:\n        context = {\n            \"file_summary\": file_summary,\n            \"block_summaries\": [{\"name\": ba['name'], \"type\": ba['type'], \"summary\": ba['analysis'][:200]} for ba in block_analyses],\n            \"dependency_graph\": nx.node_link_data(self.dependency_graph)\n        }\n        prompt = \"\"\"\n        Based on the provided information, generate a comprehensive project summary that includes:\n        1. An overview of the project's purpose and main functionality\n        2. Key components and their roles\n        3. Important relationships and dependencies between components\n        4. Main algorithms or processes implemented\n        5. Notable design patterns or architectural choices\n        6. Potential areas of complexity or importance", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "This summary should serve as a high-level guide to understanding the project structure and functionality.\n        \"\"\"\n        return await self.llm_api.analyze(json.dumps(context), prompt)\n\n    async def integrate_analyses(self, summary: str, block_analyses: List[Dict[str, Any]], entry_point_analysis: Dict[str, Any]) -> str:\n        context = {\n            \"summary\": summary,\n            \"project_summary\": self.project_summary,\n            \"entry_point_analysis\": entry_point_analysis,\n            \"global_context\": self.global_context,\n            \"critical_paths\": self.critical_paths,\n            \"context_anchors\": [vars(anchor) for anchor in self.context_anchors]\n        }\n        prompt = \"\"\"\n        Provide an integrated analysis of the entire codebase based on the following information:\n        \n        1. Overall summary\n        2. Project summary\n        3. Entry point analysis\n        4. Global context\n        5. Critical execution paths\n        6. Context anchors", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "Focus on:\n        - The overall structure and architecture of the code\n        - The program's entry point and how it initiates the main functionality\n        - Key components and their roles, especially the identified context anchors\n        - How different parts of the code interact with each other\n        - The flow of execution from the entry point through the main components, particularly along critical paths\n        - Any patterns or design principles evident in the code structure\n        - Potential areas for improvement or refactoring\n        - The significance of the identified critical paths and their impact on the program's functionality", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "Synthesize all this information to give a comprehensive understanding of the codebase, \n        highlighting how the different pieces fit together and any notable transitions or developments in the code structure.\n        Use the project summary, entry point analysis, and context anchors to provide a high-level perspective on the codebase.\n        Pay special attention to how the critical paths reveal the core functionality and execution flow of the program.\n        \"\"\"\n        return await self.llm_api.analyze(json.dumps(context), prompt)\n    \nimport openai\nimport asyncio\nimport json\nfrom typing import Dict, Any, List\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "class EnhancedLLMApi:\n    def __init__(self, api_key: str, model: str = \"gpt-4\"):\n        self.api_key = api_key\n        self.model = model\n        openai.api_key = self.api_key\n        self.conversation_history = []\n        self.max_history_length = 10\n        self.token_limit = 8000  # Adjust based on the model's actual limit", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(5))\n    async def analyze(self, content: str, prompt: str) -> str:\n        # Prepare the messages\n        messages = self._prepare_messages(content, prompt)\n        \n        # Make the API call\n        try:\n            response = await openai.ChatCompletion.acreate(\n                model=self.model,\n                messages=messages,\n                max_tokens=1000,\n                n=1,\n                stop=None,\n                temperature=0.5,\n            )\n            analysis = response.choices[0].message['content'].strip()\n            \n            # Update conversation history\n            self._update_conversation_history(prompt, analysis)\n            \n            return analysis\n        except Exception as e:\n            print(f\"Error in API call: {str(e)}\")\n            raise", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "def _prepare_messages(self, content: str, prompt: str) -> List[Dict[str, str]]:\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are an AI assistant specialized in analyzing Python code.\"},\n            {\"role\": \"user\", \"content\": f\"Here's the code or context to analyze:\\n\\n{content}\\n\\nAnalysis prompt: {prompt}\"}\n        ]\n        \n        # Add relevant conversation history\n        messages.extend(self.conversation_history)\n        \n        return messages\n\n    def _update_conversation_history(self, prompt: str, response: str):\n        self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n        \n        # Limit the history length\n        if len(self.conversation_history) > self.max_history_length:\n            self.conversation_history = self.conversation_history[-self.max_history_length:]", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "async def analyze_with_context(self, content: str, prompt: str, context: Dict[str, Any]) -> str:\n        context_str = json.dumps(context)\n        full_prompt = f\"{prompt}\\n\\nAdditional context: {context_str}\"\n        return await self.analyze(content, full_prompt)\n\n    async def summarize(self, text: str, max_length: int = 200) -> str:\n        prompt = f\"Summarize the following text in about {max_length} words:\\n\\n{text}\"\n        return await self.analyze(\"\", prompt)\n\n    async def compare_code_blocks(self, block1: str, block2: str) -> str:\n        prompt = f\"Compare and contrast the following two code blocks:\\n\\nBlock 1:\\n{block1}\\n\\nBlock 2:\\n{block2}\\n\\nFocus on their functionality, structure, and any notable differences or similarities.\"\n        return await self.analyze(\"\", prompt)", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "async def identify_design_patterns(self, code: str) -> List[str]:\n        prompt = \"Identify any design patterns used in the following code. List the patterns and briefly explain how they are implemented:\"\n        analysis = await self.analyze(code, prompt)\n        # This is a simple extraction. In a real scenario, you might want to use more sophisticated NLP techniques.\n        patterns = [line.split(':')[0] for line in analysis.split('\\n') if ':' in line]\n        return patterns\n\n    async def suggest_improvements(self, code: str) -> str:\n        prompt = \"Analyze the following code and suggest potential improvements in terms of efficiency, readability, and best practices:\"\n        return await self.analyze(code, prompt)\n\n    async def explain_code_section(self, code: str, section_name: str) -> str:\n        prompt = f\"Explain the purpose and functionality of the '{section_name}' section in the following code:\"\n        return await self.analyze(code, prompt)", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "async def analyze_complexity(self, code: str) -> Dict[str, Any]:\n        prompt = \"Analyze the complexity of the following code. Consider time complexity, space complexity, and cognitive complexity. Provide a brief explanation for each:\"\n        analysis = await self.analyze(code, prompt)\n        \n        # This is a simple parsing. In a real scenario, you might want to use more sophisticated techniques.\n        complexity = {}\n        current_key = \"\"\n        for line in analysis.split('\\n'):\n            if ':' in line:\n                key, value = line.split(':', 1)\n                current_key = key.strip().lower().replace(' ', '_')\n                complexity[current_key] = value.strip()\n            elif current_key:\n                complexity[current_key] += ' ' + line.strip()\n        \n        return complexity", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "async def generate_unit_tests(self, code: str) -> str:\n        prompt = \"Generate unit tests for the following code. Include test cases for normal operation, edge cases, and potential error conditions:\"\n        return await self.analyze(code, prompt)", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "async def analyze_security(self, code: str) -> List[Dict[str, str]]:\n        prompt = \"Analyze the following code for potential security vulnerabilities. Identify any issues and suggest mitigations:\"\n        analysis = await self.analyze(code, prompt)\n        \n        # Simple parsing of the analysis. In a real scenario, you might want to use more sophisticated techniques.\n        vulnerabilities = []\n        current_vuln = {}\n        for line in analysis.split('\\n'):\n            if line.startswith('Vulnerability:'):\n                if current_vuln:\n                    vulnerabilities.append(current_vuln)\n                current_vuln = {\"type\": line.split(':', 1)[1].strip()}\n            elif line.startswith('Mitigation:'):\n                current_vuln[\"mitigation\"] = line.split(':', 1)[1].strip()\n        if current_vuln:\n            vulnerabilities.append(current_vuln)\n        \n        return vulnerabilities", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "# Usage example\nasync def main():\n    api_key = \"your-openai-api-key-here\"\n    llm_api = EnhancedLLMApi(api_key)\n    \n    code = \"\"\"\n    def fibonacci(n):\n        if n <= 1:\n            return n\n        else:\n            return fibonacci(n-1) + fibonacci(n-2)\n    \"\"\"\n    \n    analysis = await llm_api.analyze(code, \"Explain this function and suggest any improvements.\")\n    print(\"Analysis:\", analysis)\n    \n    summary = await llm_api.summarize(code)\n    print(\"Summary:\", summary)\n    \n    patterns = await llm_api.identify_design_patterns(code)\n    print(\"Design Patterns:\", patterns)\n    \n    complexity = await llm_api.analyze_complexity(code)\n    print(\"Complexity Analysis:\", complexity)\n    \n    security_analysis = await llm_api.analyze_security(code)\n    print(\"Security Analysis:\", security_analysis)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "# \u4f7f\u7528\u793a\u4f8b\nasync def main():\n    api_key = \"your-api-key-here\"\n    llm_api = EnhancedLLMApi(api_key)  # \u5047\u8bbe\u60a8\u6709\u4e00\u4e2a EnhancedLLMApi \u7c7b\n    \n    large_file_path = \"/path/to/your/large/file.py\"\n    analyzer = LargeCodeAnalyzer(llm_api)\n    analysis_result = await analyzer.analyze_large_file(large_file_path)\n    \n    print(\"Project Summary:\")\n    print(analysis_result['project_summary'])\n    \n    print(\"\\nEntry Point Analysis:\")\n    print(analysis_result['entry_point_analysis']['analysis'])\n    \n    print(\"\\nContext Anchors:\")\n    for anchor in analysis_result['context_anchors']:\n        print(f\"{anchor['name']} ({anchor['type']}) - Importance: {anchor['importance']}\")\n        print(f\"Summary: {anchor['summary']}\")\n        print()\n    \n    print(\"\\nCritical Paths:\")\n    for i, path in enumerate(analysis_result['critical_paths']):\n        print(f\"Path {i + 1}: {' -> '.join(path)}\")\n    \n    print(\"\\nFinal Integrated Analysis:\")\n    print(analysis_result['final_analysis'])", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "if __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())", "file_path": "large_code_analyzer_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n", "symbols": ["restructure_context", "api_key", "analyzer", "critical_path_info", "get_relevant_indexed_info", "key", "analyze_block_with_context", "create_context_anchor", "max", "temperature", "main_block_start", "update_local_context", "__name__", "search", "block_summaries", "full_prompt", "entry_point_analysis", "analyze_entry_point", "identify_entry_point", "final_analysis", "analyze", "CodeIndexer", "summarize", "update_global_context", "current_key", "response", "generate_unit_tests", "sorted_items", "analysis", "global_context_str", "tfidf_vectorizer", "block_analyses", "query_vector", "similarity", "explain_code_section", "context_snapshots", "build_dependency_graph", "split_code", "should_restructure_context", "conversation_history", "_prepare_messages", "maxlen", "global_context", "get_relevant_anchors", "relevant_anchors", "main", "suggest_improvements", "int", "ContextAnchor", "context_anchors", "security_analysis", "llm_api", "analyze_security", "n", "token_limit", "min", "__init__", "anchor", "tree", "trace_path", "search_results", "EnhancedLLMApi", "blocks", "model", "analysis_result", "_update_conversation_history", "path", "analyze_with_context", "entry_point", "get_critical_path_info", "analyze_large_file", "importance", "dependency_graph", "str", "local_contexts", "vulnerabilities", "queue", "reverse", "indexer", "compare_code_blocks", "relevant_indexed_info", "generate_project_summary", "current_path", "block_analysis", "calculate_anchor_importance", "context_size_threshold", "analyze_complexity", "restructured_local", "content_vector", "generate_summary", "LargeCodeAnalyzer", "feature_names", "summary", "restructured_global", "add_to_index", "tfidf_matrix", "compressed_context", "next", "prev", "context_str", "block", "results", "position", "main_block_end", "messages", "patterns", "get_relevant_global_context", "relevant_global_context", "critical_paths", "fibonacci", "important_words", "max_history_length", "visited", "complexity", "name", "code_blocks", "get_block_context", "current_vuln", "context", "identify_design_patterns", "local_context", "main_calls", "index", "integrate_analyses", "content", "save_context_snapshot", "project_summary", "stop", "code", "prompt", "start_node", "max_tokens", "identify_critical_paths", "large_file_path", "total_context_size", "value", "paths_info", "extract_keywords", "wait", "is_context_anchor", "compress_context", "type", "local_context_str"]}, {"content": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\nimport json\nfrom typing import List, Dict, Any", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "class LLMApi:\n    def __init__(self, api_key: str = None, model: str = \"gpt-3.5-turbo\"):\n        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\n                \"OpenAI API key is required. Set it as an environment variable or pass it to the constructor.\"\n            )\n        openai.api_key = self.api_key\n        self.model = model\n\n    @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(5))\n    async def _call_openai_api(self, messages: List[Dict[str, str]]) -> str:\n        try:\n            response = await openai.ChatCompletion.acreate(\n                model=self.model, messages=messages\n            )\n            return response.choices[0].message.content.strip()\n        except openai.error.OpenAIError as e:\n            print(f\"OpenAI API error: {e}\")\n            raise", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "async def analyze(self, content: str, prompt: str) -> str:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are an elite code analyzer and business process consultant with extensive experience in software architecture, business analysis, and industry best practices. Your expertise spans multiple programming languages, design patterns, and architectural styles. Your task is to perform in-depth analysis of code and extract meaningful business processes, logic flows, and architectural insights.", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "Key responsibilities:\n1. Identify and explain core business processes embedded in the code.\n2. Analyze the overall architecture and suggest improvements.\n3. Detect potential performance bottlenecks or scalability issues.\n4. Evaluate code maintainability and suggest refactoring opportunities.\n5. Identify security vulnerabilities or data privacy concerns.\n6. Assess the effectiveness of error handling and logging mechanisms.\n7. Examine test coverage and suggest areas for improved testing.", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "Provide your analysis in a structured format, using markdown for better readability. Use code snippets or pseudocode where appropriate to illustrate your points.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Analyze the following code or content and {prompt}. Provide a comprehensive, structured analysis addressing the key responsibilities outlined in your role. Be specific, use examples from the code where relevant, and provide actionable recommendations.\\n\\nContent to analyze:\\n```\\n{content}\\n```\",\n            },\n        ]\n        return await self._call_openai_api(messages)", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "async def generate_flowchart(self, description: str) -> str:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are a world-class expert in creating detailed, accurate flowcharts from textual descriptions of business processes and code logic. Your flowcharts are known for their clarity, precision, and adherence to standard notation.\n\nKey responsibilities:\n1. Accurately represent the logical flow of processes described in the text.\n2. Use standard flowchart symbols (e.g., process, decision, input/output, start/end).\n3. Clearly label each step and decision point.\n4. Include all relevant branches and error handling paths.\n5. Optimize the layout for readability and logical flow.\n6. Add brief annotations where necessary to clarify complex steps.", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "Provide your flowchart as a series of steps in a structured text format that can be easily converted to a visual representation. Use markdown for formatting.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Based on the following description, create a detailed flowchart. Ensure that your flowchart captures all key processes, decision points, and data flows. Include any error handling or alternative paths that are crucial to understanding the overall process.\\n\\nDescription:\\n{description}\",\n            },\n        ]\n        return await self._call_openai_api(messages)", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "async def extract_business_logic(self, code: str) -> str:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are a renowned expert in extracting and interpreting business logic from source code. Your ability to distill complex code into clear business rules and processes is unparalleled. You have a deep understanding of how technical implementations map to real-world business operations.\n\nKey responsibilities:\n1. Identify and explain key business rules embedded in the code.\n2. Detect core business processes and their implementations.\n3. Recognize and interpret domain-specific logic and terminology.\n4. Identify data transformations and their business significance.\n5. Highlight decision points and their implications for business flow.\n6. Recognize patterns that indicate specific business operations or requirements.\n7. Explain technical implementations in business-friendly language.", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "Present your findings in a structured format, using markdown for clarity. Use bullet points, numbered lists, or tables where appropriate to organize information.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Examine the following code and extract the core business logic. Focus on identifying and explaining the key business rules, processes, and decision points. Relate technical implementations to real-world business concepts and operations.\\n\\nCode to analyze:\\n```\\n{code}\\n```\",\n            },\n        ]\n        return await self._call_openai_api(messages)", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "async def analyze_code_quality(self, pylint_report: str) -> str:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are a distinguished expert in code quality analysis with an encyclopedic knowledge of software engineering best practices, design patterns, and common antipatterns. Your assessments are known for their depth, accuracy, and actionable insights.\n\nKey responsibilities:\n1. Interpret static code analysis reports and provide a holistic quality assessment.\n2. Identify critical issues that may impact code reliability, performance, or security.\n3. Recognize patterns of code smells and suggest refactoring strategies.\n4. Assess code complexity and suggest simplification techniques where applicable.\n5. Evaluate adherence to coding standards and best practices.\n6. Identify potential maintenance challenges and suggest preventive measures.\n7. Prioritize issues based on their potential impact and effort to resolve.", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "Present your analysis in a structured format, using markdown for readability. Use tables or charts where appropriate to summarize findings.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Analyze the following Pylint report and provide a comprehensive assessment of the code quality. Identify major issues, potential risks, and areas for improvement. Prioritize your findings and offer specific, actionable recommendations for addressing each issue.\\n\\nPylint report:\\n```\\n{pylint_report}\\n```\",\n            },\n        ]\n        return await self._call_openai_api(messages)", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "async def analyze_code_evolution(self, evolution_data: List[Dict[str, Any]]) -> str:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are a leading expert in analyzing code evolution and software project dynamics. Your insights into development trends and project health indicators are highly valued in the software industry. You have a keen ability to spot patterns and potential issues in a project's lifecycle.\n\nKey responsibilities:\n1. Analyze trends in code complexity, size, and composition over time.\n2. Identify patterns in development velocity and team productivity.\n3. Recognize signs of technical debt accumulation or reduction.\n4. Detect shifts in coding practices or architectural decisions.\n5. Assess the impact of major refactoring or feature addition events.\n6. Evaluate the effectiveness of testing and quality assurance practices over time.\n7. Provide strategic recommendations for future development directions.", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "Present your analysis in a structured format, using markdown for clarity. Use charts or graphs (described textually) where appropriate to illustrate trends.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Examine the following code evolution data and provide a detailed analysis of how the codebase has changed over time. Focus on identifying significant trends, patterns, or events that indicate changes in development practices, code health, or project direction. Offer insights into the project's overall health and suggestions for future development strategies.\\n\\nEvolution data:\\n```json\\n{json.dumps(evolution_data, indent=2)}\\n```\",\n            },\n        ]\n        return await self._call_openai_api(messages)", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "async def analyze_critical_path(self, path_content: str) -> str:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are a world-renowned expert in analyzing critical paths and execution flows in complex software systems. Your ability to identify performance bottlenecks, optimize execution paths, and relate technical flows to business processes is unmatched in the industry.\n\nKey responsibilities:\n1. Analyze execution paths to identify performance-critical sections.\n2. Recognize potential bottlenecks or resource-intensive operations.\n3. Identify critical decision points and their impact on system behavior.\n4. Assess the efficiency of data access and manipulation along the path.\n5. Evaluate error handling and fault tolerance in critical sections.\n6. Relate technical execution flow to high-level business processes.\n7. Suggest optimizations or architectural improvements to enhance performance.", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "Present your analysis in a structured format, using markdown for readability. Use diagrams (described textually) or pseudocode where appropriate to illustrate key points.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Analyze the following execution path and explain its business and technical significance. Identify key operations, potential bottlenecks, and critical decision points. Relate the technical flow to business processes and explain how this path contributes to the overall system functionality. Suggest any potential optimizations or areas for closer monitoring.\\n\\nExecution path:\\n```\\n{path_content}\\n```\",\n            },\n        ]\n        return await self._call_openai_api(messages)", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "async def summarize_cluster(self, cluster_content: List[str]) -> str:\n        combined_content = \"\\n\\n\".join(cluster_content)\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are an elite expert in code analysis and pattern recognition, with a particular talent for identifying common themes and purposes across diverse code segments. Your ability to synthesize information from multiple sources and extract overarching concepts is unparalleled.", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "Key responsibilities:\n1. Identify common patterns, algorithms, or coding styles across multiple code blocks.\n2. Recognize shared business logic or related operations in different parts of a system.\n3. Detect potential code duplication or opportunities for consolidation.\n4. Infer high-level system architecture from patterns in code clusters.\n5. Identify cross-cutting concerns that may benefit from aspect-oriented approaches.\n6. Suggest potential abstractions or shared libraries based on common functionalities.\n7. Recognize inconsistencies in coding practices or naming conventions across clusters.", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "Present your analysis in a structured format, using markdown for clarity. Use tables or lists where appropriate to organize findings.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Examine the following collection of code blocks and provide a comprehensive summary of their common themes, purposes, and functionalities. Identify any shared patterns, repeated business logic, or related operations across these blocks. Suggest potential opportunities for code consolidation, shared libraries, or architectural improvements based on your analysis.\\n\\nCode blocks:\\n```\\n{combined_content}\\n```\",\n            },\n        ]\n        return await self._call_openai_api(messages)", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "async def interactive_query(self, query: str, context: str) -> str:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are an advanced AI assistant specializing in code analysis, software architecture, and development practices. Your vast knowledge spans multiple programming languages, frameworks, and design paradigms. You excel at providing expert insights and answering complex questions about codebases, development practices, and software design.", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "Key responsibilities:\n1. Provide accurate, detailed answers to questions about code and software design.\n2. Offer insights based on best practices and industry standards.\n3. Explain complex technical concepts in clear, understandable terms.\n4. Suggest improvements or alternative approaches where relevant.\n5. Identify potential issues or areas of concern in described scenarios.\n6. Relate technical details to broader architectural or business contexts.\n7. Acknowledge limitations in available information and provide caveats where necessary.", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "Present your responses in a structured, easy-to-read format using markdown. Use code snippets, bullet points, or numbered lists where appropriate to enhance clarity.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Based on the following context about a codebase, please answer this question:\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nProvide a detailed, informative answer. If the context doesn't provide enough information to fully answer the question, state that clearly and offer the best possible insight based on the available information and your general knowledge of software development.\",\n            },\n        ]\n        return await self._call_openai_api(messages)", "file_path": "LLMApi_1.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "LLMApi", "max", "combined_content", "expert", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "stop", "analyze", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "messages", "interactive_query", "analyze_critical_path"]}, {"content": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\nimport json\nfrom typing import List, Dict, Any", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "class LLMApi:\n    def __init__(self, api_key: str = None, model: str = \"gpt-3.5-turbo\"):\n        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\n                \"OpenAI API key is required. Set it as an environment variable or pass it to the constructor.\"\n            )\n        openai.api_key = self.api_key\n        self.model = model\n\n    @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(5))\n    async def _call_openai_api(self, messages: List[Dict[str, str]]) -> str:\n        try:\n            response = await openai.ChatCompletion.acreate(\n                model=self.model, messages=messages\n            )\n            return response.choices[0].message.content.strip()\n        except openai.error.OpenAIError as e:\n            print(f\"OpenAI API error: {e}\")\n            raise", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "async def analyze(self, content: str, prompt: str) -> str:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are a world-class software architect and code analyst with decades of experience. Your task is to provide in-depth, multi-faceted analysis of code and software systems. You have a reputation for delivering insights that transform codebases and development practices.\n\nKey Analysis Framework:\n1. Architecture Overview\n2. Code Quality Assessment\n3. Performance Analysis\n4. Security Evaluation\n5. Maintainability and Scalability\n6. Testing and Quality Assurance\n7. Business Logic and Process Flow\n8. Technology Stack Evaluation\n9. Future-proofing and Trend Alignment\n10. Recommendations and Action Items", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "For each analysis, follow these steps:\n1. Initial overview and context understanding\n2. Detailed examination using the Key Analysis Framework\n3. Synthesis of findings and identification of core issues\n4. Generation of strategic recommendations\n5. Summarization and prioritization of action items\n\nUse industry-standard terminology and concepts such as SOLID principles, DRY (Don't Repeat Yourself), KISS (Keep It Simple, Stupid), and relevant design patterns.\n\nPresent your analysis using clear, structured markdown formatting. Use tables, bullet points, and code snippets where appropriate. Aim for a comprehensive yet concise report, limiting your response to approximately 1000 words unless the complexity of the analysis requires more.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Analyze the following code or system description. Apply the Key Analysis Framework and provide a detailed report with actionable insights.\n\n{prompt}", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "{prompt}\n\nCode/System Description:\n{content}\n\nExample Output Structure:\n## 1. Architecture Overview\n[Concise description of the overall architecture]\n\n## 2. Code Quality Assessment\n- Strengths: [List key strengths]\n- Areas for Improvement: [List main areas needing improvement]\n- SOLID Principles Adherence: [Brief assessment]\n\n## 3. Performance Analysis\n[Identify potential performance bottlenecks and optimization opportunities]\n\n## 4. Security Evaluation\n- Potential Vulnerabilities: [List any identified security issues]\n- Recommendations: [Suggest security improvements]\n\n... [Continue with other sections] ...\n\n## 10. Recommendations and Action Items\n1. [High priority action item]\n2. [Medium priority action item]\n3. [Low priority action item]\n\nPlease provide your analysis following a similar structure, adapting as necessary to the specific content provided.\"\"\",\n            },\n        ]\n        return await self._call_openai_api(messages)", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "Please provide your analysis following a similar structure, adapting as necessary to the specific content provided.\"\"\",\n            },\n        ]\n        return await self._call_openai_api(messages)\n\n    async def generate_flowchart(self, description: str) -> str:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are a renowned expert in creating precise, informative flowcharts from complex system descriptions. Your flowcharts are known for their clarity, accuracy, and ability to communicate intricate processes effectively.\n\nKey Flowchart Creation Steps:\n1. Identify main processes and sub-processes\n2. Determine the logical flow and decision points\n3. Identify inputs, outputs, and data flows\n4. Incorporate error handling and alternative paths\n5. Optimize layout for readability\n6. Add annotations for clarity", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "Use standard flowchart symbols and conventions. Your output should be a textual representation of the flowchart that can be easily translated into a visual diagram.\n\nAdhere to best practices such as:\n- Consistent symbol usage\n- Clear, concise labeling\n- Logical flow from top to bottom or left to right\n- Proper use of decision diamonds\n- Clear start and end points\n\nLimit your response to approximately 500 words, focusing on the most critical elements of the process.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Create a detailed flowchart based on the following description. Ensure your flowchart captures all key processes, decision points, and data flows.\n\nProcess Description:\n{description}", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "Process Description:\n{description}\n\nExample Output Structure:\n[Start] -> \"Initial Process\"\n\"Initial Process\" -> <Decision 1>\n<Decision 1> -- Yes --> \"Process A\"\n<Decision 1> -- No --> \"Process B\"\n\"Process A\" -> \"Sub-process A1\"\n\"Sub-process A1\" -> \"Sub-process A2\"\n\"Process B\" -> <Decision 2>\n<Decision 2> -- Option 1 --> \"Process C\"\n<Decision 2> -- Option 2 --> \"Process D\"\n\"Process C\" -> [End]\n\"Process D\" -> [End]\n\n\nPlease provide your flowchart representation following a similar structure, adapting as necessary to the specific process described.\"\"\",\n            },\n        ]\n        return await self._call_openai_api(messages)", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "async def extract_business_logic(self, code: str) -> str:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are an expert business analyst with a strong background in software engineering. Your specialty is extracting and articulating business logic and processes from source code. Your insights bridge the gap between technical implementations and business operations.\n\nKey Extraction Framework:\n1. Core Business Entities\n2. Business Rules and Constraints\n3. Process Flows\n4. Data Transformations\n5. Decision Points\n6. Integration Points\n7. Reporting and Analytics Logic\n\nFor each analysis, follow these steps:\n1. Identify business-related code segments\n2. Map technical implementations to business concepts\n3. Articulate business rules in plain language\n4. Describe process flows and decision logic\n5. Highlight key data transformations and their business significance", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "Use business terminology where possible, explaining technical concepts in business-friendly language. Present your findings in a structured, easy-to-read format using markdown. Limit your response to approximately 800 words, focusing on the most significant business logic elements.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Extract and explain the core business logic embedded in the following code. Focus on identifying key business rules, processes, and decision points. Relate technical implementations to real-world business concepts and operations.\n\nCode to analyze:\n{code}\n\nExample Output Structure:\n## 1. Core Business Entities\n- Entity A: [Brief description and business significance]\n- Entity B: [Brief description and business significance]\n\n## 2. Key Business Rules\n1. [Business rule 1]\n2. [Business rule 2]\n   ...", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "## 2. Key Business Rules\n1. [Business rule 1]\n2. [Business rule 2]\n   ...\n\n## 3. Main Business Processes\n1. Process X:\n   - Steps: [Outline key steps]\n   - Business Significance: [Explain importance]\n\n2. Process Y:\n   - Steps: [Outline key steps]\n   - Business Significance: [Explain importance]\n\n## 4. Critical Decision Points\n1. [Decision point 1]: [Explain business logic and implications]\n2. [Decision point 2]: [Explain business logic and implications]\n   ...\n\n## 5. Data Transformations\n- [Transformation 1]: [Business meaning and importance]\n- [Transformation 2]: [Business meaning and importance]\n   ...\n\nPlease provide your analysis following a similar structure, adapting as necessary to the specific code provided.\"\"\",\n            },\n        ]\n        return await self._call_openai_api(messages)", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "Please provide your analysis following a similar structure, adapting as necessary to the specific code provided.\"\"\",\n            },\n        ]\n        return await self._call_openai_api(messages)\n\n    async def analyze_code_quality(self, pylint_report: str) -> str:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are a senior code quality expert with extensive experience in software engineering best practices and static code analysis. Your role is to interpret static analysis reports and provide actionable insights for improving code quality.\n\nKey Analysis Framework:\n1. Overall Code Health\n2. Coding Standards Compliance\n3. Code Complexity\n4. Maintainability Index\n5. Potential Bugs and Code Smells\n6. Security Vulnerabilities\n7. Performance Implications\n8. Documentation and Readability", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "For each analysis, follow these steps:\n1. Summarize key findings from the static analysis report\n2. Identify patterns and recurring issues\n3. Prioritize issues based on severity and potential impact\n4. Provide specific, actionable recommendations for improvement\n5. Suggest best practices and coding standards to adopt", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "Reference industry-standard concepts such as SOLID principles, DRY, KISS, and relevant design patterns where applicable. Present your analysis in a clear, structured format using markdown. Use tables or lists to summarize findings where appropriate. Limit your response to approximately 800 words, focusing on the most critical quality issues and impactful recommendations.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Analyze the following Pylint report and provide a comprehensive assessment of the code quality. Identify major issues, potential risks, and areas for improvement. Offer specific, actionable recommendations for addressing each significant issue.\n\nPylint report:\n{pylint_report}\n\nExample Output Structure:\n## 1. Overall Code Health\n[Brief summary of the overall code quality]", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "Pylint report:\n{pylint_report}\n\nExample Output Structure:\n## 1. Overall Code Health\n[Brief summary of the overall code quality]\n\n## 2. Key Issues Identified\n1. [Issue 1]: \n   - Severity: [High/Medium/Low]\n   - Description: [Brief explanation]\n   - Recommendation: [Specific action to address]\n\n2. [Issue 2]:\n   - Severity: [High/Medium/Low]\n   - Description: [Brief explanation]\n   - Recommendation: [Specific action to address]\n   ...\n\n## 3. Code Complexity Analysis\n[Summary of code complexity findings]\n\n## 4. Maintainability Concerns\n[Highlight any maintainability issues and their implications]\n\n## 5. Security Considerations\n[Identify any security-related issues found in the report]\n\n## 6. Top Recommendations\n1. [High priority recommendation]\n2. [Medium priority recommendation]\n3. [Low priority recommendation]", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "## 6. Top Recommendations\n1. [High priority recommendation]\n2. [Medium priority recommendation]\n3. [Low priority recommendation]\n\nPlease provide your analysis following a similar structure, adapting as necessary to the specific Pylint report provided.\"\"\",\n            },\n        ]\n        return await self._call_openai_api(messages)\n\n    async def analyze_code_evolution(self, evolution_data: List[Dict[str, Any]]) -> str:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are a distinguished software evolution analyst with expertise in interpreting code metrics over time. Your insights help teams understand development trends, identify potential issues, and make informed decisions about future development strategies.", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "Key Analysis Framework:\n1. Code Size and Complexity Trends\n2. Maintainability Index Changes\n3. Test Coverage Evolution\n4. Code Churn Patterns\n5. Technical Debt Accumulation/Reduction\n6. Performance Metric Trends\n7. Security Vulnerability Patterns\n8. Coding Standards Adherence Over Time\n\nFor each analysis, follow these steps:\n1. Identify significant trends and patterns in the data\n2. Correlate changes with known project events or milestones\n3. Assess the impact of observed trends on overall project health\n4. Provide insights into potential future challenges or opportunities\n5. Offer strategic recommendations for improving development practices", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "Use data visualization techniques (described textually) where appropriate to illustrate key trends. Present your analysis in a structured, easy-to-read format using markdown. Limit your response to approximately 1000 words, focusing on the most significant findings and their implications.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Analyze the following code evolution data and provide a detailed assessment of how the codebase has changed over time. Focus on identifying significant trends, patterns, or events that indicate changes in development practices, code health, or project direction.", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "Evolution data:\n```json\n{json.dumps(evolution_data, indent=2)}\nExample Output Structure:\n1. Code Size and Complexity Trends\n[Describe trends in LOC, cyclomatic complexity, etc.]\n2. Maintainability Index Analysis\n[Discuss changes in maintainability over time]\n3. Test Coverage Trends\n[Analyze how test coverage has evolved]\n4. Code Churn Patterns\n[Identify areas of high churn and potential implications]\n5. Technical Debt Assessment\n[Evaluate accumulation or reduction of technical debt]\n6. Performance and Efficiency Trends\n[Discuss any noticeable trends in performance metrics]\n7. Security Vulnerability Patterns\n[Analyze trends in security-related issues]\n8. Key Observations and Insights\n  1.[Major insight 1]\n  2.[Major insight 2]\n...\n9. Strategic Recommendations\n   1.[High priority recommendation]\n   2.[Medium priority recommendation]\n   3.[Low priority recommendation]", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "1.[Major insight 1]\n  2.[Major insight 2]\n...\n9. Strategic Recommendations\n   1.[High priority recommendation]\n   2.[Medium priority recommendation]\n   3.[Low priority recommendation]\nPlease provide your analysis following a similar structure, adapting as necessary to the specific evolution data provided.\"\"\",\n            },\n        ]\n        return await self._call_openai_api(messages)", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "async def analyze_critical_path(self, path_content: str) -> str:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are a renowned expert in software performance optimization and critical path analysis. Your expertise lies in identifying performance bottlenecks, optimizing execution paths, and relating technical flows to business processes.\n         Key Analysis Framework:\nPath Overview\nPerformance Bottlenecks\nResource Utilization\nData Flow Efficiency\nError Handling and Resilience\nScalability Considerations\nBusiness Process Alignment\nOptimization Opportunities\nFor each analysis, follow these steps:\nMap out the critical path steps\nIdentify performance-critical sections\nAnalyze resource usage and data flow\nEvaluate error handling and fault tolerance\nAssess scalability and potential bottlenecks\nRelate technical flow to business processes\nSuggest optimizations and improvements", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "Evaluate error handling and fault tolerance\nAssess scalability and potential bottlenecks\nRelate technical flow to business processes\nSuggest optimizations and improvements\nUse technical terminology where appropriate, but explain concepts clearly for a mixed technical and business audience. Present your analysis in a structured format using markdown. Use code snippets or pseudocode where helpful to illustrate points. Limit your response to approximately 800 words, focusing on the most impactful findings and recommendations.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Analyze the following execution path and explain its business and technical significance. Identify key operations, potential bottlenecks, and critical decision points. Relate the technical flow to business processes and explain how this path contributes to the overall system functionality.\nExecution path:\n{path_content}", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "Example Output Structure:\n1. Path Overview\n[Summarize the main steps in the critical path]\n2. Performance Analysis\nBottleneck 1: [Description and impact]\nBottleneck 2: [Description and impact]\n...\n3. Resource Utilization\n[Discuss key resources used and any efficiency concerns]\n4. Data Flow Efficiency\n[Analyze how data moves through the system, identifying any inefficiencies]\n5. Error Handling and Resilience\n[Evaluate the robustness of error handling in critical sections]\n6. Scalability Assessment\n[Discuss how well this path would scale under increased load]\n7. Business Process Alignment\n[Explain how this technical path aligns with and supports business processes]\n8. Optimization Recommendations\n[High priority optimization]\n[Medium priority optimization]\n[Low priority optimization]\nPlease provide your analysis following a similar structure, adapting as necessary to the specific execution path provided.\"\"\",\n            },\n        ]\n        return await self._call_openai_api(messages)", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "async def summarize_cluster(self, cluster_content: List[str]) -> str:\n        combined_content = \"\\n\\n\".join(cluster_content)\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are an expert in code pattern recognition and software architecture, with a particular talent for identifying common themes and purposes across diverse code segments. Your ability to synthesize information from multiple sources and extract overarching concepts is unparalleled.\n\nKey Analysis Framework:\n1. Common Patterns and Structures\n2. Shared Functionality\n3. Code Duplication Assessment\n4. Architectural Implications\n5. Cross-cutting Concerns\n6. Potential Abstractions\n7. Consistency in Coding Practices\n8. Opportunities for Consolidation", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "For each analysis, follow these steps:\n1. Identify recurring patterns or similar code structures\n2. Recognize shared business logic or operations\n3. Assess the level and nature of code duplication\n4. Infer architectural design from observed patterns\n5. Identify aspects that cut across multiple components\n6. Suggest potential abstractions or shared libraries\n7. Evaluate consistency in coding styles and practices\n8. Recommend opportunities for code consolidation or refactoring", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "Present your analysis in a clear, structured format using markdown. Use code snippets or pseudocode where appropriate to illustrate key points. Limit your response to approximately 800 words, focusing on the most significant patterns and impactful recommendations.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Examine the following collection of code blocks and provide a comprehensive summary of their common themes, purposes, and functionalities. Identify any shared patterns, repeated business logic, or related operations across these blocks. Suggest potential opportunities for code consolidation, shared libraries, or architectural improvements based on your analysis.\n{combined_content}\n\nExample Output Structure:\n## 1. Common Patterns and Structures\n[Describe recurring code patterns or similar structures found across the blocks]\n\n## 2. Shared Functionality\n[Identify and explain common functionalities or operations]", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "## 2. Shared Functionality\n[Identify and explain common functionalities or operations]\n\n## 3. Code Duplication Assessment\n- Level of Duplication: [High/Medium/Low]\n- Nature of Duplication: [Describe the type of code being duplicated]\n- Impact: [Discuss the implications of this duplication]\n\n## 4. Architectural Insights\n[Infer potential architectural design choices based on observed patterns]\n\n## 5. Cross-cutting Concerns\n[Identify aspects that appear across multiple components]\n\n## 6. Potential Abstractions\n1. [Suggested abstraction 1]\n2. [Suggested abstraction 2]\n   ...\n\n## 7. Coding Practice Consistency\n[Evaluate the consistency of coding styles and practices across the blocks]\n\n## 8. Consolidation Opportunities\n1. [High priority consolidation opportunity]\n2. [Medium priority consolidation opportunity]\n3. [Low priority consolidation opportunity]", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "## 8. Consolidation Opportunities\n1. [High priority consolidation opportunity]\n2. [Medium priority consolidation opportunity]\n3. [Low priority consolidation opportunity]\n\nPlease provide your analysis following a similar structure, adapting as necessary to the specific code blocks provided.\"\"\",\n            },\n        ]\n\n        return await self._call_openai_api(messages)\n\n    async def interactive_query(self, query: str, context: str) -> str:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are an advanced AI assistant specializing in software engineering, architecture, and development practices. Your vast knowledge spans multiple programming languages, frameworks, and design paradigms. You excel at providing expert insights and answering complex questions about codebases, development practices, and software design.", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "Key Responsibilities:\n1. Interpret and answer questions about code and software systems\n2. Provide insights based on best practices and industry standards\n3. Explain complex technical concepts in clear, understandable terms\n4. Suggest improvements or alternative approaches where relevant\n5. Identify potential issues or areas of concern in described scenarios\n6. Relate technical details to broader architectural or business contexts\n7. Acknowledge limitations in available information and provide caveats where necessary", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "Guidelines for Responses:\n- Start with a direct answer to the question, then provide supporting details\n- Use analogies or examples to explain complex concepts when appropriate\n- Reference relevant design patterns, architectural styles, or industry best practices\n- Provide code snippets or pseudocode to illustrate points when helpful\n- Offer multiple perspectives or solutions when the question allows for it\n- Be clear about assumptions made when information is limited\n- Suggest follow-up questions or areas for further investigation when relevant\n\nPresent your responses in a structured, easy-to-read format using markdown. Limit your response to approximately 500 words unless the complexity of the question requires more detail.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Based on the following context about a codebase, please answer this question:\n\nContext:\n{context}\n\nQuestion: {query}", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "Context:\n{context}\n\nQuestion: {query}\n\nExample Output Structure:\n## Direct Answer\n[Provide a concise, direct answer to the question]\n\n## Detailed Explanation\n[Offer a more in-depth explanation, using examples or analogies if helpful]\n\n## Code Illustration (if applicable)\n[Provide a code snippet or pseudocode to illustrate the point]\n\n## Best Practices and Considerations\n- [Relevant best practice or consideration]\n- [Another relevant point]\n  ...\n\n## Potential Issues or Limitations\n[Discuss any potential problems or limitations related to the question]\n\n## Further Investigation\n[Suggest any areas for further research or follow-up questions]\n\nPlease provide your response following a similar structure, adapting as necessary to the specific question asked.\"\"\",\n            },\n        ]\n        return await self._call_openai_api(messages)\n\n\n# Example usage\nasync def main():\n    api_key = \"your-api-key-here\"  # Replace with your actual API key\n    llm_api = LLMApi(api_key)", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "# Example usage\nasync def main():\n    api_key = \"your-api-key-here\"  # Replace with your actual API key\n    llm_api = LLMApi(api_key)\n\n    # Example: Analyze a piece of code\n    code_to_analyze = \"\"\"\n    def calculate_total(items):\n        total = 0\n        for item in items:\n            total += item.price * item.quantity\n        return total\n    \"\"\"\n    analysis = await llm_api.analyze(\n        code_to_analyze, \"Analyze this function and suggest improvements\"\n    )\n    print(analysis)\n\n    # Add more examples here to test other methods\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(main())", "file_path": "LLMApi_2.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "generate_flowchart", "api_key", "total", "LLMApi", "max", "combined_content", "__name__", "analyze_code_quality", "indent", "analyze_code_evolution", "summarize_cluster", "calculate_total", "stop", "analyze", "main", "software", "llm_api", "response", "min", "__init__", "wait", "extract_business_logic", "_call_openai_api", "model", "analysis", "messages", "code_to_analyze", "interactive_query", "analyze_critical_path"]}, {"content": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\nimport json\nimport ast\nimport networkx as nx\nfrom typing import List, Dict, Any\n\napi_key = 'your_openai_api_key'\n\n# \u521d\u59cb\u5316OpenAI\u7684API\u5ba2\u6237\u7aef\nbase_url = \"http://127.0.0.1:8000/v1/\"\n\nfrom openai import OpenAI\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(base_url=base_url, api_key=api_key)", "file_path": "LLMApi_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "_analyze_segments_with_context", "context", "LLMApi", "_segment_code", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code"]}, {"content": "class LLMApi:\n    def __init__(self, api_key: str = None, model: str = \"gpt-3.5-turbo\"):\n        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"OpenAI API key is required. Set it as an environment variable or pass it to the constructor.\")\n        openai.api_key = self.api_key\n        self.model = model\n        self.global_context = {}\n        self.analysis_graph = nx.DiGraph()\n        self.prompts = self._load_prompts()\n\n    def _load_prompts(self) -> Dict[str, str]:\n        with open('D:\\\\workspaces\\\\python_projects\\\\code_review\\\\prompts.json', 'r') as f:\n            return json.load(f)", "file_path": "LLMApi_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "_analyze_segments_with_context", "context", "LLMApi", "_segment_code", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code"]}, {"content": "def _load_prompts(self) -> Dict[str, str]:\n        with open('D:\\\\workspaces\\\\python_projects\\\\code_review\\\\prompts.json', 'r') as f:\n            return json.load(f)\n\n    @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(5))\n    async def _call_openai_api(self, messages: List[Dict[str, str]]) -> str:\n        try:\n            response = await client.chat.completions.create(\n                model=self.model,\n                messages=messages,\n                timeout= 180000,\n            )\n            print(response)\n            return response.choices[0].message.content.strip()\n        except RuntimeError as e:\n            print(f\"OpenAI API error: {e}\")\n            raise", "file_path": "LLMApi_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "_analyze_segments_with_context", "context", "LLMApi", "_segment_code", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code"]}, {"content": "async def analyze(self, content: str, prompt: str) -> str:\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are an expert code analyst and software architect.\"},\n            {\"role\": \"user\", \"content\": f\"{prompt}\\n\\nCode:\\n{content}\"}\n        ]\n        return await self._call_openai_api(messages)\n\n    async def analyze_with_global_context(self, code: str) -> Dict[str, Any]:\n        self.global_context['overview'] = await self.analyze(code, self.prompts['global_overview'])\n\n        segments = self._segment_code(code)\n        self._build_dependency_graph(segments)\n        detailed_analyses = await self._analyze_segments_with_context(segments)\n        summary = await self._summarize_analysis(detailed_analyses)\n\n        return {\n            'overview': self.global_context['overview'],\n            'detailed_analyses': detailed_analyses,\n            'summary': summary,\n            'dependency_graph': nx.node_link_data(self.analysis_graph)\n        }", "file_path": "LLMApi_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "_analyze_segments_with_context", "context", "LLMApi", "_segment_code", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code"]}, {"content": "def _segment_code(self, code: str) -> List[Dict[str, str]]:\n        tree = ast.parse(code)\n        segments = []\n\n        for node in ast.iter_child_nodes(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n                segment = {\n                    'type': 'function' if isinstance(node, ast.FunctionDef) else 'class',\n                    'name': node.name,\n                    'code': ast.get_source_segment(code, node),\n                    'docstring': ast.get_docstring(node)\n                }\n                segments.append(segment)\n\n        if not segments:\n            segments.append({\n                'type': 'module',\n                'name': 'main',\n                'code': code,\n                'docstring': ast.get_docstring(tree)\n            })\n\n        return segments\n\n    def _build_dependency_graph(self, segments: List[Dict[str, str]]):\n        for segment in segments:\n            self.analysis_graph.add_node(segment['name'], type=segment['type'])", "file_path": "LLMApi_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "_analyze_segments_with_context", "context", "LLMApi", "_segment_code", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code"]}, {"content": "def _build_dependency_graph(self, segments: List[Dict[str, str]]):\n        for segment in segments:\n            self.analysis_graph.add_node(segment['name'], type=segment['type'])\n\n        for segment in segments:\n            tree = ast.parse(segment['code'])\n            for node in ast.walk(tree):\n                if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load):\n                    for other_segment in segments:\n                        if other_segment['name'] == node.id:\n                            self.analysis_graph.add_edge(segment['name'], other_segment['name'])", "file_path": "LLMApi_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "_analyze_segments_with_context", "context", "LLMApi", "_segment_code", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code"]}, {"content": "async def _analyze_segments_with_context(self, segments: List[Dict[str, str]]) -> List[Dict[str, Any]]:\n        analyses = []\n        for segment in segments:\n            context = self._get_segment_context(segment)\n            analysis_prompt = self.prompts['segment_analysis'].format(\n                segment_type=segment['type'],\n                segment_name=segment['name'],\n                global_context=json.dumps(self.global_context, indent=2),\n                segment_context=json.dumps(context, indent=2),\n                segment_code=segment['code']\n            )\n            analysis = await self.analyze(segment['code'], analysis_prompt)\n            analyses.append({\n                'type': segment['type'],\n                'name': segment['name'],\n                'analysis': analysis\n            })\n            self.global_context[segment['name']] = analysis[:200]  # Store a summary\n\n        return analyses", "file_path": "LLMApi_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "_analyze_segments_with_context", "context", "LLMApi", "_segment_code", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code"]}, {"content": "return analyses\n\n    def _get_segment_context(self, segment: Dict[str, str]) -> Dict[str, Any]:\n        predecessors = list(self.analysis_graph.predecessors(segment['name']))\n        successors = list(self.analysis_graph.successors(segment['name']))\n        return {\n            'type': segment['type'],\n            'name': segment['name'],\n            'docstring': segment['docstring'],\n            'dependencies': predecessors,\n            'dependents': successors\n        }\n\n    async def _summarize_analysis(self, detailed_analyses: List[Dict[str, Any]]) -> str:\n        summary_prompt = self.prompts['summary_analysis'].format(\n            global_context=json.dumps(self.global_context, indent=2),\n            detailed_analyses=json.dumps(detailed_analyses, indent=2)\n        )\n        return await self.analyze(\"\", summary_prompt)", "file_path": "LLMApi_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "_analyze_segments_with_context", "context", "LLMApi", "_segment_code", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code"]}, {"content": "async def interactive_analysis(self, code: str):\n        print(\"Analyzing code and building context...\")\n        analysis_result = await self.analyze_with_global_context(code)\n        print(\"Analysis complete. You can now ask questions about the code.\")\n        \n        while True:\n            query = input(\"\\nEnter your question (or 'quit' to exit): \")\n            if query.lower() == 'quit':\n                break\n\n            prompt = self.prompts['interactive_query'].format(\n                overview=analysis_result['overview'],\n                detailed_analyses=json.dumps(analysis_result['detailed_analyses'], indent=2),\n                summary=analysis_result['summary'],\n                query=query\n            )\n            \n            answer = await self.analyze(\"\", prompt)\n            print(\"\\nAnswer:\", answer)", "file_path": "LLMApi_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "_analyze_segments_with_context", "context", "LLMApi", "_segment_code", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code"]}, {"content": "# Example usage\nasync def main():\n    api_key = \"your-api-key-here\"  # Replace with your actual API key\n    llm_api = LLMApi(api_key)\n    \n    code_to_analyze = \"\"\"\nimport math", "file_path": "LLMApi_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "_analyze_segments_with_context", "context", "LLMApi", "_segment_code", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code"]}, {"content": "def calculate_circle_area(radius):\n    #Calculate the area of a circle given its radius.\n    return math.pi * radius ** 2", "file_path": "LLMApi_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "_analyze_segments_with_context", "context", "LLMApi", "_segment_code", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code"]}, {"content": "class Rectangle:\n    def __init__(self, width, height):\n        self.width = width\n        self.height = height\n\n    def calculate_area(self):\n        #Calculate the area of the rectangle.\n        return self.width * self.height\n\ndef main():\n    # Calculate circle area\n    circle_radius = 5\n    circle_area = calculate_circle_area(circle_radius)\n    print(f\"Area of circle with radius {circle_radius}: {circle_area:.2f}\")\n\n    # Calculate rectangle area\n    rect = Rectangle(4, 6)\n    rect_area = rect.calculate_area()\n    print(f\"Area of rectangle with width {rect.width} and height {rect.height}: {rect_area}\")\n\nif __name__ == \"__main__\":\n    main()\n\"\"\"\n\n    # Perform analysis with global context\n    analysis_result = await llm_api.analyze_with_global_context(code_to_analyze)\n    print(json.dumps(analysis_result, indent=2))\n\n    # Start interactive analysis session\n    await llm_api.interactive_analysis(code_to_analyze)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())", "file_path": "LLMApi_3.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "_analyze_segments_with_context", "context", "LLMApi", "_segment_code", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code"]}, {"content": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\nimport json\nimport ast\nimport networkx as nx\nfrom typing import List, Dict, Any\nfrom sentence_transformers import SentenceTransformer, util\n\nbase_url = \"http://127.0.0.1:8000/v1/\"\n\nfrom openai import OpenAI\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(base_url=base_url, api_key=\"api_key\",timeout=600000)", "file_path": "LLMApi_4.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "vector_model", "convert_to_tensor", "_analyze_segments_with_context", "key", "context_store", "context", "LLMApi", "_segment_code", "code_embedding", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "score", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "_retrieve_additional_context", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "best_context", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code", "context_scores", "stored_embedding"]}, {"content": "class LLMApi:\n    def __init__(self, api_key: str = None, model: str = \"gpt-3.5-turbo\"):\n        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"OpenAI API key is required. Set it as an environment variable or pass it to the constructor.\")\n        openai.api_key = self.api_key\n        self.model = model\n        self.global_context = {}\n        self.analysis_graph = nx.DiGraph()\n        self.prompts = self._load_prompts()\n        self.vector_model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.context_store = {}  # This will act as our external storage for context\n\n    def _load_prompts(self) -> Dict[str, str]:\n        with open('D:\\\\workspaces\\\\python_projects\\\\code_review\\\\prompts.json', 'r') as f:\n            return json.load(f)", "file_path": "LLMApi_4.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "vector_model", "convert_to_tensor", "_analyze_segments_with_context", "key", "context_store", "context", "LLMApi", "_segment_code", "code_embedding", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "score", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "_retrieve_additional_context", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "best_context", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code", "context_scores", "stored_embedding"]}, {"content": "def _load_prompts(self) -> Dict[str, str]:\n        with open('D:\\\\workspaces\\\\python_projects\\\\code_review\\\\prompts.json', 'r') as f:\n            return json.load(f)\n\n    @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(5))\n    async def _call_openai_api(self, messages: List[Dict[str, str]]) -> str:\n        try:\n            response = await client.chat.completions.create(\n                model=self.model,\n                messages=messages\n            )\n            return response.choices[0].message.content.strip()\n        except openai.error.OpenAIError as e:\n            print(f\"OpenAI API error: {e}\")\n            raise\n\n    async def analyze(self, content: str, prompt: str) -> str:\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are an expert code analyst and software architect.\"},\n            {\"role\": \"user\", \"content\": f\"{prompt}\\n\\nCode:\\n{content}\"}\n        ]\n        return await self._call_openai_api(messages)", "file_path": "LLMApi_4.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "vector_model", "convert_to_tensor", "_analyze_segments_with_context", "key", "context_store", "context", "LLMApi", "_segment_code", "code_embedding", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "score", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "_retrieve_additional_context", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "best_context", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code", "context_scores", "stored_embedding"]}, {"content": "async def analyze_with_global_context(self, code: str) -> Dict[str, Any]:\n        self.global_context['overview'] = await self.analyze(code, self.prompts['global_overview'])\n\n        segments = self._segment_code(code)\n        self._build_dependency_graph(segments)\n        detailed_analyses = await self._analyze_segments_with_context(segments)\n        summary = await self._summarize_analysis(detailed_analyses)\n\n        return {\n            'overview': self.global_context['overview'],\n            'detailed_analyses': detailed_analyses,\n            'summary': summary,\n            'dependency_graph': nx.node_link_data(self.analysis_graph)\n        }\n\n    def _segment_code(self, code: str) -> List[Dict[str, str]]:\n        tree = ast.parse(code)\n        segments = []", "file_path": "LLMApi_4.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "vector_model", "convert_to_tensor", "_analyze_segments_with_context", "key", "context_store", "context", "LLMApi", "_segment_code", "code_embedding", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "score", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "_retrieve_additional_context", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "best_context", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code", "context_scores", "stored_embedding"]}, {"content": "def _segment_code(self, code: str) -> List[Dict[str, str]]:\n        tree = ast.parse(code)\n        segments = []\n\n        for node in ast.iter_child_nodes(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n                segment = {\n                    'type': 'function' if isinstance(node, ast.FunctionDef) else 'class',\n                    'name': node.name,\n                    'code': ast.get_source_segment(code, node),\n                    'docstring': ast.get_docstring(node)\n                }\n                segments.append(segment)\n\n        if not segments:\n            segments.append({\n                'type': 'module',\n                'name': 'main',\n                'code': code,\n                'docstring': ast.get_docstring(tree)\n            })\n\n        return segments\n\n    def _build_dependency_graph(self, segments: List[Dict[str, str]]):\n        for segment in segments:\n            self.analysis_graph.add_node(segment['name'], type=segment['type'])", "file_path": "LLMApi_4.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "vector_model", "convert_to_tensor", "_analyze_segments_with_context", "key", "context_store", "context", "LLMApi", "_segment_code", "code_embedding", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "score", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "_retrieve_additional_context", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "best_context", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code", "context_scores", "stored_embedding"]}, {"content": "def _build_dependency_graph(self, segments: List[Dict[str, str]]):\n        for segment in segments:\n            self.analysis_graph.add_node(segment['name'], type=segment['type'])\n\n        for segment in segments:\n            tree = ast.parse(segment['code'])\n            for node in ast.walk(tree):\n                if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load):\n                    for other_segment in segments:\n                        if other_segment['name'] == node.id:\n                            self.analysis_graph.add_edge(segment['name'], other_segment['name'])", "file_path": "LLMApi_4.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "vector_model", "convert_to_tensor", "_analyze_segments_with_context", "key", "context_store", "context", "LLMApi", "_segment_code", "code_embedding", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "score", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "_retrieve_additional_context", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "best_context", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code", "context_scores", "stored_embedding"]}, {"content": "async def _analyze_segments_with_context(self, segments: List[Dict[str, str]]) -> List[Dict[str, Any]]:\n        analyses = []\n        for segment in segments:\n            context = self._get_segment_context(segment)\n            if not context:\n                context = await self._retrieve_additional_context(segment['code'])\n            analysis_prompt = self.prompts['segment_analysis'].format(\n                segment_type=segment['type'],\n                segment_name=segment['name'],\n                global_context=json.dumps(self.global_context, indent=2),\n                segment_context=json.dumps(context, indent=2),\n                segment_code=segment['code']\n            )\n            analysis = await self.analyze(segment['code'], analysis_prompt)\n            analyses.append({\n                'type': segment['type'],\n                'name': segment['name'],\n                'analysis': analysis\n            })", "file_path": "LLMApi_4.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "vector_model", "convert_to_tensor", "_analyze_segments_with_context", "key", "context_store", "context", "LLMApi", "_segment_code", "code_embedding", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "score", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "_retrieve_additional_context", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "best_context", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code", "context_scores", "stored_embedding"]}, {"content": "analyses.append({\n                'type': segment['type'],\n                'name': segment['name'],\n                'analysis': analysis\n            })\n            self.global_context[segment['name']] = analysis[:200]  # Store a summary", "file_path": "LLMApi_4.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "vector_model", "convert_to_tensor", "_analyze_segments_with_context", "key", "context_store", "context", "LLMApi", "_segment_code", "code_embedding", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "score", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "_retrieve_additional_context", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "best_context", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code", "context_scores", "stored_embedding"]}, {"content": "return analyses\n\n    def _get_segment_context(self, segment: Dict[str, str]) -> Dict[str, Any]:\n        predecessors = list(self.analysis_graph.predecessors(segment['name']))\n        successors = list(self.analysis_graph.successors(segment['name']))\n        return {\n            'type': segment['type'],\n            'name': segment['name'],\n            'docstring': segment['docstring'],\n            'dependencies': predecessors,\n            'dependents': successors\n        }", "file_path": "LLMApi_4.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "vector_model", "convert_to_tensor", "_analyze_segments_with_context", "key", "context_store", "context", "LLMApi", "_segment_code", "code_embedding", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "score", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "_retrieve_additional_context", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "best_context", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code", "context_scores", "stored_embedding"]}, {"content": "async def _retrieve_additional_context(self, code: str) -> Dict[str, Any]:\n        \"\"\"\n        Retrieve additional context from external storage if needed.\n        \"\"\"\n        code_embedding = self.vector_model.encode(code, convert_to_tensor=True)\n        context_scores = []\n        for stored_code, stored_context in self.context_store.items():\n            stored_embedding = self.vector_model.encode(stored_code, convert_to_tensor=True)\n            score = util.pytorch_cos_sim(code_embedding, stored_embedding)\n            context_scores.append((score, stored_context))\n\n        if context_scores:\n            best_context = max(context_scores, key=lambda x: x[0])[1]\n            return best_context\n        else:\n            return {}", "file_path": "LLMApi_4.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "vector_model", "convert_to_tensor", "_analyze_segments_with_context", "key", "context_store", "context", "LLMApi", "_segment_code", "code_embedding", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "score", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "_retrieve_additional_context", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "best_context", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code", "context_scores", "stored_embedding"]}, {"content": "if context_scores:\n            best_context = max(context_scores, key=lambda x: x[0])[1]\n            return best_context\n        else:\n            return {}\n\n    async def _summarize_analysis(self, detailed_analyses: List[Dict[str, Any]]) -> str:\n        summary_prompt = self.prompts['summary_analysis'].format(\n            global_context=json.dumps(self.global_context, indent=2),\n            detailed_analyses=json.dumps(detailed_analyses, indent=2)\n        )\n        return await self.analyze(\"\", summary_prompt)\n\n    async def interactive_analysis(self, code: str):\n        print(\"Analyzing code and building context...\")\n        analysis_result = await self.analyze_with_global_context(code)\n        print(\"Analysis complete. You can now ask questions about the code.\")\n        \n        while True:\n            query = input(\"\\nEnter your question (or 'quit' to exit): \")\n            if query.lower() == 'quit':\n                break", "file_path": "LLMApi_4.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "vector_model", "convert_to_tensor", "_analyze_segments_with_context", "key", "context_store", "context", "LLMApi", "_segment_code", "code_embedding", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "score", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "_retrieve_additional_context", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "best_context", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code", "context_scores", "stored_embedding"]}, {"content": "prompt = self.prompts['interactive_query'].format(\n                overview=analysis_result['overview'],\n                detailed_analyses=json.dumps(analysis_result['detailed_analyses'], indent=2),\n                summary=analysis_result['summary'],\n                query=query\n            )\n            \n            answer = await self.analyze(\"\", prompt)\n            print(\"\\nAnswer:\", answer)\n\n# Example usage\nasync def main():\n    api_key = \"your-api-key-here\"  # Replace with your actual API key\n    llm_api = LLMApi(api_key)\n    \n    code_to_analyze = \"\"\"\nimport math", "file_path": "LLMApi_4.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "vector_model", "convert_to_tensor", "_analyze_segments_with_context", "key", "context_store", "context", "LLMApi", "_segment_code", "code_embedding", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "score", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "_retrieve_additional_context", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "best_context", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code", "context_scores", "stored_embedding"]}, {"content": "def calculate_circle_area(radius):\n    \\\"\\\"\\\"Calculate the area of a circle given its radius.\\\"\\\"\\\"\n    return math.pi * radius ** 2", "file_path": "LLMApi_4.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "vector_model", "convert_to_tensor", "_analyze_segments_with_context", "key", "context_store", "context", "LLMApi", "_segment_code", "code_embedding", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "score", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "_retrieve_additional_context", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "best_context", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code", "context_scores", "stored_embedding"]}, {"content": "class Rectangle:\n    def __init__(self, width, height):\n        self.width = width\n        self.height = height\n\n    def calculate_area(self):\n        \\\"\\\"\\\"Calculate the area of the rectangle.\\\"\\\"\\\"\n        return self.width * self.height", "file_path": "LLMApi_4.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "vector_model", "convert_to_tensor", "_analyze_segments_with_context", "key", "context_store", "context", "LLMApi", "_segment_code", "code_embedding", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "score", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "_retrieve_additional_context", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "best_context", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code", "context_scores", "stored_embedding"]}, {"content": "def main():\n    # Calculate circle area\n    circle_radius = 5\n    circle_area = calculate_circle_area(circle_radius)\n    print(f\"Area of circle with radius {circle_radius}: {circle_area:.2f}\")\n\n    # Calculate rectangle area\n    rect = Rectangle(4, 6)\n    rect_area = rect.calculate_area()\n    print(f\"Area of rectangle with width {rect.width} and height {rect.height}: {rect_area}\")\n\nif __name__ == \"__main__\":\n    main()\n\"\"\"\n\n    # Perform analysis with global context\n    analysis_result = await llm_api.analyze_with_global_context(code_to_analyze)\n    print(json.dumps(analysis_result, indent=2))\n\n    # Start interactive analysis session\n    await llm_api.interactive_analysis(code_to_analyze)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())", "file_path": "LLMApi_4.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\nimport os\n", "symbols": ["str", "analyze_with_global_context", "calculate_area", "api_key", "overview", "vector_model", "convert_to_tensor", "_analyze_segments_with_context", "key", "context_store", "context", "LLMApi", "_segment_code", "code_embedding", "max", "circle_radius", "summary_prompt", "__name__", "segment", "rect_area", "_load_prompts", "analysis_graph", "prompts", "_build_dependency_graph", "base_url", "segment_name", "score", "indent", "height", "rect", "global_context", "detailed_analyses", "Rectangle", "_get_segment_context", "stop", "segment_context", "prompt", "analyze", "interactive_analysis", "main", "segment_type", "analysis_prompt", "_retrieve_additional_context", "client", "summary", "llm_api", "query", "response", "min", "__init__", "circle_area", "tree", "calculate_circle_area", "wait", "segments", "_call_openai_api", "model", "analysis", "successors", "analysis_result", "answer", "timeout", "predecessors", "best_context", "messages", "type", "width", "_summarize_analysis", "analyses", "code_to_analyze", "segment_code", "context_scores", "stored_embedding"]}, {"content": "import ast\nfrom typing import List, Dict, Any\nimport networkx as nx\nfrom .file_analyzer import FileAnalyzer", "file_path": "analyzers\\business_logic_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Any\nimport networkx as nx\n", "symbols": ["business_components", "dependency_graph", "visited", "overall_summary", "file_components", "summary_prompt", "_create_business_logic_summary_prompt", "_build_dependency_graph", "_analyze_business_flows", "content", "flows", "BusinessLogicAnalyzer", "prompt", "sub_flow", "_trace_business_flow", "llm_api", "module_name", "business_flows", "flow", "__init__", "tree", "file_analyzer", "_serialize_graph", "analyze_business_logic", "encoding"]}, {"content": "class BusinessLogicAnalyzer:\n    def __init__(self, llm_api):\n        self.llm_api = llm_api\n        self.dependency_graph = nx.DiGraph()\n        self.file_analyzer = FileAnalyzer(llm_api)", "file_path": "analyzers\\business_logic_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Any\nimport networkx as nx\n", "symbols": ["business_components", "dependency_graph", "visited", "overall_summary", "file_components", "summary_prompt", "_create_business_logic_summary_prompt", "_build_dependency_graph", "_analyze_business_flows", "content", "flows", "BusinessLogicAnalyzer", "prompt", "sub_flow", "_trace_business_flow", "llm_api", "module_name", "business_flows", "flow", "__init__", "tree", "file_analyzer", "_serialize_graph", "analyze_business_logic", "encoding"]}, {"content": "async def analyze_business_logic(self, file_paths: List[str]) -> Dict[str, Any]:\n        \"\"\"\n        Analyze business logic across multiple files in the codebase.\n        \"\"\"\n        business_components = {}\n        \n        # First pass: Extract individual file business logic\n        for file_path in file_paths:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                content = f.read()\n                \n            # Parse AST and analyze file\n            tree = ast.parse(content)\n            file_components = await self.file_analyzer.analyze_file(tree, file_path)\n            business_components[file_path] = file_components\n            \n            # Build dependency graph\n            self._build_dependency_graph(tree, file_path)\n            \n        # Second pass: Analyze cross-file relationships\n        business_flows = self._analyze_business_flows(business_components)\n        \n        # Generate high-level summary", "file_path": "analyzers\\business_logic_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Any\nimport networkx as nx\n", "symbols": ["business_components", "dependency_graph", "visited", "overall_summary", "file_components", "summary_prompt", "_create_business_logic_summary_prompt", "_build_dependency_graph", "_analyze_business_flows", "content", "flows", "BusinessLogicAnalyzer", "prompt", "sub_flow", "_trace_business_flow", "llm_api", "module_name", "business_flows", "flow", "__init__", "tree", "file_analyzer", "_serialize_graph", "analyze_business_logic", "encoding"]}, {"content": "# Second pass: Analyze cross-file relationships\n        business_flows = self._analyze_business_flows(business_components)\n        \n        # Generate high-level summary\n        summary_prompt = self._create_business_logic_summary_prompt(business_components, business_flows)\n        overall_summary = await self.llm_api.analyze_code(summary_prompt)\n        \n        return {\n            \"components\": business_components,\n            \"flows\": business_flows,\n            \"summary\": overall_summary,\n            \"dependency_graph\": self._serialize_graph()\n        }", "file_path": "analyzers\\business_logic_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Any\nimport networkx as nx\n", "symbols": ["business_components", "dependency_graph", "visited", "overall_summary", "file_components", "summary_prompt", "_create_business_logic_summary_prompt", "_build_dependency_graph", "_analyze_business_flows", "content", "flows", "BusinessLogicAnalyzer", "prompt", "sub_flow", "_trace_business_flow", "llm_api", "module_name", "business_flows", "flow", "__init__", "tree", "file_analyzer", "_serialize_graph", "analyze_business_logic", "encoding"]}, {"content": "def _build_dependency_graph(self, tree: ast.AST, file_path: str):\n        \"\"\"Build a dependency graph between files based on imports.\"\"\"\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.Import, ast.ImportFrom)):\n                module_name = node.names[0].name if isinstance(node, ast.Import) else node.module\n                self.dependency_graph.add_edge(file_path, module_name)\n\n    def _analyze_business_flows(self, components: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Analyze business flows across different files.\"\"\"\n        flows = []\n        visited = set()\n        \n        for start_node in self.dependency_graph.nodes():\n            if start_node not in visited:\n                flow = self._trace_business_flow(start_node, components, visited)\n                if flow:\n                    flows.append(flow)\n                    \n        return flows", "file_path": "analyzers\\business_logic_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Any\nimport networkx as nx\n", "symbols": ["business_components", "dependency_graph", "visited", "overall_summary", "file_components", "summary_prompt", "_create_business_logic_summary_prompt", "_build_dependency_graph", "_analyze_business_flows", "content", "flows", "BusinessLogicAnalyzer", "prompt", "sub_flow", "_trace_business_flow", "llm_api", "module_name", "business_flows", "flow", "__init__", "tree", "file_analyzer", "_serialize_graph", "analyze_business_logic", "encoding"]}, {"content": "def _trace_business_flow(self, start_node: str, components: Dict[str, Any], visited: set) -> Dict[str, Any]:\n        \"\"\"Trace a business flow starting from a specific node.\"\"\"\n        if start_node in visited or start_node not in components:\n            return None\n            \n        visited.add(start_node)\n        flow = {\n            \"start\": start_node,\n            \"steps\": [],\n            \"related_components\": []\n        }\n        \n        file_components = components[start_node]\n        flow[\"related_components\"].extend(file_components[\"functions\"])\n        flow[\"related_components\"].extend(file_components[\"classes\"])\n        \n        for next_node in self.dependency_graph.successors(start_node):\n            sub_flow = self._trace_business_flow(next_node, components, visited)\n            if sub_flow:\n                flow[\"steps\"].append(sub_flow)\n                \n        return flow", "file_path": "analyzers\\business_logic_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Any\nimport networkx as nx\n", "symbols": ["business_components", "dependency_graph", "visited", "overall_summary", "file_components", "summary_prompt", "_create_business_logic_summary_prompt", "_build_dependency_graph", "_analyze_business_flows", "content", "flows", "BusinessLogicAnalyzer", "prompt", "sub_flow", "_trace_business_flow", "llm_api", "module_name", "business_flows", "flow", "__init__", "tree", "file_analyzer", "_serialize_graph", "analyze_business_logic", "encoding"]}, {"content": "def _create_business_logic_summary_prompt(self, components: Dict[str, Any], flows: List[Dict[str, Any]]) -> str:\n        \"\"\"Create a prompt for LLM to generate overall business logic summary.\"\"\"\n        prompt = \"Analyze the following business components and their relationships:\\n\\n\"\n        \n        for file_path, file_components in components.items():\n            prompt += f\"\\nFile: {file_path}\\n\"\n            prompt += \"Functions:\\n\"\n            for func in file_components[\"functions\"]:\n                prompt += f\"- {func['name']}: {func['logic']}\\n\"\n            prompt += \"Classes:\\n\"\n            for cls in file_components[\"classes\"]:\n                prompt += f\"- {cls['name']}: {cls['logic']}\\n\"\n                \n        prompt += \"\\nBusiness Flows:\\n\"\n        for flow in flows:\n            prompt += f\"Flow starting from {flow['start']}:\\n\"\n            prompt += f\"- Related components: {', '.join(c['name'] for c in flow['related_components'])}\\n\"", "file_path": "analyzers\\business_logic_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Any\nimport networkx as nx\n", "symbols": ["business_components", "dependency_graph", "visited", "overall_summary", "file_components", "summary_prompt", "_create_business_logic_summary_prompt", "_build_dependency_graph", "_analyze_business_flows", "content", "flows", "BusinessLogicAnalyzer", "prompt", "sub_flow", "_trace_business_flow", "llm_api", "module_name", "business_flows", "flow", "__init__", "tree", "file_analyzer", "_serialize_graph", "analyze_business_logic", "encoding"]}, {"content": "prompt += f\"Flow starting from {flow['start']}:\\n\"\n            prompt += f\"- Related components: {', '.join(c['name'] for c in flow['related_components'])}\\n\"\n            \n        prompt += \"\\nProvide a comprehensive summary of the business logic, including:\\n\"\n        prompt += \"1. Main business components and their responsibilities\\n\"\n        prompt += \"2. Key business flows and their interactions\\n\"\n        prompt += \"3. Important business rules and constraints\\n\"\n        \n        return prompt", "file_path": "analyzers\\business_logic_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Any\nimport networkx as nx\n", "symbols": ["business_components", "dependency_graph", "visited", "overall_summary", "file_components", "summary_prompt", "_create_business_logic_summary_prompt", "_build_dependency_graph", "_analyze_business_flows", "content", "flows", "BusinessLogicAnalyzer", "prompt", "sub_flow", "_trace_business_flow", "llm_api", "module_name", "business_flows", "flow", "__init__", "tree", "file_analyzer", "_serialize_graph", "analyze_business_logic", "encoding"]}, {"content": "def _serialize_graph(self) -> Dict[str, Any]:\n        \"\"\"Serialize the dependency graph for output.\"\"\"\n        return {\n            \"nodes\": list(self.dependency_graph.nodes()),\n            \"edges\": list(self.dependency_graph.edges())\n        }", "file_path": "analyzers\\business_logic_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Any\nimport networkx as nx\n", "symbols": ["business_components", "dependency_graph", "visited", "overall_summary", "file_components", "summary_prompt", "_create_business_logic_summary_prompt", "_build_dependency_graph", "_analyze_business_flows", "content", "flows", "BusinessLogicAnalyzer", "prompt", "sub_flow", "_trace_business_flow", "llm_api", "module_name", "business_flows", "flow", "__init__", "tree", "file_analyzer", "_serialize_graph", "analyze_business_logic", "encoding"]}, {"content": "from typing import List, Dict, Any\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np", "file_path": "analyzers\\code_indexer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n", "symbols": ["similarity", "extract_keywords", "search", "sorted_items", "query_vector", "int", "index", "CodeIndexer", "content_vector", "results", "tfidf_vectorizer", "key", "feature_names", "reverse", "add_to_index", "tfidf_matrix", "__init__"]}, {"content": "class CodeIndexer:\n    def __init__(self):\n        self.index = {}\n        self.tfidf_vectorizer = TfidfVectorizer()\n\n    def add_to_index(self, name: str, content: str, type: str):\n        self.index[name] = {\n            \"content\": content,\n            \"type\": type,\n            \"keywords\": self.extract_keywords(content)\n        }\n\n    def extract_keywords(self, content: str, top_n: int = 5) -> List[str]:\n        tfidf_matrix = self.tfidf_vectorizer.fit_transform([content])\n        feature_names = self.tfidf_vectorizer.get_feature_names_out()\n        sorted_items = sorted(zip(tfidf_matrix.tocsc().data, feature_names), key=lambda x: x[0], reverse=True)\n        return [item[1] for item in sorted_items[:top_n]]", "file_path": "analyzers\\code_indexer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n", "symbols": ["similarity", "extract_keywords", "search", "sorted_items", "query_vector", "int", "index", "CodeIndexer", "content_vector", "results", "tfidf_vectorizer", "key", "feature_names", "reverse", "add_to_index", "tfidf_matrix", "__init__"]}, {"content": "def search(self, query: str) -> List[Dict[str, Any]]:\n        query_vector = self.tfidf_vectorizer.transform([query])\n        results = []\n        for name, data in self.index.items():\n            content_vector = self.tfidf_vectorizer.transform([data['content']])\n            similarity = np.dot(query_vector.toarray(), content_vector.toarray().T)[0][0]\n            results.append({\"name\": name, \"type\": data['type'], \"similarity\": similarity})\n        return sorted(results, key=lambda x: x['similarity'], reverse=True)[:5]", "file_path": "analyzers\\code_indexer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n", "symbols": ["similarity", "extract_keywords", "search", "sorted_items", "query_vector", "int", "index", "CodeIndexer", "content_vector", "results", "tfidf_vectorizer", "key", "feature_names", "reverse", "add_to_index", "tfidf_matrix", "__init__"]}, {"content": "import ast\nfrom typing import Dict, Any", "file_path": "analyzers\\file_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import Dict, Any\n\n", "symbols": ["components", "function_prompt", "function_analysis", "_analyze_class", "_analyze_function", "class_code", "llm_api", "class_analysis", "class_prompt", "_get_file_content", "encoding", "function_code", "analyze_file", "__init__", "FileAnalyzer"]}, {"content": "class FileAnalyzer:\n    def __init__(self, llm_api):\n        self.llm_api = llm_api\n\n    async def analyze_file(self, tree: ast.AST, file_path: str) -> Dict[str, Any]:\n        \"\"\"Analyze a single file's business logic components.\"\"\"\n        components = {\n            \"functions\": [],\n            \"classes\": [],\n            \"business_rules\": []\n        }\n        \n        for node in ast.walk(tree):\n            if isinstance(node, ast.FunctionDef):\n                await self._analyze_function(node, components, file_path)\n            elif isinstance(node, ast.ClassDef):\n                await self._analyze_class(node, components, file_path)\n                \n        return components", "file_path": "analyzers\\file_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import Dict, Any\n\n", "symbols": ["components", "function_prompt", "function_analysis", "_analyze_class", "_analyze_function", "class_code", "llm_api", "class_analysis", "class_prompt", "_get_file_content", "encoding", "function_code", "analyze_file", "__init__", "FileAnalyzer"]}, {"content": "async def _analyze_function(self, node: ast.FunctionDef, components: Dict[str, Any], file_path: str):\n        \"\"\"Analyze a function's business logic.\"\"\"\n        function_code = ast.get_source_segment(self._get_file_content(file_path), node)\n        function_prompt = f\"Analyze the business logic in this function:\\n{function_code}\"\n        function_analysis = await self.llm_api.analyze_code(function_prompt)\n        \n        components[\"functions\"].append({\n            \"name\": node.name,\n            \"logic\": function_analysis,\n            \"code\": function_code\n        })", "file_path": "analyzers\\file_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import Dict, Any\n\n", "symbols": ["components", "function_prompt", "function_analysis", "_analyze_class", "_analyze_function", "class_code", "llm_api", "class_analysis", "class_prompt", "_get_file_content", "encoding", "function_code", "analyze_file", "__init__", "FileAnalyzer"]}, {"content": "async def _analyze_class(self, node: ast.ClassDef, components: Dict[str, Any], file_path: str):\n        \"\"\"Analyze a class's business logic.\"\"\"\n        class_code = ast.get_source_segment(self._get_file_content(file_path), node)\n        class_prompt = f\"Analyze the business logic in this class:\\n{class_code}\"\n        class_analysis = await self.llm_api.analyze_code(class_prompt)\n        \n        components[\"classes\"].append({\n            \"name\": node.name,\n            \"logic\": class_analysis,\n            \"code\": class_code\n        })\n\n    def _get_file_content(self, file_path: str) -> str:\n        \"\"\"Helper method to get file content.\"\"\"\n        with open(file_path, 'r', encoding='utf-8') as f:\n            return f.read()", "file_path": "analyzers\\file_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import Dict, Any\n\n", "symbols": ["components", "function_prompt", "function_analysis", "_analyze_class", "_analyze_function", "class_code", "llm_api", "class_analysis", "class_prompt", "_get_file_content", "encoding", "function_code", "analyze_file", "__init__", "FileAnalyzer"]}, {"content": "from .code_indexer import CodeIndexer\nfrom .business_logic_analyzer import BusinessLogicAnalyzer\nfrom .file_analyzer import FileAnalyzer\n\n__all__ = ['CodeIndexer', 'BusinessLogicAnalyzer', 'FileAnalyzer']", "file_path": "analyzers\\__init__.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from .code_indexer import CodeIndexer\nfrom .business_logic_analyzer import BusinessLogicAnalyzer\nfrom .file_analyzer import FileAnalyzer\n", "symbols": ["__all__"]}, {"content": "import ast\nfrom typing import List, Dict, Optional, Set\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport logging\nimport re\n\n@dataclass\nclass FunctionInfo:\n    name: str\n    docstring: Optional[str]\n    params: List[str]\n    returns: List[str]\n    calls: List[str]\n    start_line: int\n    end_line: int\n    complexity: int\n    \n@dataclass\nclass ClassInfo:\n    name: str\n    docstring: Optional[str]\n    methods: List[FunctionInfo]\n    base_classes: List[str]\n    start_line: int\n    end_line: int\n\n@dataclass\nclass ModuleInfo:\n    file_path: str\n    imports: List[str]\n    classes: List[ClassInfo]\n    functions: List[FunctionInfo]\n    docstring: Optional[str]", "file_path": "code_search\\code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Optional, Set\nfrom dataclasses import dataclass\n", "symbols": ["str", "complexity", "returns", "BusinessLogicSearcher", "_analyze_module", "_analyze_class", "name", "end_line", "calls", "class_related", "from", "code_analyzer", "language_filter", "start_line", "feature_lower", "class", "related_methods", "logic_results", "content", "module_info", "docstring", "is_related", "definition", "search_business_logic", "imports", "CodeAnalyzer", "base_classes", "_analyze_function", "methods", "all_results", "module", "k", "functions", "__init__", "search_results", "tree", "results", "classes", "find_business_logic", "analyzed_files", "file_path", "search_engine", "encoding", "analyze_file", "logger", "params"]}, {"content": "class CodeAnalyzer:\n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n    \n    def analyze_file(self, file_path: str) -> ModuleInfo:\n        \"\"\"Analyze a Python file and extract its structure and business logic.\"\"\"\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                content = f.read()\n            \n            tree = ast.parse(content)\n            return self._analyze_module(tree, file_path)\n            \n        except Exception as e:\n            self.logger.error(f\"Error analyzing file {file_path}: {str(e)}\")\n            return ModuleInfo(file_path, [], [], [], None)\n    \n    def _analyze_module(self, tree: ast.Module, file_path: str) -> ModuleInfo:\n        \"\"\"Extract information from a Python module.\"\"\"\n        imports = []\n        classes = []\n        functions = []\n        docstring = ast.get_docstring(tree)\n        \n        for node in ast.walk(tree):\n            if isinstance(node, ast.Import):", "file_path": "code_search\\code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Optional, Set\nfrom dataclasses import dataclass\n", "symbols": ["str", "complexity", "returns", "BusinessLogicSearcher", "_analyze_module", "_analyze_class", "name", "end_line", "calls", "class_related", "from", "code_analyzer", "language_filter", "start_line", "feature_lower", "class", "related_methods", "logic_results", "content", "module_info", "docstring", "is_related", "definition", "search_business_logic", "imports", "CodeAnalyzer", "base_classes", "_analyze_function", "methods", "all_results", "module", "k", "functions", "__init__", "search_results", "tree", "results", "classes", "find_business_logic", "analyzed_files", "file_path", "search_engine", "encoding", "analyze_file", "logger", "params"]}, {"content": "imports = []\n        classes = []\n        functions = []\n        docstring = ast.get_docstring(tree)\n        \n        for node in ast.walk(tree):\n            if isinstance(node, ast.Import):\n                for name in node.names:\n                    imports.append(name.name)\n            elif isinstance(node, ast.ImportFrom):\n                module = node.module or ''\n                for name in node.names:\n                    imports.append(f\"{module}.{name.name}\")\n                    \n        for node in tree.body:\n            if isinstance(node, ast.ClassDef):\n                classes.append(self._analyze_class(node))\n            elif isinstance(node, ast.FunctionDef):\n                functions.append(self._analyze_function(node))\n                \n        return ModuleInfo(file_path, imports, classes, functions, docstring)\n    \n    def _analyze_class(self, node: ast.ClassDef) -> ClassInfo:\n        \"\"\"Extract information from a class definition.\"\"\"\n        methods = []", "file_path": "code_search\\code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Optional, Set\nfrom dataclasses import dataclass\n", "symbols": ["str", "complexity", "returns", "BusinessLogicSearcher", "_analyze_module", "_analyze_class", "name", "end_line", "calls", "class_related", "from", "code_analyzer", "language_filter", "start_line", "feature_lower", "class", "related_methods", "logic_results", "content", "module_info", "docstring", "is_related", "definition", "search_business_logic", "imports", "CodeAnalyzer", "base_classes", "_analyze_function", "methods", "all_results", "module", "k", "functions", "__init__", "search_results", "tree", "results", "classes", "find_business_logic", "analyzed_files", "file_path", "search_engine", "encoding", "analyze_file", "logger", "params"]}, {"content": "def _analyze_class(self, node: ast.ClassDef) -> ClassInfo:\n        \"\"\"Extract information from a class definition.\"\"\"\n        methods = []\n        base_classes = []\n        \n        for base in node.bases:\n            if isinstance(base, ast.Name):\n                base_classes.append(base.id)\n            elif isinstance(base, ast.Attribute):\n                base_classes.append(f\"{base.value.id}.{base.attr}\")\n                \n        for child in node.body:\n            if isinstance(child, ast.FunctionDef):\n                methods.append(self._analyze_function(child))\n                \n        return ClassInfo(\n            name=node.name,\n            docstring=ast.get_docstring(node),\n            methods=methods,\n            base_classes=base_classes,\n            start_line=node.lineno,\n            end_line=node.end_lineno or node.lineno\n        )\n    \n    def _analyze_function(self, node: ast.FunctionDef) -> FunctionInfo:", "file_path": "code_search\\code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Optional, Set\nfrom dataclasses import dataclass\n", "symbols": ["str", "complexity", "returns", "BusinessLogicSearcher", "_analyze_module", "_analyze_class", "name", "end_line", "calls", "class_related", "from", "code_analyzer", "language_filter", "start_line", "feature_lower", "class", "related_methods", "logic_results", "content", "module_info", "docstring", "is_related", "definition", "search_business_logic", "imports", "CodeAnalyzer", "base_classes", "_analyze_function", "methods", "all_results", "module", "k", "functions", "__init__", "search_results", "tree", "results", "classes", "find_business_logic", "analyzed_files", "file_path", "search_engine", "encoding", "analyze_file", "logger", "params"]}, {"content": "start_line=node.lineno,\n            end_line=node.end_lineno or node.lineno\n        )\n    \n    def _analyze_function(self, node: ast.FunctionDef) -> FunctionInfo:\n        \"\"\"Extract information from a function definition.\"\"\"\n        calls = []\n        returns = []\n        complexity = 1  # Base complexity\n        \n        # Analyze function body\n        for child in ast.walk(node):\n            if isinstance(child, ast.Call):\n                if isinstance(child.func, ast.Name):\n                    calls.append(child.func.id)\n                elif isinstance(child.func, ast.Attribute):\n                    calls.append(f\"{child.func.value.id}.{child.func.attr}\")\n            elif isinstance(child, ast.Return):\n                if isinstance(child.value, ast.Name):\n                    returns.append(child.value.id)\n            # Calculate cyclomatic complexity\n            elif isinstance(child, (ast.If, ast.While, ast.For, ast.Try, ast.ExceptHandler)):", "file_path": "code_search\\code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Optional, Set\nfrom dataclasses import dataclass\n", "symbols": ["str", "complexity", "returns", "BusinessLogicSearcher", "_analyze_module", "_analyze_class", "name", "end_line", "calls", "class_related", "from", "code_analyzer", "language_filter", "start_line", "feature_lower", "class", "related_methods", "logic_results", "content", "module_info", "docstring", "is_related", "definition", "search_business_logic", "imports", "CodeAnalyzer", "base_classes", "_analyze_function", "methods", "all_results", "module", "k", "functions", "__init__", "search_results", "tree", "results", "classes", "find_business_logic", "analyzed_files", "file_path", "search_engine", "encoding", "analyze_file", "logger", "params"]}, {"content": "returns.append(child.value.id)\n            # Calculate cyclomatic complexity\n            elif isinstance(child, (ast.If, ast.While, ast.For, ast.Try, ast.ExceptHandler)):\n                complexity += 1\n                \n        # Get parameter names\n        params = [arg.arg for arg in node.args.args]\n        \n        return FunctionInfo(\n            name=node.name,\n            docstring=ast.get_docstring(node),\n            params=params,\n            returns=returns,\n            calls=list(set(calls)),  # Remove duplicates\n            start_line=node.lineno,\n            end_line=node.end_lineno or node.lineno,\n            complexity=complexity\n        )\n    \n    def find_business_logic(self, module_info: ModuleInfo, feature_description: str) -> List[Dict]:\n        \"\"\"Find code elements related to a specific business feature.\"\"\"\n        results = []\n        \n        # Convert feature description to lowercase for case-insensitive matching", "file_path": "code_search\\code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Optional, Set\nfrom dataclasses import dataclass\n", "symbols": ["str", "complexity", "returns", "BusinessLogicSearcher", "_analyze_module", "_analyze_class", "name", "end_line", "calls", "class_related", "from", "code_analyzer", "language_filter", "start_line", "feature_lower", "class", "related_methods", "logic_results", "content", "module_info", "docstring", "is_related", "definition", "search_business_logic", "imports", "CodeAnalyzer", "base_classes", "_analyze_function", "methods", "all_results", "module", "k", "functions", "__init__", "search_results", "tree", "results", "classes", "find_business_logic", "analyzed_files", "file_path", "search_engine", "encoding", "analyze_file", "logger", "params"]}, {"content": "\"\"\"Find code elements related to a specific business feature.\"\"\"\n        results = []\n        \n        # Convert feature description to lowercase for case-insensitive matching\n        feature_lower = feature_description.lower()\n        \n        # Helper function to check if text is related to feature\n        def is_related(text: Optional[str]) -> bool:\n            if not text:\n                return False\n            return any(word in text.lower() for word in feature_lower.split())\n        \n        # Check module-level docstring\n        if is_related(module_info.docstring):\n            results.append({\n                'type': 'module',\n                'file': module_info.file_path,\n                'docstring': module_info.docstring\n            })\n        \n        # Check classes\n        for class_info in module_info.classes:\n            class_related = False\n            \n            if is_related(class_info.docstring):\n                class_related = True", "file_path": "code_search\\code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Optional, Set\nfrom dataclasses import dataclass\n", "symbols": ["str", "complexity", "returns", "BusinessLogicSearcher", "_analyze_module", "_analyze_class", "name", "end_line", "calls", "class_related", "from", "code_analyzer", "language_filter", "start_line", "feature_lower", "class", "related_methods", "logic_results", "content", "module_info", "docstring", "is_related", "definition", "search_business_logic", "imports", "CodeAnalyzer", "base_classes", "_analyze_function", "methods", "all_results", "module", "k", "functions", "__init__", "search_results", "tree", "results", "classes", "find_business_logic", "analyzed_files", "file_path", "search_engine", "encoding", "analyze_file", "logger", "params"]}, {"content": "for class_info in module_info.classes:\n            class_related = False\n            \n            if is_related(class_info.docstring):\n                class_related = True\n            \n            related_methods = []\n            for method in class_info.methods:\n                if is_related(method.docstring):\n                    related_methods.append({\n                        'name': method.name,\n                        'docstring': method.docstring,\n                        'params': method.params,\n                        'calls': method.calls,\n                        'complexity': method.complexity,\n                        'lines': (method.start_line, method.end_line)\n                    })\n            \n            if class_related or related_methods:\n                results.append({\n                    'type': 'class',\n                    'name': class_info.name,\n                    'docstring': class_info.docstring,", "file_path": "code_search\\code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Optional, Set\nfrom dataclasses import dataclass\n", "symbols": ["str", "complexity", "returns", "BusinessLogicSearcher", "_analyze_module", "_analyze_class", "name", "end_line", "calls", "class_related", "from", "code_analyzer", "language_filter", "start_line", "feature_lower", "class", "related_methods", "logic_results", "content", "module_info", "docstring", "is_related", "definition", "search_business_logic", "imports", "CodeAnalyzer", "base_classes", "_analyze_function", "methods", "all_results", "module", "k", "functions", "__init__", "search_results", "tree", "results", "classes", "find_business_logic", "analyzed_files", "file_path", "search_engine", "encoding", "analyze_file", "logger", "params"]}, {"content": "results.append({\n                    'type': 'class',\n                    'name': class_info.name,\n                    'docstring': class_info.docstring,\n                    'base_classes': class_info.base_classes,\n                    'methods': related_methods,\n                    'lines': (class_info.start_line, class_info.end_line)\n                })\n        \n        # Check standalone functions\n        for func in module_info.functions:\n            if is_related(func.docstring):\n                results.append({\n                    'type': 'function',\n                    'name': func.name,\n                    'docstring': func.docstring,\n                    'params': func.params,\n                    'calls': func.calls,\n                    'complexity': func.complexity,\n                    'lines': (func.start_line, func.end_line)\n                })\n        \n        return results", "file_path": "code_search\\code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Optional, Set\nfrom dataclasses import dataclass\n", "symbols": ["str", "complexity", "returns", "BusinessLogicSearcher", "_analyze_module", "_analyze_class", "name", "end_line", "calls", "class_related", "from", "code_analyzer", "language_filter", "start_line", "feature_lower", "class", "related_methods", "logic_results", "content", "module_info", "docstring", "is_related", "definition", "search_business_logic", "imports", "CodeAnalyzer", "base_classes", "_analyze_function", "methods", "all_results", "module", "k", "functions", "__init__", "search_results", "tree", "results", "classes", "find_business_logic", "analyzed_files", "file_path", "search_engine", "encoding", "analyze_file", "logger", "params"]}, {"content": "class BusinessLogicSearcher:\n    def __init__(self, search_engine, code_analyzer):\n        self.search_engine = search_engine\n        self.code_analyzer = code_analyzer\n        self.logger = logging.getLogger(__name__)\n    \n    def search_business_logic(self, feature_description: str, language_filter: str = 'python') -> List[Dict]:\n        \"\"\"Search for business logic related to a specific feature.\"\"\"\n        # First use search engine to find relevant files\n        search_results = self.search_engine.search(feature_description, k=5, language_filter=language_filter)\n        \n        all_results = []\n        analyzed_files = set()\n        \n        for result in search_results:\n            file_path = str(Path(result.snippet.repo_path) / result.snippet.file_path)\n            \n            # Skip if file was already analyzed\n            if file_path in analyzed_files:\n                continue\n                \n            analyzed_files.add(file_path)", "file_path": "code_search\\code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Optional, Set\nfrom dataclasses import dataclass\n", "symbols": ["str", "complexity", "returns", "BusinessLogicSearcher", "_analyze_module", "_analyze_class", "name", "end_line", "calls", "class_related", "from", "code_analyzer", "language_filter", "start_line", "feature_lower", "class", "related_methods", "logic_results", "content", "module_info", "docstring", "is_related", "definition", "search_business_logic", "imports", "CodeAnalyzer", "base_classes", "_analyze_function", "methods", "all_results", "module", "k", "functions", "__init__", "search_results", "tree", "results", "classes", "find_business_logic", "analyzed_files", "file_path", "search_engine", "encoding", "analyze_file", "logger", "params"]}, {"content": "# Skip if file was already analyzed\n            if file_path in analyzed_files:\n                continue\n                \n            analyzed_files.add(file_path)\n            \n            # Only analyze Python files\n            if not file_path.endswith('.py'):\n                continue\n                \n            # Analyze the file\n            module_info = self.code_analyzer.analyze_file(file_path)\n            \n            # Find business logic related to the feature\n            logic_results = self.code_analyzer.find_business_logic(module_info, feature_description)\n            \n            if logic_results:\n                all_results.append({\n                    'file': file_path,\n                    'score': result.score,\n                    'elements': logic_results\n                })\n        \n        return all_results", "file_path": "code_search\\code_analyzer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import ast\nfrom typing import List, Dict, Optional, Set\nfrom dataclasses import dataclass\n", "symbols": ["str", "complexity", "returns", "BusinessLogicSearcher", "_analyze_module", "_analyze_class", "name", "end_line", "calls", "class_related", "from", "code_analyzer", "language_filter", "start_line", "feature_lower", "class", "related_methods", "logic_results", "content", "module_info", "docstring", "is_related", "definition", "search_business_logic", "imports", "CodeAnalyzer", "base_classes", "_analyze_function", "methods", "all_results", "module", "k", "functions", "__init__", "search_results", "tree", "results", "classes", "find_business_logic", "analyzed_files", "file_path", "search_engine", "encoding", "analyze_file", "logger", "params"]}, {"content": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\nimport logging\nfrom dataclasses import dataclass\nimport numpy as np\nfrom .layered_index import LayeredIndex\nfrom .incremental_indexer import IncrementalIndexer\nfrom .model_fusion import ModelFusion\nimport ast\nfrom collections import defaultdict\n\n@dataclass\nclass SearchConfig:\n    \"\"\"\u641c\u7d22\u914d\u7f6e\"\"\"\n    min_score: float = 0.5\n    max_results: int = 10\n    language_filter: Optional[str] = None\n    search_layers: List[str] = None  # \u6307\u5b9a\u641c\u7d22\u7684\u7d22\u5f15\u5c42\uff0c\u5982['semantic', 'text', 'dependency']\n    include_context: bool = True\n    include_symbols: bool = True\n    include_dependencies: bool = True\n    \nclass SearchResult:\n    def __init__(self, file_path: str, score: float, context: str, metadata: Dict[str, Any]):\n        self.file_path = file_path\n        self.score = score\n        self.context = context\n        self.metadata = metadata", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "class EnhancedSearchEngine:\n    def __init__(self, \n                 watch_dirs: List[str],\n                 models_config: List[Dict[str, Any]] = None,\n                 cache_dir: str = None):\n        \"\"\"\n        \u521d\u59cb\u5316\u589e\u5f3a\u7248\u641c\u7d22\u5f15\u64ce\n        \n        Args:\n            watch_dirs: \u8981\u76d1\u63a7\u7684\u76ee\u5f55\u5217\u8868\n            models_config: \u6a21\u578b\u914d\u7f6e\u5217\u8868\uff0c\u7528\u4e8e\u591a\u6a21\u578b\u878d\u5408\n            cache_dir: \u7f13\u5b58\u76ee\u5f55\uff0c\u7528\u4e8e\u4fdd\u5b58\u7d22\u5f15\u72b6\u6001\n        \"\"\"\n        # \u521d\u59cb\u5316\u65e5\u5fd7\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.INFO)\n        \n        # \u521d\u59cb\u5316\u7f13\u5b58\u76ee\u5f55\n        self.cache_dir = cache_dir\n        if cache_dir:\n            os.makedirs(cache_dir, exist_ok=True)\n            \n        # \u521d\u59cb\u5316\u9ed8\u8ba4\u6a21\u578b\u914d\u7f6e\n        if models_config is None:\n            models_config = [\n                {\n                    'name': 'sentence-bert',\n                    'type': 'sentence_transformers',\n                    'model_name': 'all-MiniLM-L6-v2',\n                    'weight': 0.4\n                },\n                {\n                    'name': 'code-bert',", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "'model_name': 'all-MiniLM-L6-v2',\n                    'weight': 0.4\n                },\n                {\n                    'name': 'code-bert',\n                    'type': 'huggingface',\n                    'model_name': 'microsoft/codebert-base',\n                    'weight': 0.4\n                },\n                {\n                    'name': 'tfidf',\n                    'type': 'tfidf',\n                    'weight': 0.2\n                }\n            ]\n            \n        # \u521d\u59cb\u5316\u7ec4\u4ef6\n        self.model_fusion = ModelFusion(models_config)\n        self.layered_index = LayeredIndex()\n        self.incremental_indexer = IncrementalIndexer(self.layered_index, watch_dirs)\n        \n        # \u52a0\u8f7d\u7f13\u5b58\u7684\u72b6\u6001\n        if cache_dir and os.path.exists(os.path.join(cache_dir, 'index')):\n            self.load_state()\n        else:\n            # \u6784\u5efa\u521d\u59cb\u7d22\u5f15\n            self.incremental_indexer.build_initial_index()\n            \n        # \u542f\u52a8\u6587\u4ef6\u76d1\u63a7", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "self.load_state()\n        else:\n            # \u6784\u5efa\u521d\u59cb\u7d22\u5f15\n            self.incremental_indexer.build_initial_index()\n            \n        # \u542f\u52a8\u6587\u4ef6\u76d1\u63a7\n        self.incremental_indexer.start_watching()\n        \n    def search(self, query: str, config: SearchConfig = None) -> List[SearchResult]:\n        \"\"\"\n        \u6267\u884c\u589e\u5f3a\u641c\u7d22\uff0c\u6574\u5408\u591a\u4e2a\u5c42\u6b21\u7684\u641c\u7d22\u7ed3\u679c\n        \"\"\"\n        if config is None:\n            config = SearchConfig()\n            \n        # 1. \u57fa\u7840\u641c\u7d22\n        basic_results = self._basic_search(query, config)\n        \n        # 2. AST\u5206\u6790\u589e\u5f3a\n        ast_results = self._ast_enhanced_search(query)\n        \n        # 3. \u8bed\u4e49\u589e\u5f3a\n        semantic_results = self._semantic_enhanced_search(query)\n        \n        # 4. \u6574\u5408\u7ed3\u679c\n        combined_results = self._combine_search_results(\n            basic_results, \n            ast_results,\n            semantic_results,\n            weights={\n                'basic': 0.3,\n                'ast': 0.3,\n                'semantic': 0.4\n            }\n        )", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "ast_results,\n            semantic_results,\n            weights={\n                'basic': 0.3,\n                'ast': 0.3,\n                'semantic': 0.4\n            }\n        )\n        \n        return combined_results[:config.max_results]", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "def _basic_search(self, query: str, config: SearchConfig) -> List[SearchResult]:\n        \"\"\"\n        \u57fa\u7840\u641c\u7d22\n        \"\"\"\n        results = []\n        for layer in config.search_layers or ['all']:\n            layer_results = self.layered_index.search(\n                query=query,\n                layer=layer,\n                top_k=config.max_results\n            )\n            \n            # \u5e94\u7528\u8fc7\u6ee4\u6761\u4ef6\n            filtered_results = []\n            for result in layer_results:\n                # \u68c0\u67e5\u5206\u6570\u9608\u503c\n                if result['score'] < config.min_score:\n                    continue\n                    \n                # \u68c0\u67e5\u8bed\u8a00\u8fc7\u6ee4\n                metadata = result['metadata']\n                if config.language_filter and metadata.get('language') != config.language_filter:\n                    continue\n                    \n                # \u6839\u636e\u914d\u7f6e\u6dfb\u52a0\u989d\u5916\u4fe1\u606f\n                if not config.include_context:\n                    result.pop('context', None)\n                if not config.include_symbols:", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "# \u6839\u636e\u914d\u7f6e\u6dfb\u52a0\u989d\u5916\u4fe1\u606f\n                if not config.include_context:\n                    result.pop('context', None)\n                if not config.include_symbols:\n                    result.pop('symbols', None)\n                if not config.include_dependencies:\n                    result.pop('dependencies', None)\n                    \n                filtered_results.append(SearchResult(\n                    file_path=result['file_path'],\n                    score=result['score'],\n                    context=result.get('context', ''),\n                    metadata=result['metadata']\n                ))\n                \n            results.extend(filtered_results)\n            \n        # \u5bf9\u6240\u6709\u7ed3\u679c\u6309\u5206\u6570\u6392\u5e8f\n        results.sort(key=lambda x: x.score, reverse=True)\n        \n        return results", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "def _ast_enhanced_search(self, query: str) -> List[SearchResult]:\n        \"\"\"\n        \u57fa\u4e8eAST\u7684\u589e\u5f3a\u641c\u7d22\n        \"\"\"\n        results = []\n        for file_path in self.incremental_indexer.indexed_files:\n            try:\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                tree = ast.parse(content)\n                visitor = CodeStructureVisitor()\n                visitor.visit(tree)\n                \n                # \u5206\u6790\u63a7\u5236\u6d41\n                cfg = self._build_control_flow_graph(tree)\n                \n                # \u5206\u6790\u6570\u636e\u6d41\n                dfg = self._build_data_flow_graph(tree)\n                \n                # \u8bc6\u522b\u8bbe\u8ba1\u6a21\u5f0f\n                patterns = self._detect_design_patterns(tree)\n                \n                # \u8ba1\u7b97\u76f8\u5173\u6027\u5f97\u5206\n                score = self._calculate_structure_similarity(\n                    query,\n                    visitor.structure_info,\n                    cfg,\n                    dfg,", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "score = self._calculate_structure_similarity(\n                    query,\n                    visitor.structure_info,\n                    cfg,\n                    dfg,\n                    patterns\n                )\n                \n                if score > self.min_score:\n                    results.append(SearchResult(\n                        file_path=file_path,\n                        score=score,\n                        context=visitor.get_context(),\n                        metadata={\n                            'patterns': patterns,\n                            'complexity': visitor.complexity\n                        }\n                    ))\n            except Exception as e:\n                self.logger.warning(f\"AST analysis failed for {file_path}: {str(e)}\")\n                \n        return sorted(results, key=lambda x: x.score, reverse=True)", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "def _semantic_enhanced_search(self, query: str) -> List[SearchResult]:\n        \"\"\"\n        \u8bed\u4e49\u589e\u5f3a\u641c\u7d22\n        \"\"\"\n        results = []\n        \n        # 1. \u63d0\u53d6\u67e5\u8be2\u610f\u56fe\n        query_intent = self._extract_query_intent(query)\n        \n        # 2. \u5206\u6790\u4ee3\u7801\u6587\u6863\n        for file_path in self.incremental_indexer.indexed_files:\n            try:\n                doc_info = self._extract_documentation(file_path)\n                \n                # \u5206\u6790\u6ce8\u91ca\n                comments = self._extract_comments(file_path)\n                \n                # \u5206\u6790\u53d8\u66f4\u5386\u53f2\n                history = self._analyze_change_history(file_path)\n                \n                # \u8ba1\u7b97\u8bed\u4e49\u76f8\u5173\u6027\n                score = self._calculate_semantic_similarity(\n                    query_intent,\n                    doc_info,\n                    comments,\n                    history\n                )\n                \n                if score > self.min_score:\n                    results.append(SearchResult(", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "comments,\n                    history\n                )\n                \n                if score > self.min_score:\n                    results.append(SearchResult(\n                        file_path=file_path,\n                        score=score,\n                        context=doc_info.summary,\n                        metadata={\n                            'doc_quality': doc_info.quality_score,\n                            'last_modified': history.last_change,\n                            'change_frequency': history.frequency\n                        }\n                    ))\n            except Exception as e:\n                self.logger.warning(f\"Semantic analysis failed for {file_path}: {str(e)}\")\n                \n        return sorted(results, key=lambda x: x.score, reverse=True)", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "def _combine_search_results(\n        self,\n        basic_results: List[SearchResult],\n        ast_results: List[SearchResult],\n        semantic_results: List[SearchResult],\n        weights: Dict[str, float]\n    ) -> List[SearchResult]:\n        \"\"\"\n        \u7ec4\u5408\u591a\u4e2a\u641c\u7d22\u7ed3\u679c\uff0c\u4f7f\u7528\u52a0\u6743\u5f97\u5206\n        \"\"\"\n        combined_scores = defaultdict(float)\n        result_metadata = {}\n        \n        # 1. \u5408\u5e76\u57fa\u7840\u641c\u7d22\u7ed3\u679c\n        for result in basic_results:\n            combined_scores[result.file_path] += result.score * weights['basic']\n            result_metadata[result.file_path] = result.metadata\n            \n        # 2. \u5408\u5e76AST\u5206\u6790\u7ed3\u679c\n        for result in ast_results:\n            combined_scores[result.file_path] += result.score * weights['ast']\n            if result.file_path in result_metadata:\n                result_metadata[result.file_path].update(result.metadata)\n            else:\n                result_metadata[result.file_path] = result.metadata\n                \n        # 3. \u5408\u5e76\u8bed\u4e49\u5206\u6790\u7ed3\u679c", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "result_metadata[result.file_path].update(result.metadata)\n            else:\n                result_metadata[result.file_path] = result.metadata\n                \n        # 3. \u5408\u5e76\u8bed\u4e49\u5206\u6790\u7ed3\u679c\n        for result in semantic_results:\n            combined_scores[result.file_path] += result.score * weights['semantic']\n            if result.file_path in result_metadata:\n                result_metadata[result.file_path].update(result.metadata)\n            else:\n                result_metadata[result.file_path] = result.metadata\n                \n        # 4. \u521b\u5efa\u6700\u7ec8\u7ed3\u679c\u5217\u8868\n        final_results = []\n        for file_path, score in combined_scores.items():\n            final_results.append(SearchResult(\n                file_path=file_path,\n                score=score,\n                context=self._get_best_context(file_path, basic_results, ast_results, semantic_results),\n                metadata=result_metadata[file_path]\n            ))", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "context=self._get_best_context(file_path, basic_results, ast_results, semantic_results),\n                metadata=result_metadata[file_path]\n            ))\n            \n        return sorted(final_results, key=lambda x: x.score, reverse=True)", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "def _build_control_flow_graph(self, tree: ast.AST) -> Dict[str, List[str]]:\n        \"\"\"\u6784\u5efa\u63a7\u5236\u6d41\u56fe\n        \n        Args:\n            tree: AST\u8bed\u6cd5\u6811\n            \n        Returns:\n            \u63a7\u5236\u6d41\u56fe\uff0ckey\u4e3a\u8282\u70b9ID\uff0cvalue\u4e3a\u540e\u7ee7\u8282\u70b9\u5217\u8868\n        \"\"\"\n        cfg = {}\n        current_node = None\n        \n        class CFGVisitor(ast.NodeVisitor):\n            def __init__(self):\n                self.node_counter = 0\n                self.current_node = None\n                self.cfg = {}\n                \n            def get_node_id(self):\n                self.node_counter += 1\n                return f\"node_{self.node_counter}\"\n                \n            def visit_FunctionDef(self, node):\n                node_id = self.get_node_id()\n                self.cfg[node_id] = []\n                prev_node = self.current_node\n                self.current_node = node_id\n                \n                # \u8bbf\u95ee\u51fd\u6570\u4f53\n                for stmt in node.body:\n                    self.visit(stmt)", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "self.current_node = node_id\n                \n                # \u8bbf\u95ee\u51fd\u6570\u4f53\n                for stmt in node.body:\n                    self.visit(stmt)\n                    \n                self.current_node = prev_node\n                \n            def visit_If(self, node):\n                if_id = self.get_node_id()\n                self.cfg[if_id] = []\n                \n                if self.current_node:\n                    self.cfg[self.current_node].append(if_id)\n                \n                # \u8bbf\u95eeif\u5206\u652f\n                prev_node = self.current_node\n                self.current_node = if_id\n                for stmt in node.body:\n                    self.visit(stmt)\n                \n                # \u8bbf\u95eeelse\u5206\u652f\n                if node.orelse:\n                    else_id = self.get_node_id()\n                    self.cfg[else_id] = []\n                    self.cfg[if_id].append(else_id)\n                    self.current_node = else_id\n                    for stmt in node.orelse:", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "self.cfg[else_id] = []\n                    self.cfg[if_id].append(else_id)\n                    self.current_node = else_id\n                    for stmt in node.orelse:\n                        self.visit(stmt)\n                        \n                self.current_node = prev_node\n                \n            def visit_While(self, node):\n                while_id = self.get_node_id()\n                self.cfg[while_id] = []\n                \n                if self.current_node:\n                    self.cfg[self.current_node].append(while_id)\n                \n                # \u8bbf\u95ee\u5faa\u73af\u4f53\n                prev_node = self.current_node\n                self.current_node = while_id\n                for stmt in node.body:\n                    self.visit(stmt)\n                    \n                # \u5faa\u73af\u56de\u8fb9\n                self.cfg[self.current_node].append(while_id)\n                self.current_node = prev_node\n                \n            def generic_visit(self, node):", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "# \u5faa\u73af\u56de\u8fb9\n                self.cfg[self.current_node].append(while_id)\n                self.current_node = prev_node\n                \n            def generic_visit(self, node):\n                if isinstance(node, ast.stmt):\n                    node_id = self.get_node_id()\n                    self.cfg[node_id] = []\n                    if self.current_node:\n                        self.cfg[self.current_node].append(node_id)\n                    self.current_node = node_id\n                super().generic_visit(node)\n                \n        visitor = CFGVisitor()\n        visitor.visit(tree)\n        return visitor.cfg", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "def _build_data_flow_graph(self, tree: ast.AST) -> Dict[str, Set[str]]:\n        \"\"\"\u6784\u5efa\u6570\u636e\u6d41\u56fe\n        \n        Args:\n            tree: AST\u8bed\u6cd5\u6811\n            \n        Returns:\n            \u6570\u636e\u6d41\u56fe\uff0ckey\u4e3a\u53d8\u91cf\u540d\uff0cvalue\u4e3a\u4f9d\u8d56\u53d8\u91cf\u96c6\u5408\n        \"\"\"\n        dfg = {}\n        \n        class DFGVisitor(ast.NodeVisitor):\n            def __init__(self):\n                self.dfg = {}\n                self.current_scope = set()\n                \n            def visit_Assign(self, node):\n                # \u83b7\u53d6\u88ab\u8d4b\u503c\u7684\u53d8\u91cf\n                targets = []\n                for target in node.targets:\n                    if isinstance(target, ast.Name):\n                        targets.append(target.id)\n                        \n                # \u5206\u6790\u53f3\u4fa7\u8868\u8fbe\u5f0f\u4e2d\u7684\u53d8\u91cf\u4f9d\u8d56\n                deps = set()\n                class VarCollector(ast.NodeVisitor):\n                    def visit_Name(self, node):\n                        if isinstance(node.ctx, ast.Load):\n                            deps.add(node.id)", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "def visit_Name(self, node):\n                        if isinstance(node.ctx, ast.Load):\n                            deps.add(node.id)\n                            \n                collector = VarCollector()\n                collector.visit(node.value)\n                \n                # \u66f4\u65b0\u6570\u636e\u6d41\u56fe\n                for target in targets:\n                    self.dfg[target] = deps\n                    \n            def visit_FunctionDef(self, node):\n                # \u5206\u6790\u51fd\u6570\u53c2\u6570\n                params = set()\n                for arg in node.args.args:\n                    params.add(arg.arg)\n                    \n                # \u4fdd\u5b58\u5f53\u524d\u4f5c\u7528\u57df\n                prev_scope = self.current_scope\n                self.current_scope = params\n                \n                # \u5206\u6790\u51fd\u6570\u4f53\n                for stmt in node.body:\n                    self.visit(stmt)\n                    \n                # \u6062\u590d\u4f5c\u7528\u57df\n                self.current_scope = prev_scope\n                \n        visitor = DFGVisitor()", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "self.visit(stmt)\n                    \n                # \u6062\u590d\u4f5c\u7528\u57df\n                self.current_scope = prev_scope\n                \n        visitor = DFGVisitor()\n        visitor.visit(tree)\n        return visitor.dfg", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "def _detect_design_patterns(self, tree: ast.AST) -> List[Dict[str, Any]]:\n        \"\"\"\u8bc6\u522b\u4ee3\u7801\u4e2d\u7684\u8bbe\u8ba1\u6a21\u5f0f\n        \n        Args:\n            tree: AST\u8bed\u6cd5\u6811\n            \n        Returns:\n            \u8bc6\u522b\u51fa\u7684\u8bbe\u8ba1\u6a21\u5f0f\u5217\u8868\uff0c\u6bcf\u4e2a\u6a21\u5f0f\u5305\u542b\u7c7b\u578b\u548c\u76f8\u5173\u8282\u70b9\n        \"\"\"\n        patterns = []\n        \n        class PatternDetector(ast.NodeVisitor):\n            def __init__(self):\n                self.classes = {}\n                self.current_class = None\n                \n            def visit_ClassDef(self, node):\n                class_info = {\n                    'name': node.name,\n                    'bases': [base.id for base in node.bases if isinstance(base, ast.Name)],\n                    'methods': set(),\n                    'fields': set()\n                }\n                \n                prev_class = self.current_class\n                self.current_class = class_info\n                \n                # \u5206\u6790\u7c7b\u6210\u5458\n                for stmt in node.body:\n                    if isinstance(stmt, ast.FunctionDef):", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "self.current_class = class_info\n                \n                # \u5206\u6790\u7c7b\u6210\u5458\n                for stmt in node.body:\n                    if isinstance(stmt, ast.FunctionDef):\n                        class_info['methods'].add(stmt.name)\n                    elif isinstance(stmt, ast.Assign):\n                        for target in stmt.targets:\n                            if isinstance(target, ast.Name):\n                                class_info['fields'].add(target.id)\n                                \n                self.classes[node.name] = class_info\n                self.current_class = prev_class\n                \n            def detect_patterns(self):\n                patterns = []\n                \n                # \u68c0\u6d4b\u5355\u4f8b\u6a21\u5f0f\n                for name, info in self.classes.items():\n                    if '_instance' in info['fields'] and '__new__' in info['methods']:\n                        patterns.append({\n                            'type': 'Singleton',", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "if '_instance' in info['fields'] and '__new__' in info['methods']:\n                        patterns.append({\n                            'type': 'Singleton',\n                            'class': name\n                        })\n                        \n                # \u68c0\u6d4b\u5de5\u5382\u6a21\u5f0f\n                for name, info in self.classes.items():\n                    if 'create' in info['methods'] or 'factory' in info['methods']:\n                        patterns.append({\n                            'type': 'Factory',\n                            'class': name\n                        })\n                        \n                # \u68c0\u6d4b\u89c2\u5bdf\u8005\u6a21\u5f0f\n                for name, info in self.classes.items():\n                    if all(m in info['methods'] for m in ['attach', 'detach', 'notify']):\n                        patterns.append({\n                            'type': 'Observer',\n                            'class': name\n                        })", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "patterns.append({\n                            'type': 'Observer',\n                            'class': name\n                        })\n                        \n                return patterns\n                \n        detector = PatternDetector()\n        detector.visit(tree)\n        return detector.detect_patterns()", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "def _calculate_structure_similarity(\n        self,\n        query: str,\n        structure_info: Dict[str, Any],\n        cfg: Dict[str, List[str]],\n        dfg: Dict[str, Set[str]],\n        patterns: List[Dict[str, Any]]\n    ) -> float:\n        \"\"\"\u8ba1\u7b97\u4ee3\u7801\u7ed3\u6784\u76f8\u4f3c\u5ea6\n        \n        Args:\n            query: \u641c\u7d22\u67e5\u8be2\n            structure_info: \u4ee3\u7801\u7ed3\u6784\u4fe1\u606f\n            cfg: \u63a7\u5236\u6d41\u56fe\n            dfg: \u6570\u636e\u6d41\u56fe\n            patterns: \u8bc6\u522b\u51fa\u7684\u8bbe\u8ba1\u6a21\u5f0f\n            \n        Returns:\n            \u76f8\u4f3c\u5ea6\u5206\u6570 (0-1)\n        \"\"\"\n        score = 0.0\n        \n        # 1. \u57fa\u4e8e\u63a7\u5236\u6d41\u7684\u76f8\u4f3c\u5ea6 (30%)\n        if cfg:\n            cfg_complexity = len(cfg)  # \u8282\u70b9\u6570\u91cf\n            cfg_depth = max(self._calculate_graph_depth(cfg, node) for node in cfg)\n            cfg_score = 0.3 * (1.0 / (1 + abs(cfg_complexity - 10)))  # \u5047\u8bbe\u7406\u60f3\u590d\u6742\u5ea6\u4e3a10\n            score += cfg_score\n            \n        # 2. \u57fa\u4e8e\u6570\u636e\u6d41\u7684\u76f8\u4f3c\u5ea6 (30%)\n        if dfg:\n            dfg_complexity = sum(len(deps) for deps in dfg.values())  # \u603b\u4f9d\u8d56\u6570", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "score += cfg_score\n            \n        # 2. \u57fa\u4e8e\u6570\u636e\u6d41\u7684\u76f8\u4f3c\u5ea6 (30%)\n        if dfg:\n            dfg_complexity = sum(len(deps) for deps in dfg.values())  # \u603b\u4f9d\u8d56\u6570\n            dfg_score = 0.3 * (1.0 / (1 + abs(dfg_complexity - 5)))  # \u5047\u8bbe\u7406\u60f3\u4f9d\u8d56\u6570\u4e3a5\n            score += dfg_score\n            \n        # 3. \u57fa\u4e8e\u8bbe\u8ba1\u6a21\u5f0f\u7684\u76f8\u4f3c\u5ea6 (40%)\n        if patterns:\n            # \u5c06\u67e5\u8be2\u5206\u8bcd\n            query_terms = set(query.lower().split())\n            \n            # \u8ba1\u7b97\u67e5\u8be2\u4e0e\u8bbe\u8ba1\u6a21\u5f0f\u7684\u5339\u914d\u5ea6\n            pattern_score = 0.0\n            for pattern in patterns:\n                pattern_terms = set(pattern['type'].lower().split())\n                if pattern_terms & query_terms:  # \u5982\u679c\u6709\u91cd\u53e0\u8bcd\n                    pattern_score += 0.4 / len(patterns)\n                    \n            score += pattern_score\n            \n        return min(1.0, score)", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "def _calculate_graph_depth(self, graph: Dict[str, List[str]], start_node: str, visited: Set[str] = None) -> int:\n        \"\"\"\u8ba1\u7b97\u56fe\u7684\u6df1\u5ea6\n        \n        Args:\n            graph: \u56fe\u7684\u90bb\u63a5\u8868\u8868\u793a\n            start_node: \u8d77\u59cb\u8282\u70b9\n            visited: \u5df2\u8bbf\u95ee\u8282\u70b9\u96c6\u5408\n            \n        Returns:\n            \u4ece\u8d77\u59cb\u8282\u70b9\u51fa\u53d1\u7684\u6700\u5927\u6df1\u5ea6\n        \"\"\"\n        if visited is None:\n            visited = set()\n            \n        if start_node in visited:\n            return 0\n            \n        visited.add(start_node)\n        \n        if not graph[start_node]:  # \u53f6\u5b50\u8282\u70b9\n            return 1\n            \n        max_depth = 0\n        for next_node in graph[start_node]:\n            depth = self._calculate_graph_depth(graph, next_node, visited)\n            max_depth = max(max_depth, depth)\n            \n        return 1 + max_depth", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "def _extract_query_intent(self, query: str) -> Dict[str, Any]:\n        \"\"\"\u63d0\u53d6\u67e5\u8be2\u610f\u56fe\n        \n        Args:\n            query: \u641c\u7d22\u67e5\u8be2\n            \n        Returns:\n            \u67e5\u8be2\u610f\u56fe\u4fe1\u606f\uff0c\u5305\u542b\u610f\u56fe\u7c7b\u578b\u548c\u5173\u952e\u8bcd\n        \"\"\"\n        # \u5b9a\u4e49\u610f\u56fe\u5173\u952e\u8bcd\u6620\u5c04\n        intent_keywords = {\n            'implementation': {'implement', 'how', 'example', 'usage'},\n            'definition': {'what', 'define', 'declaration'},\n            'bug': {'error', 'bug', 'fix', 'issue'},\n            'performance': {'slow', 'performance', 'optimize'},\n            'security': {'security', 'vulnerability', 'safe'},\n            'test': {'test', 'assert', 'verify'}\n        }\n        \n        # \u5c06\u67e5\u8be2\u8f6c\u6362\u4e3a\u5c0f\u5199\u5e76\u5206\u8bcd\n        query_terms = set(query.lower().split())\n        \n        # \u8bc6\u522b\u610f\u56fe\n        intents = []\n        for intent, keywords in intent_keywords.items():\n            if query_terms & keywords:\n                intents.append(intent)\n                \n        # \u63d0\u53d6\u5173\u952e\u8bcd\uff08\u6392\u9664\u610f\u56fe\u5173\u952e\u8bcd\uff09\n        all_intent_keywords = set().union(*intent_keywords.values())", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "if query_terms & keywords:\n                intents.append(intent)\n                \n        # \u63d0\u53d6\u5173\u952e\u8bcd\uff08\u6392\u9664\u610f\u56fe\u5173\u952e\u8bcd\uff09\n        all_intent_keywords = set().union(*intent_keywords.values())\n        keywords = query_terms - all_intent_keywords\n        \n        return {\n            'intents': intents or ['general'],\n            'keywords': list(keywords),\n            'original_query': query\n        }", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "def _extract_documentation(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"\u63d0\u53d6\u4ee3\u7801\u6587\u6863\u4fe1\u606f\n        \n        Args:\n            file_path: \u6587\u4ef6\u8def\u5f84\n            \n        Returns:\n            \u6587\u6863\u4fe1\u606f\uff0c\u5305\u542b\u6458\u8981\u3001\u8d28\u91cf\u8bc4\u5206\u7b49\n        \"\"\"\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                content = f.read()\n                \n            tree = ast.parse(content)\n            \n            doc_info = {\n                'summary': '',\n                'quality_score': 0.0,\n                'docstrings': [],\n                'todos': []\n            }\n            \n            class DocVisitor(ast.NodeVisitor):\n                def visit_Module(self, node):\n                    if ast.get_docstring(node):\n                        doc_info['docstrings'].append({\n                            'type': 'module',\n                            'content': ast.get_docstring(node)\n                        })\n                    self.generic_visit(node)", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "'content': ast.get_docstring(node)\n                        })\n                    self.generic_visit(node)\n                    \n                def visit_ClassDef(self, node):\n                    if ast.get_docstring(node):\n                        doc_info['docstrings'].append({\n                            'type': 'class',\n                            'name': node.name,\n                            'content': ast.get_docstring(node)\n                        })\n                    self.generic_visit(node)\n                    \n                def visit_FunctionDef(self, node):\n                    if ast.get_docstring(node):\n                        doc_info['docstrings'].append({\n                            'type': 'function',\n                            'name': node.name,\n                            'content': ast.get_docstring(node)\n                        })\n                    self.generic_visit(node)\n                    \n            DocVisitor().visit(tree)", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "'content': ast.get_docstring(node)\n                        })\n                    self.generic_visit(node)\n                    \n            DocVisitor().visit(tree)\n            \n            # \u751f\u6210\u6458\u8981\n            if doc_info['docstrings']:\n                doc_info['summary'] = doc_info['docstrings'][0]['content'].split('\\n')[0]\n                \n            # \u8ba1\u7b97\u6587\u6863\u8d28\u91cf\u8bc4\u5206\n            doc_info['quality_score'] = self._calculate_doc_quality(doc_info['docstrings'])\n            \n            return doc_info\n            \n        except Exception as e:\n            self.logger.warning(f\"Failed to extract documentation from {file_path}: {str(e)}\")\n            return {\n                'summary': '',\n                'quality_score': 0.0,\n                'docstrings': [],\n                'todos': []\n            }", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "def _calculate_doc_quality(self, docstrings: List[Dict[str, str]]) -> float:\n        \"\"\"\u8ba1\u7b97\u6587\u6863\u8d28\u91cf\u8bc4\u5206\n        \n        Args:\n            docstrings: \u6587\u6863\u5b57\u7b26\u4e32\u5217\u8868\n            \n        Returns:\n            \u8d28\u91cf\u8bc4\u5206 (0-1)\n        \"\"\"\n        if not docstrings:\n            return 0.0\n            \n        total_score = 0.0\n        \n        for doc in docstrings:\n            content = doc['content']\n            \n            # 1. \u957f\u5ea6\u8bc4\u5206 (0.3)\n            length_score = min(len(content.split()) / 50, 1.0) * 0.3\n            \n            # 2. \u5b8c\u6574\u6027\u8bc4\u5206 (0.4)\n            completeness_score = 0.0\n            if 'Args:' in content: completeness_score += 0.1\n            if 'Returns:' in content: completeness_score += 0.1\n            if 'Raises:' in content: completeness_score += 0.1\n            if 'Example:' in content: completeness_score += 0.1\n            \n            # 3. \u53ef\u8bfb\u6027\u8bc4\u5206 (0.3)\n            readability_score = 0.3\n            if len(content) > 500: readability_score *= 0.8  # \u8fc7\u957f\u7684\u6587\u6863\u53ef\u80fd\u4e0d\u6613\u7406\u89e3", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "# 3. \u53ef\u8bfb\u6027\u8bc4\u5206 (0.3)\n            readability_score = 0.3\n            if len(content) > 500: readability_score *= 0.8  # \u8fc7\u957f\u7684\u6587\u6863\u53ef\u80fd\u4e0d\u6613\u7406\u89e3\n            \n            total_score += length_score + completeness_score + readability_score\n            \n        return min(1.0, total_score / len(docstrings))", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "def _extract_comments(self, file_path: str) -> List[Dict[str, Any]]:\n        \"\"\"\u63d0\u53d6\u4ee3\u7801\u6ce8\u91ca\n        \n        Args:\n            file_path: \u6587\u4ef6\u8def\u5f84\n            \n        Returns:\n            \u6ce8\u91ca\u5217\u8868\uff0c\u6bcf\u4e2a\u6ce8\u91ca\u5305\u542b\u5185\u5bb9\u548c\u4f4d\u7f6e\u4fe1\u606f\n        \"\"\"\n        comments = []\n        \n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                lines = f.readlines()\n                \n            in_block_comment = False\n            block_comment_content = []\n            \n            for i, line in enumerate(lines):\n                line = line.strip()\n                \n                # \u5904\u7406\u5757\u6ce8\u91ca\n                if line.startswith('\"\"\"') or line.startswith(\"'''\"):\n                    if not in_block_comment:\n                        in_block_comment = True\n                        block_comment_content = [line]\n                    else:\n                        in_block_comment = False\n                        block_comment_content.append(line)\n                        comments.append({", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "else:\n                        in_block_comment = False\n                        block_comment_content.append(line)\n                        comments.append({\n                            'type': 'block',\n                            'content': '\\n'.join(block_comment_content),\n                            'line': i - len(block_comment_content) + 1\n                        })\n                elif in_block_comment:\n                    block_comment_content.append(line)\n                    \n                # \u5904\u7406\u884c\u6ce8\u91ca\n                elif line.startswith('#'):\n                    comments.append({\n                        'type': 'line',\n                        'content': line[1:].strip(),\n                        'line': i + 1\n                    })\n                    \n            return comments\n            \n        except Exception as e:\n            self.logger.warning(f\"Failed to extract comments from {file_path}: {str(e)}\")\n            return []", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "def _analyze_change_history(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"\u5206\u6790\u6587\u4ef6\u53d8\u66f4\u5386\u53f2\n        \n        Args:\n            file_path: \u6587\u4ef6\u8def\u5f84\n            \n        Returns:\n            \u53d8\u66f4\u5386\u53f2\u4fe1\u606f\n        \"\"\"\n        try:\n            import os\n            import time\n            \n            stat = os.stat(file_path)\n            \n            return {\n                'last_change': stat.st_mtime,\n                'created': stat.st_ctime,\n                'size': stat.st_size,\n                'frequency': 1.0  # \u7b80\u5316\u5904\u7406\uff0c\u5b9e\u9645\u5e94\u8be5\u901a\u8fc7\u7248\u672c\u63a7\u5236\u7cfb\u7edf\u83b7\u53d6\n            }\n            \n        except Exception as e:\n            self.logger.warning(f\"Failed to analyze change history for {file_path}: {str(e)}\")\n            return {\n                'last_change': 0,\n                'created': 0,\n                'size': 0,\n                'frequency': 0\n            }", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "def _calculate_semantic_similarity(\n        self,\n        query_intent: Dict[str, Any],\n        doc_info: Dict[str, Any],\n        comments: List[Dict[str, Any]],\n        history: Dict[str, Any]\n    ) -> float:\n        \"\"\"\u8ba1\u7b97\u8bed\u4e49\u76f8\u4f3c\u5ea6\n        \n        Args:\n            query_intent: \u67e5\u8be2\u610f\u56fe\n            doc_info: \u6587\u6863\u4fe1\u606f\n            comments: \u6ce8\u91ca\u4fe1\u606f\n            history: \u53d8\u66f4\u5386\u53f2\n            \n        Returns:\n            \u76f8\u4f3c\u5ea6\u5206\u6570 (0-1)\n        \"\"\"\n        score = 0.0\n        \n        # 1. \u57fa\u4e8e\u6587\u6863\u7684\u76f8\u4f3c\u5ea6 (40%)\n        if doc_info['docstrings']:\n            # \u5c06\u6240\u6709\u6587\u6863\u5185\u5bb9\u5408\u5e76\n            doc_text = ' '.join(d['content'] for d in doc_info['docstrings'])\n            \n            # \u8ba1\u7b97\u5173\u952e\u8bcd\u5339\u914d\u5ea6\n            matches = sum(1 for kw in query_intent['keywords'] if kw.lower() in doc_text.lower())\n            doc_score = 0.4 * (matches / len(query_intent['keywords']) if query_intent['keywords'] else 0)\n            score += doc_score\n            \n        # 2. \u57fa\u4e8e\u6ce8\u91ca\u7684\u76f8\u4f3c\u5ea6 (30%)\n        if comments:", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "doc_score = 0.4 * (matches / len(query_intent['keywords']) if query_intent['keywords'] else 0)\n            score += doc_score\n            \n        # 2. \u57fa\u4e8e\u6ce8\u91ca\u7684\u76f8\u4f3c\u5ea6 (30%)\n        if comments:\n            comment_text = ' '.join(c['content'] for c in comments)\n            matches = sum(1 for kw in query_intent['keywords'] if kw.lower() in comment_text.lower())\n            comment_score = 0.3 * (matches / len(query_intent['keywords']) if query_intent['keywords'] else 0)\n            score += comment_score\n            \n        # 3. \u57fa\u4e8e\u610f\u56fe\u7684\u76f8\u4f3c\u5ea6 (20%)\n        if 'bug' in query_intent['intents'] and doc_info['todos']:\n            score += 0.2  # \u5982\u679c\u5728\u67e5\u627ebug\u4e14\u6709TODO\u6807\u8bb0\n        elif 'implementation' in query_intent['intents'] and doc_info['quality_score'] > 0.7:\n            score += 0.2  # \u5982\u679c\u5728\u67e5\u627e\u5b9e\u73b0\u4e14\u6587\u6863\u8d28\u91cf\u9ad8\n            \n        # 4. \u57fa\u4e8e\u65f6\u95f4\u7684\u76f8\u4f3c\u5ea6 (10%)\n        if history['last_change'] > 0:\n            # \u6700\u8fd1\u4fee\u6539\u7684\u6587\u4ef6\u5f97\u5206\u9ad8", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "score += 0.2  # \u5982\u679c\u5728\u67e5\u627e\u5b9e\u73b0\u4e14\u6587\u6863\u8d28\u91cf\u9ad8\n            \n        # 4. \u57fa\u4e8e\u65f6\u95f4\u7684\u76f8\u4f3c\u5ea6 (10%)\n        if history['last_change'] > 0:\n            # \u6700\u8fd1\u4fee\u6539\u7684\u6587\u4ef6\u5f97\u5206\u9ad8\n            time_factor = 1.0 / (1 + (time.time() - history['last_change']) / (30 * 24 * 3600))  # 30\u5929\u8870\u51cf\n            score += 0.1 * time_factor\n            \n        return min(1.0, score)", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "def _get_best_context(\n        self,\n        file_path: str,\n        basic_results: List[SearchResult],\n        ast_results: List[SearchResult],\n        semantic_results: List[SearchResult]\n    ) -> str:\n        \"\"\"\u83b7\u53d6\u6700\u4f73\u4e0a\u4e0b\u6587\n        \n        Args:\n            file_path: \u6587\u4ef6\u8def\u5f84\n            basic_results: \u57fa\u7840\u641c\u7d22\u7ed3\u679c\n            ast_results: AST\u5206\u6790\u7ed3\u679c\n            semantic_results: \u8bed\u4e49\u5206\u6790\u7ed3\u679c\n            \n        Returns:\n            \u6700\u4f73\u4e0a\u4e0b\u6587\u63cf\u8ff0\n        \"\"\"\n        contexts = []\n        \n        # 1. \u6536\u96c6\u6240\u6709\u7ed3\u679c\u4e2d\u7684\u4e0a\u4e0b\u6587\n        for result in basic_results + ast_results + semantic_results:\n            if result.file_path == file_path and result.context:\n                contexts.append(result.context)\n                \n        if not contexts:\n            return \"\"\n            \n        # 2. \u9009\u62e9\u6700\u957f\u7684\u4e0a\u4e0b\u6587\u4f5c\u4e3a\u57fa\u7840\n        base_context = max(contexts, key=len)\n        \n        # 3. \u5982\u679c\u6709\u5176\u4ed6\u4e0a\u4e0b\u6587\u5305\u542b\u4e0d\u540c\u7684\u4fe1\u606f\uff0c\u8fdb\u884c\u5408\u5e76\n        final_context = base_context\n        for ctx in contexts:", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "# 2. \u9009\u62e9\u6700\u957f\u7684\u4e0a\u4e0b\u6587\u4f5c\u4e3a\u57fa\u7840\n        base_context = max(contexts, key=len)\n        \n        # 3. \u5982\u679c\u6709\u5176\u4ed6\u4e0a\u4e0b\u6587\u5305\u542b\u4e0d\u540c\u7684\u4fe1\u606f\uff0c\u8fdb\u884c\u5408\u5e76\n        final_context = base_context\n        for ctx in contexts:\n            if ctx != base_context and ctx not in base_context:\n                final_context += f\"\\n{ctx}\"\n                \n        return final_context", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "def add_feedback(self, feedback_data: List[Dict[str, Any]]):\n        \"\"\"\u6dfb\u52a0\u641c\u7d22\u53cd\u9988\uff0c\u7528\u4e8e\u4f18\u5316\u6a21\u578b\u6743\u91cd\"\"\"\n        self.model_fusion.update_weights(feedback_data)\n        \n        if self.cache_dir:\n            self.model_fusion.save_weights(\n                os.path.join(self.cache_dir, 'model_weights.json')\n            )\n            \n    def save_state(self):\n        \"\"\"\u4fdd\u5b58\u641c\u7d22\u5f15\u64ce\u72b6\u6001\"\"\"\n        if not self.cache_dir:\n            return\n            \n        # \u4fdd\u5b58\u7d22\u5f15\u72b6\u6001\n        self.incremental_indexer.save_state(\n            os.path.join(self.cache_dir, 'index')\n        )\n        \n        # \u4fdd\u5b58\u6a21\u578b\u6743\u91cd\n        self.model_fusion.save_weights(\n            os.path.join(self.cache_dir, 'model_weights.json')\n        )\n        \n    def load_state(self):\n        \"\"\"\u52a0\u8f7d\u641c\u7d22\u5f15\u64ce\u72b6\u6001\"\"\"\n        if not self.cache_dir:\n            return\n            \n        # \u52a0\u8f7d\u7d22\u5f15\u72b6\u6001\n        self.incremental_indexer.load_state(\n            os.path.join(self.cache_dir, 'index')\n        )\n        \n        # \u52a0\u8f7d\u6a21\u578b\u6743\u91cd", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "return\n            \n        # \u52a0\u8f7d\u7d22\u5f15\u72b6\u6001\n        self.incremental_indexer.load_state(\n            os.path.join(self.cache_dir, 'index')\n        )\n        \n        # \u52a0\u8f7d\u6a21\u578b\u6743\u91cd\n        weights_path = os.path.join(self.cache_dir, 'model_weights.json')\n        if os.path.exists(weights_path):\n            self.model_fusion.load_weights(weights_path)\n            \n    def __del__(self):\n        \"\"\"\u6e05\u7406\u8d44\u6e90\"\"\"\n        # \u505c\u6b62\u6587\u4ef6\u76d1\u63a7\n        self.incremental_indexer.stop_watching()\n        \n        # \u4fdd\u5b58\u72b6\u6001\n        if self.cache_dir:\n            self.save_state()", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "class CodeStructureVisitor(ast.NodeVisitor):\n    def __init__(self):\n        self.structure_info = {}\n        self.complexity = 0\n\n    def visit(self, node):\n        # TODO: \u5b9e\u73b0AST\u8282\u70b9\u8bbf\u95ee\n        pass\n\n    def get_context(self):\n        # TODO: \u5b9e\u73b0\u4e0a\u4e0b\u6587\u83b7\u53d6\n        pass", "file_path": "code_search\\enhanced_search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import List, Dict, Any, Optional, Set\nimport os\nfrom pathlib import Path\n", "symbols": ["base_context", "_extract_documentation", "_calculate_graph_depth", "_build_control_flow_graph", "key", "class", "search", "_calculate_doc_quality", "while_id", "prev_node", "combined_results", "query_intent", "query_terms", "get_context", "prev_scope", "length_score", "visitor", "_analyze_change_history", "CodeStructureVisitor", "add_feedback", "detector", "_get_best_context", "prev_class", "final_results", "semantic_results", "cfg_complexity", "file_path", "dfg", "model_fusion", "visit_FunctionDef", "params", "completeness_score", "weights_path", "_semantic_enhanced_search", "collector", "if_id", "_build_data_flow_graph", "visit_Name", "PatternDetector", "_combine_search_results", "visit_Assign", "stat", "get_node_id", "_basic_search", "exist_ok", "else_id", "comment_text", "DFGVisitor", "filtered_results", "int", "detect_patterns", "ast_results", "targets", "class_info", "self", "visit_If", "SearchConfig", "load_state", "__init__", "dfg_score", "config", "tree", "comment_score", "classes", "current_class", "visit_Module", "combined_scores", "layer", "encoding", "_extract_comments", "current_scope", "DocVisitor", "str", "intents", "basic_results", "float", "save_state", "reverse", "__del__", "pattern_score", "def", "generic_visit", "score", "models_config", "top_k", "comments", "lines", "doc_score", "cache_dir", "cfg_depth", "CFGVisitor", "max_depth", "intent_keywords", "readability_score", "in_block_comment", "EnhancedSearchEngine", "current_node", "_detect_design_patterns", "node_id", "visit_ClassDef", "results", "history", "matches", "patterns", "bool", "deps", "visited", "complexity", "all_intent_keywords", "dfg_complexity", "context", "line", "metadata", "final_context", "layered_index", "total_score", "depth", "_calculate_semantic_similarity", "layer_results", "block_comment_content", "time_factor", "_calculate_structure_similarity", "weights", "keywords", "content", "_ast_enhanced_search", "node_counter", "doc_info", "_extract_query_intent", "VarCollector", "visit_While", "query", "SearchResult", "result_metadata", "pattern_terms", "cfg_score", "contexts", "import", "incremental_indexer", "cfg", "structure_info", "doc_text", "visit", "logger"]}, {"content": "from search_engine import CodeSearchEngine\nfrom code_analyzer import CodeAnalyzer, BusinessLogicSearcher\nimport logging\nimport json", "file_path": "code_search\\example.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from search_engine import CodeSearchEngine\nfrom code_analyzer import CodeAnalyzer, BusinessLogicSearcher\nimport logging\n", "symbols": ["main", "logger", "features", "repos", "level", "results", "__name__", "search_engine", "logic_searcher", "code_analyzer"]}, {"content": "def main():\n    # Set up logging\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n\n    # Initialize components\n    logger.info(\"Initializing search engine and analyzer...\")\n    search_engine = CodeSearchEngine()\n    code_analyzer = CodeAnalyzer()\n    logic_searcher = BusinessLogicSearcher(search_engine, code_analyzer)\n\n    # Add repositories\n    repos = [\n        \"d:/workspaces/python_projects/ai_demo/code_analyze\",\n        # Add more repository paths here\n    ]\n\n    for repo_path in repos:\n        logger.info(f\"Indexing repository: {repo_path}\")\n        search_engine.add_repository(repo_path)\n\n    # Save the index\n    logger.info(\"Saving search index...\")\n    search_engine.save_index(\"./search_index\")\n\n    # Example feature searches\n    features = [\n        \"code search functionality\",\n        \"analyze python code structure\",\n        \"extract business logic from code\"\n    ]", "file_path": "code_search\\example.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from search_engine import CodeSearchEngine\nfrom code_analyzer import CodeAnalyzer, BusinessLogicSearcher\nimport logging\n", "symbols": ["main", "logger", "features", "repos", "level", "results", "__name__", "search_engine", "logic_searcher", "code_analyzer"]}, {"content": "for feature in features:\n        logger.info(f\"\\nSearching for feature: {feature}\")\n        results = logic_searcher.search_business_logic(feature)\n        \n        if not results:\n            logger.info(\"No relevant business logic found.\")\n            continue\n            \n        for result in results:\n            logger.info(f\"\\nFile: {result['file']} (score: {result['score']:.3f})\")\n            \n            for element in result['elements']:\n                if element['type'] == 'module':\n                    logger.info(f\"Module docstring: {element['docstring']}\")\n                    \n                elif element['type'] == 'class':\n                    logger.info(f\"\\nClass: {element['name']}\")\n                    if element['docstring']:\n                        logger.info(f\"Description: {element['docstring']}\")\n                    if element['base_classes']:\n                        logger.info(f\"Inherits from: {', '.join(element['base_classes'])}\")", "file_path": "code_search\\example.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from search_engine import CodeSearchEngine\nfrom code_analyzer import CodeAnalyzer, BusinessLogicSearcher\nimport logging\n", "symbols": ["main", "logger", "features", "repos", "level", "results", "__name__", "search_engine", "logic_searcher", "code_analyzer"]}, {"content": "if element['base_classes']:\n                        logger.info(f\"Inherits from: {', '.join(element['base_classes'])}\")\n                        \n                    for method in element['methods']:\n                        logger.info(f\"\\n  Method: {method['name']}\")\n                        if method['docstring']:\n                            logger.info(f\"  Description: {method['docstring']}\")\n                        logger.info(f\"  Parameters: {', '.join(method['params'])}\")\n                        logger.info(f\"  Calls: {', '.join(method['calls'])}\")\n                        logger.info(f\"  Complexity: {method['complexity']}\")\n                        logger.info(f\"  Lines: {method['lines']}\")\n                        \n                elif element['type'] == 'function':\n                    logger.info(f\"\\nFunction: {element['name']}\")\n                    if element['docstring']:\n                        logger.info(f\"Description: {element['docstring']}\")", "file_path": "code_search\\example.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from search_engine import CodeSearchEngine\nfrom code_analyzer import CodeAnalyzer, BusinessLogicSearcher\nimport logging\n", "symbols": ["main", "logger", "features", "repos", "level", "results", "__name__", "search_engine", "logic_searcher", "code_analyzer"]}, {"content": "logger.info(f\"\\nFunction: {element['name']}\")\n                    if element['docstring']:\n                        logger.info(f\"Description: {element['docstring']}\")\n                    logger.info(f\"Parameters: {', '.join(element['params'])}\")\n                    logger.info(f\"Calls: {', '.join(element['calls'])}\")\n                    logger.info(f\"Complexity: {element['complexity']}\")\n                    logger.info(f\"Lines: {element['lines']}\")", "file_path": "code_search\\example.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from search_engine import CodeSearchEngine\nfrom code_analyzer import CodeAnalyzer, BusinessLogicSearcher\nimport logging\n", "symbols": ["main", "logger", "features", "repos", "level", "results", "__name__", "search_engine", "logic_searcher", "code_analyzer"]}, {"content": "if __name__ == \"__main__\":\n    main()", "file_path": "code_search\\example.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from search_engine import CodeSearchEngine\nfrom code_analyzer import CodeAnalyzer, BusinessLogicSearcher\nimport logging\n", "symbols": ["main", "logger", "features", "repos", "level", "results", "__name__", "search_engine", "logic_searcher", "code_analyzer"]}, {"content": "import os\nimport hashlib\nfrom typing import Dict, List, Any, Optional\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nfrom .layered_index import LayeredIndex\n\nclass FileChangeHandler(FileSystemEventHandler):\n    def __init__(self, indexer):\n        self.indexer = indexer\n        \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.indexer.handle_file_change(event.src_path, 'modified')\n            \n    def on_created(self, event):\n        if not event.is_directory:\n            self.indexer.handle_file_change(event.src_path, 'created')\n            \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.indexer.handle_file_change(event.src_path, 'deleted')", "file_path": "code_search\\incremental_indexer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nimport hashlib\nfrom typing import Dict, List, Any, Optional\n", "symbols": ["on_deleted", "event_handler", "file_hashes", "save_state", "observer", "metadata", "doc_id", "indexer", "IncrementalIndexer", "handle_file_change", "index", "watch_dirs", "analyze_dependencies", "exist_ok", "content", "business_logic", "on_created", "FileChangeHandler", "event_type", "compute_file_hash", "on_modified", "stop_watching", "old_hash", "build_initial_index", "load_state", "__init__", "tree", "dependencies", "new_hash", "file_path", "analyze_business_logic", "start_watching", "recursive", "encoding"]}, {"content": "class IncrementalIndexer:\n    def __init__(self, index: LayeredIndex, watch_dirs: List[str]):\n        self.index = index\n        self.watch_dirs = watch_dirs\n        self.file_hashes = {}  # \u5b58\u50a8\u6587\u4ef6\u54c8\u5e0c\u503c\n        self.observer = Observer()\n        self.event_handler = FileChangeHandler(self)\n        \n    def start_watching(self):\n        \"\"\"\u5f00\u59cb\u76d1\u63a7\u6587\u4ef6\u53d8\u5316\"\"\"\n        for dir_path in self.watch_dirs:\n            self.observer.schedule(self.event_handler, dir_path, recursive=True)\n        self.observer.start()\n        \n    def stop_watching(self):\n        \"\"\"\u505c\u6b62\u76d1\u63a7\u6587\u4ef6\u53d8\u5316\"\"\"\n        self.observer.stop()\n        self.observer.join()\n        \n    def compute_file_hash(self, file_path: str) -> str:\n        \"\"\"\u8ba1\u7b97\u6587\u4ef6\u54c8\u5e0c\u503c\"\"\"\n        with open(file_path, 'rb') as f:\n            return hashlib.md5(f.read()).hexdigest()\n            \n    def handle_file_change(self, file_path: str, event_type: str):\n        \"\"\"\u5904\u7406\u6587\u4ef6\u53d8\u66f4\"\"\"\n        if event_type == 'deleted':\n            if file_path in self.file_hashes:", "file_path": "code_search\\incremental_indexer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nimport hashlib\nfrom typing import Dict, List, Any, Optional\n", "symbols": ["on_deleted", "event_handler", "file_hashes", "save_state", "observer", "metadata", "doc_id", "indexer", "IncrementalIndexer", "handle_file_change", "index", "watch_dirs", "analyze_dependencies", "exist_ok", "content", "business_logic", "on_created", "FileChangeHandler", "event_type", "compute_file_hash", "on_modified", "stop_watching", "old_hash", "build_initial_index", "load_state", "__init__", "tree", "dependencies", "new_hash", "file_path", "analyze_business_logic", "start_watching", "recursive", "encoding"]}, {"content": "def handle_file_change(self, file_path: str, event_type: str):\n        \"\"\"\u5904\u7406\u6587\u4ef6\u53d8\u66f4\"\"\"\n        if event_type == 'deleted':\n            if file_path in self.file_hashes:\n                self.index.remove_document(file_path)\n                del self.file_hashes[file_path]\n        else:\n            # \u5bf9\u4e8e\u65b0\u5efa\u6216\u4fee\u6539\u7684\u6587\u4ef6\n            new_hash = self.compute_file_hash(file_path)\n            old_hash = self.file_hashes.get(file_path)\n            \n            if new_hash != old_hash:\n                # \u6587\u4ef6\u5185\u5bb9\u53d1\u751f\u53d8\u5316\uff0c\u66f4\u65b0\u7d22\u5f15\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                    \n                # \u83b7\u53d6\u6587\u4ef6\u5143\u6570\u636e\n                metadata = {\n                    'path': file_path,\n                    'size': os.path.getsize(file_path),\n                    'last_modified': os.path.getmtime(file_path)\n                }\n                \n                # \u5206\u6790\u6587\u4ef6\u4f9d\u8d56\u5173\u7cfb\n                dependencies = self.analyze_dependencies(file_path)", "file_path": "code_search\\incremental_indexer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nimport hashlib\nfrom typing import Dict, List, Any, Optional\n", "symbols": ["on_deleted", "event_handler", "file_hashes", "save_state", "observer", "metadata", "doc_id", "indexer", "IncrementalIndexer", "handle_file_change", "index", "watch_dirs", "analyze_dependencies", "exist_ok", "content", "business_logic", "on_created", "FileChangeHandler", "event_type", "compute_file_hash", "on_modified", "stop_watching", "old_hash", "build_initial_index", "load_state", "__init__", "tree", "dependencies", "new_hash", "file_path", "analyze_business_logic", "start_watching", "recursive", "encoding"]}, {"content": "'last_modified': os.path.getmtime(file_path)\n                }\n                \n                # \u5206\u6790\u6587\u4ef6\u4f9d\u8d56\u5173\u7cfb\n                dependencies = self.analyze_dependencies(file_path)\n                \n                # \u5206\u6790\u4e1a\u52a1\u903b\u8f91\n                business_logic = self.analyze_business_logic(file_path, content)\n                \n                # \u66f4\u65b0\u7d22\u5f15\n                self.index.update_document(\n                    doc_id=file_path,\n                    content=content,\n                    metadata=metadata,\n                    dependencies=dependencies,\n                    business_logic=business_logic\n                )\n                \n                # \u66f4\u65b0\u6587\u4ef6\u54c8\u5e0c\u503c\n                self.file_hashes[file_path] = new_hash\n                \n    def analyze_dependencies(self, file_path: str) -> List[str]:\n        \"\"\"\u5206\u6790\u6587\u4ef6\u4f9d\u8d56\u5173\u7cfb\"\"\"\n        dependencies = []\n        with open(file_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n            \n        # \u5206\u6790Python import\u8bed\u53e5", "file_path": "code_search\\incremental_indexer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nimport hashlib\nfrom typing import Dict, List, Any, Optional\n", "symbols": ["on_deleted", "event_handler", "file_hashes", "save_state", "observer", "metadata", "doc_id", "indexer", "IncrementalIndexer", "handle_file_change", "index", "watch_dirs", "analyze_dependencies", "exist_ok", "content", "business_logic", "on_created", "FileChangeHandler", "event_type", "compute_file_hash", "on_modified", "stop_watching", "old_hash", "build_initial_index", "load_state", "__init__", "tree", "dependencies", "new_hash", "file_path", "analyze_business_logic", "start_watching", "recursive", "encoding"]}, {"content": "\"\"\"\u5206\u6790\u6587\u4ef6\u4f9d\u8d56\u5173\u7cfb\"\"\"\n        dependencies = []\n        with open(file_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n            \n        # \u5206\u6790Python import\u8bed\u53e5\n        if file_path.endswith('.py'):\n            import ast\n            try:\n                tree = ast.parse(content)\n                for node in ast.walk(tree):\n                    if isinstance(node, ast.Import):\n                        for name in node.names:\n                            dependencies.append(name.name)\n                    elif isinstance(node, ast.ImportFrom):\n                        if node.module:\n                            dependencies.append(node.module)\n            except:\n                pass\n                \n        return dependencies\n        \n    def analyze_business_logic(self, file_path: str, content: str) -> Dict[str, Any]:\n        \"\"\"\u5206\u6790\u6587\u4ef6\u4e1a\u52a1\u903b\u8f91\"\"\"\n        business_logic = {\n            'functions': [],\n            'classes': [],\n            'variables': []\n        }", "file_path": "code_search\\incremental_indexer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nimport hashlib\nfrom typing import Dict, List, Any, Optional\n", "symbols": ["on_deleted", "event_handler", "file_hashes", "save_state", "observer", "metadata", "doc_id", "indexer", "IncrementalIndexer", "handle_file_change", "index", "watch_dirs", "analyze_dependencies", "exist_ok", "content", "business_logic", "on_created", "FileChangeHandler", "event_type", "compute_file_hash", "on_modified", "stop_watching", "old_hash", "build_initial_index", "load_state", "__init__", "tree", "dependencies", "new_hash", "file_path", "analyze_business_logic", "start_watching", "recursive", "encoding"]}, {"content": "\"\"\"\u5206\u6790\u6587\u4ef6\u4e1a\u52a1\u903b\u8f91\"\"\"\n        business_logic = {\n            'functions': [],\n            'classes': [],\n            'variables': []\n        }\n        \n        if file_path.endswith('.py'):\n            import ast\n            try:\n                tree = ast.parse(content)\n                \n                # \u63d0\u53d6\u51fd\u6570\u4fe1\u606f\n                for node in ast.walk(tree):\n                    if isinstance(node, ast.FunctionDef):\n                        business_logic['functions'].append({\n                            'name': node.name,\n                            'args': [arg.arg for arg in node.args.args],\n                            'docstring': ast.get_docstring(node)\n                        })\n                    elif isinstance(node, ast.ClassDef):\n                        business_logic['classes'].append({\n                            'name': node.name,\n                            'methods': [m.name for m in node.body if isinstance(m, ast.FunctionDef)],", "file_path": "code_search\\incremental_indexer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nimport hashlib\nfrom typing import Dict, List, Any, Optional\n", "symbols": ["on_deleted", "event_handler", "file_hashes", "save_state", "observer", "metadata", "doc_id", "indexer", "IncrementalIndexer", "handle_file_change", "index", "watch_dirs", "analyze_dependencies", "exist_ok", "content", "business_logic", "on_created", "FileChangeHandler", "event_type", "compute_file_hash", "on_modified", "stop_watching", "old_hash", "build_initial_index", "load_state", "__init__", "tree", "dependencies", "new_hash", "file_path", "analyze_business_logic", "start_watching", "recursive", "encoding"]}, {"content": "'name': node.name,\n                            'methods': [m.name for m in node.body if isinstance(m, ast.FunctionDef)],\n                            'docstring': ast.get_docstring(node)\n                        })\n                    elif isinstance(node, ast.Assign):\n                        for target in node.targets:\n                            if isinstance(target, ast.Name):\n                                business_logic['variables'].append(target.id)\n            except:\n                pass\n                \n        return business_logic\n        \n    def build_initial_index(self):\n        \"\"\"\u6784\u5efa\u521d\u59cb\u7d22\u5f15\"\"\"\n        for dir_path in self.watch_dirs:\n            for root, _, files in os.walk(dir_path):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    self.handle_file_change(file_path, 'created')\n                    \n    def save_state(self, save_dir: str):\n        \"\"\"\u4fdd\u5b58\u7d22\u5f15\u72b6\u6001\"\"\"", "file_path": "code_search\\incremental_indexer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nimport hashlib\nfrom typing import Dict, List, Any, Optional\n", "symbols": ["on_deleted", "event_handler", "file_hashes", "save_state", "observer", "metadata", "doc_id", "indexer", "IncrementalIndexer", "handle_file_change", "index", "watch_dirs", "analyze_dependencies", "exist_ok", "content", "business_logic", "on_created", "FileChangeHandler", "event_type", "compute_file_hash", "on_modified", "stop_watching", "old_hash", "build_initial_index", "load_state", "__init__", "tree", "dependencies", "new_hash", "file_path", "analyze_business_logic", "start_watching", "recursive", "encoding"]}, {"content": "self.handle_file_change(file_path, 'created')\n                    \n    def save_state(self, save_dir: str):\n        \"\"\"\u4fdd\u5b58\u7d22\u5f15\u72b6\u6001\"\"\"\n        os.makedirs(save_dir, exist_ok=True)\n        \n        # \u4fdd\u5b58\u6587\u4ef6\u54c8\u5e0c\u503c\n        import json\n        with open(os.path.join(save_dir, 'file_hashes.json'), 'w') as f:\n            json.dump(self.file_hashes, f)\n            \n        # \u4fdd\u5b58\u7d22\u5f15\n        self.index.save_index(os.path.join(save_dir, 'index'))\n        \n    def load_state(self, save_dir: str):\n        \"\"\"\u52a0\u8f7d\u7d22\u5f15\u72b6\u6001\"\"\"\n        # \u52a0\u8f7d\u6587\u4ef6\u54c8\u5e0c\u503c\n        import json\n        try:\n            with open(os.path.join(save_dir, 'file_hashes.json'), 'r') as f:\n                self.file_hashes = json.load(f)\n        except FileNotFoundError:\n            self.file_hashes = {}\n            \n        # \u52a0\u8f7d\u7d22\u5f15\n        self.index.load_index(os.path.join(save_dir, 'index'))", "file_path": "code_search\\incremental_indexer.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nimport hashlib\nfrom typing import Dict, List, Any, Optional\n", "symbols": ["on_deleted", "event_handler", "file_hashes", "save_state", "observer", "metadata", "doc_id", "indexer", "IncrementalIndexer", "handle_file_change", "index", "watch_dirs", "analyze_dependencies", "exist_ok", "content", "business_logic", "on_created", "FileChangeHandler", "event_type", "compute_file_hash", "on_modified", "stop_watching", "old_hash", "build_initial_index", "load_state", "__init__", "tree", "dependencies", "new_hash", "file_path", "analyze_business_logic", "start_watching", "recursive", "encoding"]}, {"content": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sentence_transformers import SentenceTransformer\nimport networkx as nx\nimport json\nimport os", "file_path": "code_search\\layered_index.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n", "symbols": ["allow_pickle", "str", "dependency_graph", "load_index", "key", "sorted_docs", "semantic_scores", "metadata", "reverse", "semantic_vector", "search", "current_nodes", "semantic_vectors", "score", "get_dependencies", "text_vectors", "exist_ok", "add_document", "semantic_index", "text_vector", "next_nodes", "text_scores", "result", "int", "semantic_model", "save_index", "business_index", "__init__", "successors", "incoming", "results", "predecessors", "remove_document", "LayeredIndex", "tfidf_vectorizer", "text_index", "layer", "combined_scores", "outgoing", "query_vector", "update_document"]}, {"content": "class LayeredIndex:\n    def __init__(self, vector_model: str = 'all-MiniLM-L6-v2'):\n        # \u8bed\u4e49\u5c42\u7d22\u5f15\n        self.semantic_model = SentenceTransformer(vector_model)\n        self.semantic_index = {}\n        \n        # \u6587\u672c\u5c42\u7d22\u5f15\n        self.tfidf_vectorizer = TfidfVectorizer()\n        self.text_index = {}\n        \n        # \u4f9d\u8d56\u5c42\u7d22\u5f15\n        self.dependency_graph = nx.DiGraph()\n        \n        # \u4e1a\u52a1\u903b\u8f91\u5c42\u7d22\u5f15\n        self.business_index = {}\n        \n        # \u5143\u6570\u636e\u7d22\u5f15\n        self.metadata = {}\n        \n    def add_document(self, doc_id: str, content: str, metadata: Dict[str, Any], \n                    dependencies: List[str] = None, business_logic: Dict[str, Any] = None):\n        \"\"\"\u6dfb\u52a0\u6587\u6863\u5230\u591a\u5c42\u7d22\u5f15\"\"\"\n        # 1. \u66f4\u65b0\u8bed\u4e49\u5c42\u7d22\u5f15\n        semantic_vector = self.semantic_model.encode(content)\n        self.semantic_index[doc_id] = semantic_vector\n        \n        # 2. \u66f4\u65b0\u6587\u672c\u5c42\u7d22\u5f15\n        if not self.text_index:\n            self.tfidf_vectorizer.fit([content])", "file_path": "code_search\\layered_index.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n", "symbols": ["allow_pickle", "str", "dependency_graph", "load_index", "key", "sorted_docs", "semantic_scores", "metadata", "reverse", "semantic_vector", "search", "current_nodes", "semantic_vectors", "score", "get_dependencies", "text_vectors", "exist_ok", "add_document", "semantic_index", "text_vector", "next_nodes", "text_scores", "result", "int", "semantic_model", "save_index", "business_index", "__init__", "successors", "incoming", "results", "predecessors", "remove_document", "LayeredIndex", "tfidf_vectorizer", "text_index", "layer", "combined_scores", "outgoing", "query_vector", "update_document"]}, {"content": "self.semantic_index[doc_id] = semantic_vector\n        \n        # 2. \u66f4\u65b0\u6587\u672c\u5c42\u7d22\u5f15\n        if not self.text_index:\n            self.tfidf_vectorizer.fit([content])\n        text_vector = self.tfidf_vectorizer.transform([content]).toarray()[0]\n        self.text_index[doc_id] = text_vector\n        \n        # 3. \u66f4\u65b0\u4f9d\u8d56\u5c42\u7d22\u5f15\n        if dependencies:\n            for dep in dependencies:\n                self.dependency_graph.add_edge(doc_id, dep)\n        \n        # 4. \u66f4\u65b0\u4e1a\u52a1\u903b\u8f91\u5c42\u7d22\u5f15\n        if business_logic:\n            self.business_index[doc_id] = business_logic\n            \n        # 5. \u66f4\u65b0\u5143\u6570\u636e\n        self.metadata[doc_id] = metadata\n        \n    def remove_document(self, doc_id: str):\n        \"\"\"\u4ece\u7d22\u5f15\u4e2d\u79fb\u9664\u6587\u6863\"\"\"\n        if doc_id in self.semantic_index:\n            del self.semantic_index[doc_id]\n        if doc_id in self.text_index:\n            del self.text_index[doc_id]\n        if doc_id in self.business_index:\n            del self.business_index[doc_id]\n        if doc_id in self.metadata:", "file_path": "code_search\\layered_index.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n", "symbols": ["allow_pickle", "str", "dependency_graph", "load_index", "key", "sorted_docs", "semantic_scores", "metadata", "reverse", "semantic_vector", "search", "current_nodes", "semantic_vectors", "score", "get_dependencies", "text_vectors", "exist_ok", "add_document", "semantic_index", "text_vector", "next_nodes", "text_scores", "result", "int", "semantic_model", "save_index", "business_index", "__init__", "successors", "incoming", "results", "predecessors", "remove_document", "LayeredIndex", "tfidf_vectorizer", "text_index", "layer", "combined_scores", "outgoing", "query_vector", "update_document"]}, {"content": "if doc_id in self.text_index:\n            del self.text_index[doc_id]\n        if doc_id in self.business_index:\n            del self.business_index[doc_id]\n        if doc_id in self.metadata:\n            del self.metadata[doc_id]\n        if self.dependency_graph.has_node(doc_id):\n            self.dependency_graph.remove_node(doc_id)\n            \n    def update_document(self, doc_id: str, content: str, metadata: Dict[str, Any],\n                       dependencies: List[str] = None, business_logic: Dict[str, Any] = None):\n        \"\"\"\u66f4\u65b0\u6587\u6863\u7d22\u5f15\"\"\"\n        self.remove_document(doc_id)\n        self.add_document(doc_id, content, metadata, dependencies, business_logic)\n        \n    def search(self, query: str, layer: str = 'all', top_k: int = 5) -> List[Dict[str, Any]]:\n        \"\"\"\u591a\u5c42\u7d22\u5f15\u641c\u7d22\"\"\"\n        results = []\n        \n        if layer in ['all', 'semantic']:\n            # \u8bed\u4e49\u5c42\u641c\u7d22\n            query_vector = self.semantic_model.encode(query)\n            semantic_scores = {}", "file_path": "code_search\\layered_index.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n", "symbols": ["allow_pickle", "str", "dependency_graph", "load_index", "key", "sorted_docs", "semantic_scores", "metadata", "reverse", "semantic_vector", "search", "current_nodes", "semantic_vectors", "score", "get_dependencies", "text_vectors", "exist_ok", "add_document", "semantic_index", "text_vector", "next_nodes", "text_scores", "result", "int", "semantic_model", "save_index", "business_index", "__init__", "successors", "incoming", "results", "predecessors", "remove_document", "LayeredIndex", "tfidf_vectorizer", "text_index", "layer", "combined_scores", "outgoing", "query_vector", "update_document"]}, {"content": "results = []\n        \n        if layer in ['all', 'semantic']:\n            # \u8bed\u4e49\u5c42\u641c\u7d22\n            query_vector = self.semantic_model.encode(query)\n            semantic_scores = {}\n            for doc_id, vector in self.semantic_index.items():\n                score = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n                semantic_scores[doc_id] = score\n                \n        if layer in ['all', 'text']:\n            # \u6587\u672c\u5c42\u641c\u7d22\n            query_vector = self.tfidf_vectorizer.transform([query]).toarray()[0]\n            text_scores = {}\n            for doc_id, vector in self.text_index.items():\n                score = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n                text_scores[doc_id] = score\n                \n        if layer == 'all':\n            # \u878d\u5408\u641c\u7d22\u7ed3\u679c\n            combined_scores = {}\n            for doc_id in self.semantic_index.keys():", "file_path": "code_search\\layered_index.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n", "symbols": ["allow_pickle", "str", "dependency_graph", "load_index", "key", "sorted_docs", "semantic_scores", "metadata", "reverse", "semantic_vector", "search", "current_nodes", "semantic_vectors", "score", "get_dependencies", "text_vectors", "exist_ok", "add_document", "semantic_index", "text_vector", "next_nodes", "text_scores", "result", "int", "semantic_model", "save_index", "business_index", "__init__", "successors", "incoming", "results", "predecessors", "remove_document", "LayeredIndex", "tfidf_vectorizer", "text_index", "layer", "combined_scores", "outgoing", "query_vector", "update_document"]}, {"content": "text_scores[doc_id] = score\n                \n        if layer == 'all':\n            # \u878d\u5408\u641c\u7d22\u7ed3\u679c\n            combined_scores = {}\n            for doc_id in self.semantic_index.keys():\n                combined_scores[doc_id] = (\n                    0.6 * semantic_scores.get(doc_id, 0) +  # \u8bed\u4e49\u5c42\u6743\u91cd\n                    0.4 * text_scores.get(doc_id, 0)        # \u6587\u672c\u5c42\u6743\u91cd\n                )\n        elif layer == 'semantic':\n            combined_scores = semantic_scores\n        elif layer == 'text':\n            combined_scores = text_scores\n            \n        # \u6392\u5e8f\u5e76\u8fd4\u56de\u7ed3\u679c\n        sorted_docs = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n        \n        for doc_id, score in sorted_docs:\n            result = {\n                'doc_id': doc_id,\n                'score': score,\n                'metadata': self.metadata.get(doc_id, {}),\n            }\n            \n            # \u6dfb\u52a0\u4f9d\u8d56\u4fe1\u606f\n            if self.dependency_graph.has_node(doc_id):", "file_path": "code_search\\layered_index.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n", "symbols": ["allow_pickle", "str", "dependency_graph", "load_index", "key", "sorted_docs", "semantic_scores", "metadata", "reverse", "semantic_vector", "search", "current_nodes", "semantic_vectors", "score", "get_dependencies", "text_vectors", "exist_ok", "add_document", "semantic_index", "text_vector", "next_nodes", "text_scores", "result", "int", "semantic_model", "save_index", "business_index", "__init__", "successors", "incoming", "results", "predecessors", "remove_document", "LayeredIndex", "tfidf_vectorizer", "text_index", "layer", "combined_scores", "outgoing", "query_vector", "update_document"]}, {"content": "'score': score,\n                'metadata': self.metadata.get(doc_id, {}),\n            }\n            \n            # \u6dfb\u52a0\u4f9d\u8d56\u4fe1\u606f\n            if self.dependency_graph.has_node(doc_id):\n                result['dependencies'] = {\n                    'incoming': list(self.dependency_graph.predecessors(doc_id)),\n                    'outgoing': list(self.dependency_graph.successors(doc_id))\n                }\n                \n            # \u6dfb\u52a0\u4e1a\u52a1\u903b\u8f91\u4fe1\u606f\n            if doc_id in self.business_index:\n                result['business_logic'] = self.business_index[doc_id]\n                \n            results.append(result)\n            \n        return results\n    \n    def get_dependencies(self, doc_id: str, depth: int = 1) -> Dict[str, List[str]]:\n        \"\"\"\u83b7\u53d6\u6587\u6863\u7684\u4f9d\u8d56\u5173\u7cfb\"\"\"\n        if not self.dependency_graph.has_node(doc_id):\n            return {'incoming': [], 'outgoing': []}\n            \n        incoming = set()\n        outgoing = set()\n        \n        # \u83b7\u53d6\u5165\u8fb9\u4f9d\u8d56", "file_path": "code_search\\layered_index.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n", "symbols": ["allow_pickle", "str", "dependency_graph", "load_index", "key", "sorted_docs", "semantic_scores", "metadata", "reverse", "semantic_vector", "search", "current_nodes", "semantic_vectors", "score", "get_dependencies", "text_vectors", "exist_ok", "add_document", "semantic_index", "text_vector", "next_nodes", "text_scores", "result", "int", "semantic_model", "save_index", "business_index", "__init__", "successors", "incoming", "results", "predecessors", "remove_document", "LayeredIndex", "tfidf_vectorizer", "text_index", "layer", "combined_scores", "outgoing", "query_vector", "update_document"]}, {"content": "if not self.dependency_graph.has_node(doc_id):\n            return {'incoming': [], 'outgoing': []}\n            \n        incoming = set()\n        outgoing = set()\n        \n        # \u83b7\u53d6\u5165\u8fb9\u4f9d\u8d56\n        current_nodes = {doc_id}\n        for _ in range(depth):\n            next_nodes = set()\n            for node in current_nodes:\n                predecessors = set(self.dependency_graph.predecessors(node))\n                incoming.update(predecessors)\n                next_nodes.update(predecessors)\n            current_nodes = next_nodes\n            \n        # \u83b7\u53d6\u51fa\u8fb9\u4f9d\u8d56\n        current_nodes = {doc_id}\n        for _ in range(depth):\n            next_nodes = set()\n            for node in current_nodes:\n                successors = set(self.dependency_graph.successors(node))\n                outgoing.update(successors)\n                next_nodes.update(successors)\n            current_nodes = next_nodes\n            \n        return {\n            'incoming': list(incoming),", "file_path": "code_search\\layered_index.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n", "symbols": ["allow_pickle", "str", "dependency_graph", "load_index", "key", "sorted_docs", "semantic_scores", "metadata", "reverse", "semantic_vector", "search", "current_nodes", "semantic_vectors", "score", "get_dependencies", "text_vectors", "exist_ok", "add_document", "semantic_index", "text_vector", "next_nodes", "text_scores", "result", "int", "semantic_model", "save_index", "business_index", "__init__", "successors", "incoming", "results", "predecessors", "remove_document", "LayeredIndex", "tfidf_vectorizer", "text_index", "layer", "combined_scores", "outgoing", "query_vector", "update_document"]}, {"content": "outgoing.update(successors)\n                next_nodes.update(successors)\n            current_nodes = next_nodes\n            \n        return {\n            'incoming': list(incoming),\n            'outgoing': list(outgoing)\n        }\n    \n    def save_index(self, save_dir: str):\n        \"\"\"\u4fdd\u5b58\u7d22\u5f15\u5230\u6587\u4ef6\"\"\"\n        os.makedirs(save_dir, exist_ok=True)\n        \n        # \u4fdd\u5b58\u8bed\u4e49\u7d22\u5f15\n        np.save(os.path.join(save_dir, 'semantic_vectors.npy'),\n                {k: v.tolist() for k, v in self.semantic_index.items()})\n        \n        # \u4fdd\u5b58\u6587\u672c\u7d22\u5f15\n        np.save(os.path.join(save_dir, 'text_vectors.npy'),\n                {k: v.tolist() for k, v in self.text_index.items()})\n        \n        # \u4fdd\u5b58\u4f9d\u8d56\u56fe\n        nx.write_gpickle(self.dependency_graph,\n                        os.path.join(save_dir, 'dependency_graph.gpickle'))\n        \n        # \u4fdd\u5b58\u4e1a\u52a1\u903b\u8f91\u7d22\u5f15\n        with open(os.path.join(save_dir, 'business_index.json'), 'w') as f:\n            json.dump(self.business_index, f)", "file_path": "code_search\\layered_index.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n", "symbols": ["allow_pickle", "str", "dependency_graph", "load_index", "key", "sorted_docs", "semantic_scores", "metadata", "reverse", "semantic_vector", "search", "current_nodes", "semantic_vectors", "score", "get_dependencies", "text_vectors", "exist_ok", "add_document", "semantic_index", "text_vector", "next_nodes", "text_scores", "result", "int", "semantic_model", "save_index", "business_index", "__init__", "successors", "incoming", "results", "predecessors", "remove_document", "LayeredIndex", "tfidf_vectorizer", "text_index", "layer", "combined_scores", "outgoing", "query_vector", "update_document"]}, {"content": "# \u4fdd\u5b58\u4e1a\u52a1\u903b\u8f91\u7d22\u5f15\n        with open(os.path.join(save_dir, 'business_index.json'), 'w') as f:\n            json.dump(self.business_index, f)\n            \n        # \u4fdd\u5b58\u5143\u6570\u636e\n        with open(os.path.join(save_dir, 'metadata.json'), 'w') as f:\n            json.dump(self.metadata, f)\n            \n    def load_index(self, save_dir: str):\n        \"\"\"\u4ece\u6587\u4ef6\u52a0\u8f7d\u7d22\u5f15\"\"\"\n        # \u52a0\u8f7d\u8bed\u4e49\u7d22\u5f15\n        semantic_vectors = np.load(os.path.join(save_dir, 'semantic_vectors.npy'),\n                                 allow_pickle=True).item()\n        self.semantic_index = {k: np.array(v) for k, v in semantic_vectors.items()}\n        \n        # \u52a0\u8f7d\u6587\u672c\u7d22\u5f15\n        text_vectors = np.load(os.path.join(save_dir, 'text_vectors.npy'),\n                             allow_pickle=True).item()\n        self.text_index = {k: np.array(v) for k, v in text_vectors.items()}\n        \n        # \u52a0\u8f7d\u4f9d\u8d56\u56fe\n        self.dependency_graph = nx.read_gpickle(\n            os.path.join(save_dir, 'dependency_graph.gpickle'))", "file_path": "code_search\\layered_index.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n", "symbols": ["allow_pickle", "str", "dependency_graph", "load_index", "key", "sorted_docs", "semantic_scores", "metadata", "reverse", "semantic_vector", "search", "current_nodes", "semantic_vectors", "score", "get_dependencies", "text_vectors", "exist_ok", "add_document", "semantic_index", "text_vector", "next_nodes", "text_scores", "result", "int", "semantic_model", "save_index", "business_index", "__init__", "successors", "incoming", "results", "predecessors", "remove_document", "LayeredIndex", "tfidf_vectorizer", "text_index", "layer", "combined_scores", "outgoing", "query_vector", "update_document"]}, {"content": "# \u52a0\u8f7d\u4f9d\u8d56\u56fe\n        self.dependency_graph = nx.read_gpickle(\n            os.path.join(save_dir, 'dependency_graph.gpickle'))\n        \n        # \u52a0\u8f7d\u4e1a\u52a1\u903b\u8f91\u7d22\u5f15\n        with open(os.path.join(save_dir, 'business_index.json'), 'r') as f:\n            self.business_index = json.load(f)\n            \n        # \u52a0\u8f7d\u5143\u6570\u636e\n        with open(os.path.join(save_dir, 'metadata.json'), 'r') as f:\n            self.metadata = json.load(f)", "file_path": "code_search\\layered_index.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n", "symbols": ["allow_pickle", "str", "dependency_graph", "load_index", "key", "sorted_docs", "semantic_scores", "metadata", "reverse", "semantic_vector", "search", "current_nodes", "semantic_vectors", "score", "get_dependencies", "text_vectors", "exist_ok", "add_document", "semantic_index", "text_vector", "next_nodes", "text_scores", "result", "int", "semantic_model", "save_index", "business_index", "__init__", "successors", "incoming", "results", "predecessors", "remove_document", "LayeredIndex", "tfidf_vectorizer", "text_index", "layer", "combined_scores", "outgoing", "query_vector", "update_document"]}, {"content": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F", "file_path": "code_search\\model_fusion.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n", "symbols": ["similarity", "model_name", "total_score", "save_weights", "inputs", "return_tensors", "relevance", "update_weights", "weights", "outputs", "padding", "predicted_score", "model_scores", "models", "final_similarity", "truncation", "ModelFusion", "doc_vector", "error", "dim", "vectorizers", "query", "tokenizer", "__init__", "model", "compute_similarity", "embeddings", "load_weights", "vectorizer", "similarities", "encode_text", "document", "query_vector", "model_type", "get_tfidf_vector"]}, {"content": "class ModelFusion:\n    def __init__(self, models_config: List[Dict[str, Any]]):\n        \"\"\"\n        \u521d\u59cb\u5316\u591a\u6a21\u578b\u878d\u5408\u7cfb\u7edf\n        \n        models_config\u683c\u5f0f\u793a\u4f8b\uff1a\n        [\n            {\n                'name': 'sentence-bert',\n                'type': 'sentence_transformers',\n                'model_name': 'all-MiniLM-L6-v2',\n                'weight': 0.4\n            },\n            {\n                'name': 'code-bert',\n                'type': 'huggingface',\n                'model_name': 'microsoft/codebert-base',\n                'weight': 0.4\n            },\n            {\n                'name': 'tfidf',\n                'type': 'tfidf',\n                'weight': 0.2\n            }\n        ]\n        \"\"\"\n        self.models = {}\n        self.weights = {}\n        self.vectorizers = {}\n        \n        for config in models_config:\n            model_name = config['name']\n            model_type = config['type']\n            \n            if model_type == 'sentence_transformers':", "file_path": "code_search\\model_fusion.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n", "symbols": ["similarity", "model_name", "total_score", "save_weights", "inputs", "return_tensors", "relevance", "update_weights", "weights", "outputs", "padding", "predicted_score", "model_scores", "models", "final_similarity", "truncation", "ModelFusion", "doc_vector", "error", "dim", "vectorizers", "query", "tokenizer", "__init__", "model", "compute_similarity", "embeddings", "load_weights", "vectorizer", "similarities", "encode_text", "document", "query_vector", "model_type", "get_tfidf_vector"]}, {"content": "for config in models_config:\n            model_name = config['name']\n            model_type = config['type']\n            \n            if model_type == 'sentence_transformers':\n                self.models[model_name] = SentenceTransformer(config['model_name'])\n            elif model_type == 'huggingface':\n                self.models[model_name] = {\n                    'tokenizer': AutoTokenizer.from_pretrained(config['model_name']),\n                    'model': AutoModel.from_pretrained(config['model_name'])\n                }\n            elif model_type == 'tfidf':\n                self.vectorizers[model_name] = TfidfVectorizer()\n                \n            self.weights[model_name] = config['weight']\n            \n    def encode_text(self, text: str, model_name: str) -> np.ndarray:\n        \"\"\"\u4f7f\u7528\u6307\u5b9a\u6a21\u578b\u7f16\u7801\u6587\u672c\"\"\"\n        model = self.models.get(model_name)\n        \n        if model is None:\n            return None\n            \n        if isinstance(model, SentenceTransformer):", "file_path": "code_search\\model_fusion.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n", "symbols": ["similarity", "model_name", "total_score", "save_weights", "inputs", "return_tensors", "relevance", "update_weights", "weights", "outputs", "padding", "predicted_score", "model_scores", "models", "final_similarity", "truncation", "ModelFusion", "doc_vector", "error", "dim", "vectorizers", "query", "tokenizer", "__init__", "model", "compute_similarity", "embeddings", "load_weights", "vectorizer", "similarities", "encode_text", "document", "query_vector", "model_type", "get_tfidf_vector"]}, {"content": "\"\"\"\u4f7f\u7528\u6307\u5b9a\u6a21\u578b\u7f16\u7801\u6587\u672c\"\"\"\n        model = self.models.get(model_name)\n        \n        if model is None:\n            return None\n            \n        if isinstance(model, SentenceTransformer):\n            return model.encode(text)\n        elif isinstance(model, dict):  # Hugging Face models\n            tokenizer = model['tokenizer']\n            model = model['model']\n            \n            inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n            with torch.no_grad():\n                outputs = model(**inputs)\n                \n            # \u4f7f\u7528\u6700\u540e\u4e00\u5c42\u9690\u85cf\u72b6\u6001\u7684\u5e73\u5747\u503c\u4f5c\u4e3a\u6587\u672c\u8868\u793a\n            embeddings = outputs.last_hidden_state.mean(dim=1)\n            return embeddings.numpy()\n            \n    def get_tfidf_vector(self, text: str, vectorizer_name: str) -> np.ndarray:\n        \"\"\"\u83b7\u53d6TF-IDF\u5411\u91cf\"\"\"\n        vectorizer = self.vectorizers.get(vectorizer_name)\n        if vectorizer is None:\n            return None\n            \n        if not hasattr(vectorizer, 'vocabulary_'):", "file_path": "code_search\\model_fusion.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n", "symbols": ["similarity", "model_name", "total_score", "save_weights", "inputs", "return_tensors", "relevance", "update_weights", "weights", "outputs", "padding", "predicted_score", "model_scores", "models", "final_similarity", "truncation", "ModelFusion", "doc_vector", "error", "dim", "vectorizers", "query", "tokenizer", "__init__", "model", "compute_similarity", "embeddings", "load_weights", "vectorizer", "similarities", "encode_text", "document", "query_vector", "model_type", "get_tfidf_vector"]}, {"content": "vectorizer = self.vectorizers.get(vectorizer_name)\n        if vectorizer is None:\n            return None\n            \n        if not hasattr(vectorizer, 'vocabulary_'):\n            vectorizer.fit([text])\n            \n        return vectorizer.transform([text]).toarray()[0]\n        \n    def compute_similarity(self, query: str, document: str) -> float:\n        \"\"\"\u8ba1\u7b97\u67e5\u8be2\u548c\u6587\u6863\u7684\u76f8\u4f3c\u5ea6\"\"\"\n        similarities = {}\n        \n        # \u5bf9\u6bcf\u4e2a\u6a21\u578b\u8ba1\u7b97\u76f8\u4f3c\u5ea6\n        for model_name, weight in self.weights.items():\n            if model_name in self.models:\n                # \u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\n                query_vector = self.encode_text(query, model_name)\n                doc_vector = self.encode_text(document, model_name)\n                \n                if query_vector is not None and doc_vector is not None:\n                    similarity = F.cosine_similarity(\n                        torch.tensor(query_vector),\n                        torch.tensor(doc_vector),\n                        dim=-1", "file_path": "code_search\\model_fusion.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n", "symbols": ["similarity", "model_name", "total_score", "save_weights", "inputs", "return_tensors", "relevance", "update_weights", "weights", "outputs", "padding", "predicted_score", "model_scores", "models", "final_similarity", "truncation", "ModelFusion", "doc_vector", "error", "dim", "vectorizers", "query", "tokenizer", "__init__", "model", "compute_similarity", "embeddings", "load_weights", "vectorizer", "similarities", "encode_text", "document", "query_vector", "model_type", "get_tfidf_vector"]}, {"content": "similarity = F.cosine_similarity(\n                        torch.tensor(query_vector),\n                        torch.tensor(doc_vector),\n                        dim=-1\n                    ).item()\n                    similarities[model_name] = similarity\n            else:\n                # \u4f7f\u7528TF-IDF\n                query_vector = self.get_tfidf_vector(query, model_name)\n                doc_vector = self.get_tfidf_vector(document, model_name)\n                \n                if query_vector is not None and doc_vector is not None:\n                    similarity = np.dot(query_vector, doc_vector) / (\n                        np.linalg.norm(query_vector) * np.linalg.norm(doc_vector)\n                    )\n                    similarities[model_name] = similarity\n                    \n        # \u52a0\u6743\u878d\u5408\u76f8\u4f3c\u5ea6\u5206\u6570\n        final_similarity = sum(\n            similarities.get(model_name, 0) * weight\n            for model_name, weight in self.weights.items()\n        )", "file_path": "code_search\\model_fusion.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n", "symbols": ["similarity", "model_name", "total_score", "save_weights", "inputs", "return_tensors", "relevance", "update_weights", "weights", "outputs", "padding", "predicted_score", "model_scores", "models", "final_similarity", "truncation", "ModelFusion", "doc_vector", "error", "dim", "vectorizers", "query", "tokenizer", "__init__", "model", "compute_similarity", "embeddings", "load_weights", "vectorizer", "similarities", "encode_text", "document", "query_vector", "model_type", "get_tfidf_vector"]}, {"content": "# \u52a0\u6743\u878d\u5408\u76f8\u4f3c\u5ea6\u5206\u6570\n        final_similarity = sum(\n            similarities.get(model_name, 0) * weight\n            for model_name, weight in self.weights.items()\n        )\n        \n        return final_similarity\n        \n    def update_weights(self, feedback_data: List[Dict[str, Any]]):\n        \"\"\"\u6839\u636e\u53cd\u9988\u66f4\u65b0\u6a21\u578b\u6743\u91cd\"\"\"\n        # feedback_data\u683c\u5f0f\uff1a\n        # [\n        #     {\n        #         'query': '\u67e5\u8be2\u6587\u672c',\n        #         'document': '\u6587\u6863\u6587\u672c',\n        #         'relevance': 1.0  # \u76f8\u5173\u6027\u5206\u6570\uff0c\u8303\u56f4[0, 1]\n        #     },\n        #     ...\n        # ]\n        \n        # \u8ba1\u7b97\u6bcf\u4e2a\u6a21\u578b\u7684\u6027\u80fd\u5206\u6570\n        model_scores = {model_name: 0.0 for model_name in self.weights}\n        \n        for feedback in feedback_data:\n            query = feedback['query']\n            document = feedback['document']\n            relevance = feedback['relevance']\n            \n            # \u8ba1\u7b97\u6bcf\u4e2a\u6a21\u578b\u7684\u9884\u6d4b\u5206\u6570\n            for model_name in self.weights:\n                if model_name in self.models:", "file_path": "code_search\\model_fusion.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n", "symbols": ["similarity", "model_name", "total_score", "save_weights", "inputs", "return_tensors", "relevance", "update_weights", "weights", "outputs", "padding", "predicted_score", "model_scores", "models", "final_similarity", "truncation", "ModelFusion", "doc_vector", "error", "dim", "vectorizers", "query", "tokenizer", "__init__", "model", "compute_similarity", "embeddings", "load_weights", "vectorizer", "similarities", "encode_text", "document", "query_vector", "model_type", "get_tfidf_vector"]}, {"content": "relevance = feedback['relevance']\n            \n            # \u8ba1\u7b97\u6bcf\u4e2a\u6a21\u578b\u7684\u9884\u6d4b\u5206\u6570\n            for model_name in self.weights:\n                if model_name in self.models:\n                    query_vector = self.encode_text(query, model_name)\n                    doc_vector = self.encode_text(document, model_name)\n                else:\n                    query_vector = self.get_tfidf_vector(query, model_name)\n                    doc_vector = self.get_tfidf_vector(document, model_name)\n                    \n                if query_vector is not None and doc_vector is not None:\n                    predicted_score = np.dot(query_vector, doc_vector) / (\n                        np.linalg.norm(query_vector) * np.linalg.norm(doc_vector)\n                    )\n                    # \u8ba1\u7b97\u9884\u6d4b\u5206\u6570\u4e0e\u5b9e\u9645\u76f8\u5173\u6027\u7684\u5dee\u5f02\n                    error = abs(predicted_score - relevance)\n                    model_scores[model_name] += (1 - error)\n                    \n        # \u5f52\u4e00\u5316\u5206\u6570", "file_path": "code_search\\model_fusion.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n", "symbols": ["similarity", "model_name", "total_score", "save_weights", "inputs", "return_tensors", "relevance", "update_weights", "weights", "outputs", "padding", "predicted_score", "model_scores", "models", "final_similarity", "truncation", "ModelFusion", "doc_vector", "error", "dim", "vectorizers", "query", "tokenizer", "__init__", "model", "compute_similarity", "embeddings", "load_weights", "vectorizer", "similarities", "encode_text", "document", "query_vector", "model_type", "get_tfidf_vector"]}, {"content": "# \u8ba1\u7b97\u9884\u6d4b\u5206\u6570\u4e0e\u5b9e\u9645\u76f8\u5173\u6027\u7684\u5dee\u5f02\n                    error = abs(predicted_score - relevance)\n                    model_scores[model_name] += (1 - error)\n                    \n        # \u5f52\u4e00\u5316\u5206\u6570\n        total_score = sum(model_scores.values())\n        if total_score > 0:\n            for model_name in self.weights:\n                self.weights[model_name] = model_scores[model_name] / total_score\n                \n    def save_weights(self, save_path: str):\n        \"\"\"\u4fdd\u5b58\u6a21\u578b\u6743\u91cd\"\"\"\n        import json\n        with open(save_path, 'w') as f:\n            json.dump(self.weights, f)\n            \n    def load_weights(self, save_path: str):\n        \"\"\"\u52a0\u8f7d\u6a21\u578b\u6743\u91cd\"\"\"\n        import json\n        with open(save_path, 'r') as f:\n            self.weights = json.load(f)", "file_path": "code_search\\model_fusion.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, List, Any, Optional\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n", "symbols": ["similarity", "model_name", "total_score", "save_weights", "inputs", "return_tensors", "relevance", "update_weights", "weights", "outputs", "padding", "predicted_score", "model_scores", "models", "final_similarity", "truncation", "ModelFusion", "doc_vector", "error", "dim", "vectorizers", "query", "tokenizer", "__init__", "model", "compute_similarity", "embeddings", "load_weights", "vectorizer", "similarities", "encode_text", "document", "query_vector", "model_type", "get_tfidf_vector"]}, {"content": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\nimport numpy as np\nfrom pathlib import Path\nimport logging\nfrom dataclasses import dataclass\nfrom concurrent.futures import ThreadPoolExecutor\nimport json\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import Language, RecursiveCharacterTextSplitter\nfrom langchain.schema import Document\nimport hashlib\nfrom functools import lru_cache\nimport re\n\n@dataclass\nclass CodeSnippet:\n    content: str\n    file_path: str\n    repo_path: str\n    start_line: int\n    end_line: int\n    language: str\n    context: Optional[str] = None\n    symbols: Optional[List[str]] = None\n\n@dataclass\nclass SearchResult:\n    snippet: CodeSnippet\n    score: float", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "class EnhancedIndex:\n    \"\"\"Enhanced FAISS index with optimizations for code search.\"\"\"\n    def __init__(self, dimension: int, nlist: int = 100):\n        # Use IVF (Inverted File Index) for faster search\n        quantizer = faiss.IndexFlatL2(dimension)\n        self.index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_L2)\n        self.is_trained = False\n        \n    def train(self, vectors: np.ndarray):\n        if not self.is_trained and len(vectors) > 0:\n            self.index.train(vectors)\n            self.is_trained = True\n    \n    def add(self, vectors: np.ndarray):\n        if not self.is_trained:\n            self.train(vectors)\n        self.index.add(vectors)\n    \n    def search(self, query: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:\n        # Adjust nprobe based on k to balance speed and recall\n        self.index.nprobe = min(20, k * 2)\n        return self.index.search(query, k)\n    \n    def save(self, path: str):", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "# Adjust nprobe based on k to balance speed and recall\n        self.index.nprobe = min(20, k * 2)\n        return self.index.search(query, k)\n    \n    def save(self, path: str):\n        faiss.write_index(self.index, path)\n    \n    def load(self, path: str):\n        self.index = faiss.read_index(path)\n        self.is_trained = True", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "class CodeSearchEngine:\n    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\", cache_size: int = 1000):\n        self.embeddings = HuggingFaceEmbeddings(\n            model_name=model_name,\n            model_kwargs={'device': 'cuda' if os.environ.get('USE_CUDA', '0') == '1' else 'cpu'},\n            encode_kwargs={'normalize_embeddings': True}\n        )\n        self.index = None\n        self.snippets: List[CodeSnippet] = []\n        self.index_to_snippet: Dict[int, CodeSnippet] = {}\n        self.indexed_files: Set[str] = set()\n        self.symbol_cache: Dict[str, Set[str]] = {}\n        \n        # Initialize logger\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.INFO)", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "# Initialize text splitters for different languages\n        self.language_splitters = {\n            'python': RecursiveCharacterTextSplitter.from_language(\n                language=Language.PYTHON,\n                chunk_size=1000,\n                chunk_overlap=200\n            ),\n            'javascript': RecursiveCharacterTextSplitter.from_language(\n                language=Language.JS,\n                chunk_size=1000,\n                chunk_overlap=200\n            ),\n            'java': RecursiveCharacterTextSplitter.from_language(\n                language=Language.JAVA,\n                chunk_size=1000,\n                chunk_overlap=200\n            ),\n            'cpp': RecursiveCharacterTextSplitter.from_language(\n                language=Language.CPP,\n                chunk_size=1000,\n                chunk_overlap=200\n            ),\n            'rust': RecursiveCharacterTextSplitter.from_language(\n                language=Language.RUST,\n                chunk_size=1000,", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "chunk_overlap=200\n            ),\n            'rust': RecursiveCharacterTextSplitter.from_language(\n                language=Language.RUST,\n                chunk_size=1000,\n                chunk_overlap=200\n            ),\n            'go': RecursiveCharacterTextSplitter.from_language(\n                language=Language.GO,\n                chunk_size=1000,\n                chunk_overlap=200\n            ),\n            'default': RecursiveCharacterTextSplitter(\n                chunk_size=1000,\n                chunk_overlap=200,\n                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n            )\n        }\n        \n        # Initialize symbol extractors\n        self.symbol_patterns = {\n            'python': [\n                r'class\\s+(\\w+)',\n                r'def\\s+(\\w+)',\n                r'(\\w+)\\s*=\\s*',\n            ],\n            'javascript': [\n                r'class\\s+(\\w+)',\n                r'function\\s+(\\w+)',\n                r'const\\s+(\\w+)\\s*=',", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "r'(\\w+)\\s*=\\s*',\n            ],\n            'javascript': [\n                r'class\\s+(\\w+)',\n                r'function\\s+(\\w+)',\n                r'const\\s+(\\w+)\\s*=',\n                r'let\\s+(\\w+)\\s*=',\n            ],\n            'java': [\n                r'class\\s+(\\w+)',\n                r'(public|private|protected)\\s+\\w+\\s+(\\w+)\\s*\\(',\n                r'interface\\s+(\\w+)',\n            ],\n        }\n        \n        # Setup caching\n        self._get_code_embedding = lru_cache(maxsize=cache_size)(self._compute_embedding)\n        \n    def _compute_embedding(self, code: str) -> np.ndarray:\n        \"\"\"Compute embedding for a code snippet.\"\"\"\n        try:\n            embeddings = self.embeddings.embed_documents([code])\n            return np.array(embeddings[0])\n        except Exception as e:\n            self.logger.error(f\"Error generating embedding: {str(e)}\")\n            return np.zeros(384)", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "def _extract_symbols(self, content: str, language: str) -> Set[str]:\n        \"\"\"Extract code symbols (class names, function names, etc.) from content.\"\"\"\n        patterns = self.symbol_patterns.get(language, [])\n        symbols = set()\n        \n        for pattern in patterns:\n            matches = re.finditer(pattern, content)\n            for match in matches:\n                # Get the last group (actual symbol name)\n                symbol = match.groups()[-1]\n                if symbol:\n                    symbols.add(symbol)\n                    \n        return symbols", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "def _get_file_context(self, file_path: str, start_line: int, end_line: int, context_lines: int = 3) -> str:\n        \"\"\"Get surrounding context for a code snippet.\"\"\"\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                lines = f.readlines()\n                \n            start_ctx = max(0, start_line - context_lines)\n            end_ctx = min(len(lines), end_line + context_lines)\n            \n            return ''.join(lines[start_ctx:end_ctx])\n        except Exception as e:\n            self.logger.error(f\"Error getting context: {str(e)}\")\n            return \"\"", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "def _detect_language(self, file_path: str) -> str:\n        \"\"\"Detect programming language from file extension.\"\"\"\n        ext = Path(file_path).suffix.lower()\n        language_map = {\n            '.py': 'python',\n            '.js': 'javascript',\n            '.java': 'java',\n            '.cpp': 'cpp',\n            '.c': 'cpp',\n            '.rs': 'rust',\n            '.go': 'go',\n            '.ts': 'javascript',\n            '.rb': 'ruby',\n            '.php': 'php',\n        }\n        return language_map.get(ext, 'default')\n\n    def _split_code_to_snippets(self, content: str, language: str) -> List[Document]:\n        \"\"\"Split code content into snippets using LangChain's text splitter.\"\"\"\n        splitter = self.language_splitters.get(language, self.language_splitters['default'])\n        return splitter.create_documents([content])", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "def add_repository(self, repo_path: str, file_patterns: Optional[List[str]] = None):\n        \"\"\"Index all code files in a repository.\"\"\"\n        if file_patterns is None:\n            file_patterns = ['*.py', '*.js', '*.java', '*.cpp', '*.c', '*.rs', '*.go', '*.ts']\n            \n        repo_path = Path(repo_path)\n        new_snippets = []\n        \n        def process_file(file_path: Path):\n            if str(file_path) in self.indexed_files:\n                return []\n                \n            try:\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                \n                language = self._detect_language(str(file_path))\n                documents = self._split_code_to_snippets(content, language)\n                \n                # Extract symbols from the entire file\n                file_symbols = self._extract_symbols(content, language)\n                \n                file_snippets = []", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "# Extract symbols from the entire file\n                file_symbols = self._extract_symbols(content, language)\n                \n                file_snippets = []\n                for doc in documents:\n                    start_line = doc.metadata.get('start_line', 0)\n                    end_line = doc.metadata.get('end_line', 0)\n                    \n                    # Get context for the snippet\n                    context = self._get_file_context(\n                        str(file_path), \n                        start_line, \n                        end_line\n                    )\n                    \n                    snippet = CodeSnippet(\n                        content=doc.page_content,\n                        file_path=str(file_path.relative_to(repo_path)),\n                        repo_path=str(repo_path),\n                        start_line=start_line,\n                        end_line=end_line,\n                        language=language,", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "repo_path=str(repo_path),\n                        start_line=start_line,\n                        end_line=end_line,\n                        language=language,\n                        context=context,\n                        symbols=list(file_symbols)\n                    )\n                    file_snippets.append(snippet)\n                    \n                return file_snippets\n                \n            except Exception as e:\n                self.logger.error(f\"Error processing file {file_path}: {str(e)}\")\n                return []", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "# Collect all files matching patterns\n        all_files = []\n        for pattern in file_patterns:\n            all_files.extend(repo_path.rglob(pattern))\n\n        # Process files in parallel\n        with ThreadPoolExecutor() as executor:\n            for file_snippets in executor.map(process_file, all_files):\n                new_snippets.extend(file_snippets)\n                \n        if not new_snippets:\n            return\n\n        # Generate embeddings for new snippets\n        embeddings = []\n        for snippet in new_snippets:\n            # Create enhanced embedding by combining code and symbols\n            enhanced_content = f\"{snippet.content}\\n{' '.join(snippet.symbols or [])}\"\n            embedding = self._get_code_embedding(enhanced_content)\n            embeddings.append(embedding)\n\n        embeddings = np.array(embeddings)", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "# Update or create FAISS index\n        if self.index is None:\n            self.index = EnhancedIndex(embeddings.shape[1])\n            \n        self.index.add(embeddings)\n        \n        # Update snippets and mapping\n        start_idx = len(self.snippets)\n        for i, snippet in enumerate(new_snippets):\n            self.index_to_snippet[start_idx + i] = snippet\n            self.indexed_files.add(str(Path(snippet.repo_path) / snippet.file_path))\n            \n        self.snippets.extend(new_snippets)\n        \n    def search(self, \n              query: str, \n              k: int = 5, \n              language_filter: Optional[str] = None,\n              min_score: float = 0.5\n              ) -> List[SearchResult]:\n        \"\"\"Search for code snippets similar to the query with enhanced filtering.\"\"\"\n        if self.index is None or len(self.snippets) == 0:\n            return []\n        \n        # Enhance query with potential code symbols", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "if self.index is None or len(self.snippets) == 0:\n            return []\n        \n        # Enhance query with potential code symbols\n        symbols = set(re.findall(r'\\b\\w+\\b', query))\n        enhanced_query = query\n        for symbol in symbols:\n            if any(symbol in s.symbols for s in self.snippets if s.symbols):\n                enhanced_query = f\"{enhanced_query} {symbol}\"\n            \n        query_embedding = self._get_code_embedding(enhanced_query)\n        distances, indices = self.index.search(query_embedding.reshape(1, -1), k * 2)\n        \n        results = []\n        for distance, idx in zip(distances[0], indices[0]):\n            score = 1.0 / (1.0 + distance)\n            if score < min_score:\n                continue\n                \n            snippet = self.index_to_snippet[idx]\n            \n            # Apply language filter if specified\n            if language_filter and snippet.language != language_filter:\n                continue", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "# Apply language filter if specified\n            if language_filter and snippet.language != language_filter:\n                continue\n                \n            results.append(SearchResult(\n                snippet=snippet,\n                score=score\n            ))\n            \n            if len(results) >= k:\n                break\n                \n        return results", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "def save_index(self, path: str):\n        \"\"\"Save the search index and metadata to disk.\"\"\"\n        if self.index is None:\n            raise ValueError(\"No index to save\")\n            \n        os.makedirs(path, exist_ok=True)\n        \n        # Save FAISS index\n        self.index.save(os.path.join(path, \"code_search.index\"))\n        \n        # Save snippets and mapping\n        metadata = {\n            \"snippets\": [\n                {\n                    \"content\": s.content,\n                    \"file_path\": s.file_path,\n                    \"repo_path\": s.repo_path,\n                    \"start_line\": s.start_line,\n                    \"end_line\": s.end_line,\n                    \"language\": s.language,\n                    \"context\": s.context,\n                    \"symbols\": s.symbols\n                }\n                for s in self.snippets\n            ],\n            \"indexed_files\": list(self.indexed_files)\n        }", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "\"symbols\": s.symbols\n                }\n                for s in self.snippets\n            ],\n            \"indexed_files\": list(self.indexed_files)\n        }\n        \n        with open(os.path.join(path, \"metadata.json\"), 'w') as f:\n            json.dump(metadata, f)\n            \n    def load_index(self, path: str):\n        \"\"\"Load the search index and metadata from disk.\"\"\"\n        if not os.path.exists(path):\n            raise ValueError(f\"Index path {path} does not exist\")\n            \n        # Load FAISS index\n        self.index = EnhancedIndex(384)  # Default dimension for all-MiniLM-L6-v2\n        self.index.load(os.path.join(path, \"code_search.index\"))\n        \n        # Load metadata\n        with open(os.path.join(path, \"metadata.json\"), 'r') as f:\n            metadata = json.load(f)\n            \n        self.snippets = []\n        self.index_to_snippet = {}\n        \n        for i, s in enumerate(metadata[\"snippets\"]):\n            snippet = CodeSnippet(", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "self.snippets = []\n        self.index_to_snippet = {}\n        \n        for i, s in enumerate(metadata[\"snippets\"]):\n            snippet = CodeSnippet(\n                content=s[\"content\"],\n                file_path=s[\"file_path\"],\n                repo_path=s[\"repo_path\"],\n                start_line=s[\"start_line\"],\n                end_line=s[\"end_line\"],\n                language=s[\"language\"],\n                context=s.get(\"context\"),\n                symbols=s.get(\"symbols\", [])\n            )\n            self.snippets.append(snippet)\n            self.index_to_snippet[i] = snippet\n            \n        self.indexed_files = set(metadata[\"indexed_files\"])", "file_path": "code_search\\search_engine.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "import os\nfrom typing import List, Dict, Optional, Set, Tuple\nimport faiss\n", "symbols": ["model_name", "train", "enhanced_query", "class", "names", "search", "maxsize", "language", "embedding", "indices", "start_idx", "EnhancedIndex", "ext", "start_ctx", "repo_path", "file_snippets", "file_path", "separators", "_split_code_to_snippets", "_get_code_embedding", "start_line", "exist_ok", "splitter", "index_to_snippet", "load", "save", "int", "CodeSearchEngine", "file_symbols", "add", "symbol", "enhanced_content", "__init__", "snippet", "encoding", "file_patterns", "str", "snippets", "all_files", "load_index", "indexed_files", "float", "from", "is_trained", "language_map", "language_splitters", "score", "documents", "lines", "symbols", "nprobe", "_detect_language", "_get_file_context", "chunk_overlap", "new_snippets", "embeddings", "results", "matches", "patterns", "chunk_size", "end_line", "context", "add_repository", "metadata", "index", "_extract_symbols", "encode_kwargs", "content", "process_file", "symbol_patterns", "_compute_embedding", "quantizer", "save_index", "model_kwargs", "query_embedding", "end_ctx", "logger"]}, {"content": "from typing import Dict, Any\nfrom .base_expert import ExpertModel\n\nclass ArchitectureExpert(ExpertModel):\n    \"\"\"Expert model for analyzing code architecture and design patterns.\"\"\"\n    \n    async def analyze(self, code: str, context: Dict[str, Any]) -> str:\n        prompt = \"\"\"\n        As an architecture expert, analyze the given code and context. Focus on:\n        1. Overall code structure and design patterns\n        2. Modularity and component interactions\n        3. Scalability and maintainability of the architecture\n        4. Suggestions for architectural improvements\n\n        Code:\n        {code}\n\n        Context:\n        {context}\n\n        Provide a detailed analysis of the code's architecture.\n        \"\"\"\n        return await self._call_openai_api(prompt.format(code=code, context=json.dumps(context)))", "file_path": "experts\\architecture_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, Any\nfrom .base_expert import ExpertModel\n\n", "symbols": ["ArchitectureExpert", "context", "code", "prompt", "analyze"]}, {"content": "from abc import ABC, abstractmethod\nfrom typing import Any, Dict\n\nclass ExpertModel(ABC):\n    \"\"\"Base class for all expert models.\"\"\"\n    def __init__(self, api_key: str, model: str = \"gpt-3.5-turbo\"):\n        self.api_key = api_key\n        self.model = model\n\n    @abstractmethod\n    async def analyze(self, code: str, context: Dict[str, Any]) -> str:\n        \"\"\"Analyze code with given context and return analysis results.\"\"\"\n        pass", "file_path": "experts\\base_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from abc import ABC, abstractmethod\nfrom typing import Any, Dict\n\n", "symbols": ["str", "api_key", "model", "for", "ExpertModel", "__init__", "analyze"]}, {"content": "from typing import Dict, Any\nimport json\nfrom .base_expert import ExpertModel\n\nclass CodeQualityExpert(ExpertModel):\n    \"\"\"Expert model for analyzing code quality and maintainability.\"\"\"\n    \n    async def analyze(self, code: str, context: Dict[str, Any]) -> str:\n        prompt = \"\"\"\n        As a code quality expert, analyze the given code and context. Focus on:\n        1. Adherence to coding standards and best practices\n        2. Code readability and maintainability\n        3. Proper use of comments and documentation\n        4. Suggestions for improving code quality\n\n        Code:\n        {code}\n\n        Context:\n        {context}\n\n        Provide a detailed analysis of the code's quality.\n        \"\"\"\n        return await self._call_openai_api(prompt.format(code=code, context=json.dumps(context)))", "file_path": "experts\\code_quality_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, Any\nimport json\nfrom .base_expert import ExpertModel\n", "symbols": ["prompt", "context", "code", "CodeQualityExpert", "analyze"]}, {"content": "from typing import Dict, Any\nimport json\nfrom .base_expert import ExpertModel", "file_path": "experts\\code_summary_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, Any\nimport json\nfrom .base_expert import ExpertModel\n", "symbols": ["CodeSummaryExpert", "context", "code", "prompt", "analyze"]}, {"content": "class CodeSummaryExpert(ExpertModel):\n    \"\"\"Expert model for generating code summaries.\"\"\"\n    \n    async def analyze(self, code: str, context: Dict[str, Any]) -> str:\n        prompt = \"\"\"\n        As a code summarization expert, analyze the given code and context. Your task is to generate a high-level summary of the code that captures its main functionality, structure, and purpose. Focus on:\n\n        1. The overall purpose of the code\n        2. Key components or modules and their roles\n        3. Main algorithms or processes implemented\n        4. Important data structures used\n        5. External dependencies and their purposes\n        6. Any notable design patterns or architectural choices\n\n        Provide a concise yet informative summary that would help a developer quickly understand the essence of this code without delving into every detail.\n\n        Code:\n        {code}\n\n        Context:\n        {context}", "file_path": "experts\\code_summary_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, Any\nimport json\nfrom .base_expert import ExpertModel\n", "symbols": ["CodeSummaryExpert", "context", "code", "prompt", "analyze"]}, {"content": "Code:\n        {code}\n\n        Context:\n        {context}\n\n        Generate a comprehensive summary of the code in about 200-300 words.\n        \"\"\"\n        return await self._call_openai_api(prompt.format(code=code, context=json.dumps(context)))", "file_path": "experts\\code_summary_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, Any\nimport json\nfrom .base_expert import ExpertModel\n", "symbols": ["CodeSummaryExpert", "context", "code", "prompt", "analyze"]}, {"content": "from typing import Dict, Any\nimport json\nfrom .base_expert import ExpertModel\n\nclass PerformanceExpert(ExpertModel):\n    \"\"\"Expert model for analyzing code performance and efficiency.\"\"\"\n    \n    async def analyze(self, code: str, context: Dict[str, Any]) -> str:\n        prompt = \"\"\"\n        As a performance expert, analyze the given code and context. Focus on:\n        1. Algorithmic efficiency\n        2. Resource usage (CPU, memory, I/O)\n        3. Potential bottlenecks and performance hotspots\n        4. Optimization suggestions\n\n        Code:\n        {code}\n\n        Context:\n        {context}\n\n        Provide a detailed analysis of the code's performance characteristics.\n        \"\"\"\n        return await self._call_openai_api(prompt.format(code=code, context=json.dumps(context)))", "file_path": "experts\\performance_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, Any\nimport json\nfrom .base_expert import ExpertModel\n", "symbols": ["context", "code", "prompt", "analyze", "PerformanceExpert"]}, {"content": "from typing import Dict, Any\nimport json\nfrom .base_expert import ExpertModel\n\nclass SecurityExpert(ExpertModel):\n    \"\"\"Expert model for analyzing code security and vulnerabilities.\"\"\"\n    \n    async def analyze(self, code: str, context: Dict[str, Any]) -> str:\n        prompt = \"\"\"\n        As a security expert, analyze the given code and context. Focus on:\n        1. Potential security vulnerabilities\n        2. Adherence to security best practices\n        3. Data handling and privacy concerns\n        4. Recommendations for security improvements\n\n        Code:\n        {code}\n\n        Context:\n        {context}\n\n        Provide a detailed security analysis of the code.\n        \"\"\"\n        return await self._call_openai_api(prompt.format(code=code, context=json.dumps(context)))", "file_path": "experts\\security_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, Any\nimport json\nfrom .base_expert import ExpertModel\n", "symbols": ["SecurityExpert", "context", "code", "prompt", "analyze"]}, {"content": "from typing import Dict, Any\nimport json\nfrom .base_expert import ExpertModel\n\nclass TestingExpert(ExpertModel):\n    \"\"\"Expert model for analyzing code testing aspects.\"\"\"\n    \n    async def analyze(self, code: str, context: Dict[str, Any]) -> str:\n        prompt = \"\"\"\n        As a testing expert, analyze the given code and context. Focus on:\n        1. Test coverage and quality\n        2. Potential edge cases and error scenarios\n        3. Testability of the code\n        4. Suggestions for improving test suite\n\n        Code:\n        {code}\n\n        Context:\n        {context}\n\n        Provide a detailed analysis of the code's testing aspects.\n        \"\"\"\n        return await self._call_openai_api(prompt.format(code=code, context=json.dumps(context)))", "file_path": "experts\\testing_expert.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from typing import Dict, Any\nimport json\nfrom .base_expert import ExpertModel\n", "symbols": ["TestingExpert", "context", "code", "prompt", "analyze"]}, {"content": "from .base_expert import ExpertModel\nfrom .architecture_expert import ArchitectureExpert\nfrom .performance_expert import PerformanceExpert\nfrom .security_expert import SecurityExpert\nfrom .code_quality_expert import CodeQualityExpert\nfrom .testing_expert import TestingExpert\nfrom .code_summary_expert import CodeSummaryExpert\n\n__all__ = [\n    'ExpertModel',\n    'ArchitectureExpert',\n    'PerformanceExpert',\n    'SecurityExpert',\n    'CodeQualityExpert',\n    'TestingExpert',\n    'CodeSummaryExpert'\n]", "file_path": "experts\\__init__.py", "repo_path": "d:\\workspaces\\python_projects\\ai_demo\\code_analyze", "start_line": 0, "end_line": 0, "language": "python", "context": "from .base_expert import ExpertModel\nfrom .architecture_expert import ArchitectureExpert\nfrom .performance_expert import PerformanceExpert\n", "symbols": ["__all__"]}], "indexed_files": ["d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\LLMApi_4.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\large_code_analyzer.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\LLMApi_3.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\code_search\\example.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\experts\\code_summary_expert.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\analyzers\\__init__.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\LLMApi_1.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\enhanced_llm_api.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\large_code_analyzer_2.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\analyzers\\business_logic_analyzer.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\git_utils.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\experts\\code_quality_expert.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\large_code_analyzer_3.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\Large_code_analyzer4.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\code_search\\model_fusion.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\analyzers\\file_analyzer.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\code_search\\enhanced_search_engine.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\experts\\testing_expert.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\code_search\\incremental_indexer.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\EnhancedLLMApi.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\analyzers\\code_indexer.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\experts\\architecture_expert.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\experts\\security_expert.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\example.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\experts\\__init__.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\java_parse.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\experts\\performance_expert.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\code_search\\code_analyzer.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\code_search\\search_engine.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\LLMApi_2.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\experts\\base_expert.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\analyze_expert.py", "d:\\workspaces\\python_projects\\ai_demo\\code_analyze\\code_search\\layered_index.py"]}