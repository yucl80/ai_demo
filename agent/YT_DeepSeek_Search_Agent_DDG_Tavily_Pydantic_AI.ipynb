{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYMN38rMsnJp"
      },
      "source": [
        "# Building Structured Agents with Pydantic AI and DeepSeek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBChdle68C3E",
        "outputId": "cc5cb25b-e888-49be-a9a0-cdbce25b56b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llama-index-core 0.12.1 requires pydantic<2.10.0,>=2.7.0, but you have pydantic 2.10.6 which is incompatible.\n",
            "llama-index-core 0.12.1 requires tenacity!=8.4.0,<9.0.0,>=8.2.0, but you have tenacity 9.0.0 which is incompatible.\n",
            "llama-index-legacy 0.9.48.post4 requires tenacity<9.0.0,>=8.2.0, but you have tenacity 9.0.0 which is incompatible.\n",
            "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "unstructured-client 0.28.0 requires pydantic<2.10.0,>=2.9.2, but you have pydantic 2.10.6 which is incompatible.\n",
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip -q install pydantic-ai\n",
        "!pip -q install nest_asyncio\n",
        "!pip -q install devtools\n",
        "!pip -q install tavily-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2rUPJxt4t-r"
      },
      "source": [
        "### **Restart the notebook**\n",
        "\n",
        "You will need to restart the notebook after you've installed Pydantic AI and the other dependencies above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icRZD-DW8JZf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GOOGLE_AI_STUDIO')\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "os.environ[\"DEEPSEEK_API_KEY\"] = userdata.get('DEEPSEEK_API_KEY')\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY') # If you dont have this use the DDGS alternative below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKpEfbHCO0b5"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUsfwyaIgVZE"
      },
      "source": [
        "## Setting Up search Tavily\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CEVVhRnscdr"
      },
      "source": [
        "### Tavily Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3r_wxilsepA",
        "outputId": "51eb3f06-3134-48a7-dae7-722fca71480e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'title': 'DeepSeek-R1: In-Depth Review and Usage Guide', 'url': 'https://neuroflash.com/blog/deepseek-r1/', 'content': \"Helpcenter Access guides and support on how to use neuroflash's AI tools. Free Guides & Templates Download free documents with knowledge on AI content creation. DALL-E 3, GPT-4, Brand Voices, file upload and more - integrated in the neuroflash AI Marketing Content Suite DALL-E 3, GPT-4, Brand Voices, file upload and more - integrated in the neuroflash AI Marketing Content Suite DeepSeek-R1-Lite-Preview is an AI model developed by the Chinese company DeepSeek, similar to ChatGPT. While DeepSeek-R1 excels in reasoning capabilities, when it comes to AI text generation and personalization, neuroflash’s ChatFlash emerges as a top alternative of all GPT models. DeepSeek-R1 is poised to revolutionize the way we understand AI reasoning models, offering superior performance, transparency, and versatility.\", 'score': 0.9035075, 'raw_content': None}, {'title': \"DeepSeek Unveils DeepSeek-R1 RL Models: What's New and How It is better ...\", 'url': 'https://www.geeksforgeeks.org/deepseek-r1-rl-models-whats-new/', 'content': \"In a recent innovative announcement, Chinese AI lab DeepSeek (which recently launched DeepSeek-V3 that outperformed models like Meta and OpenAI) has now revealed its latest powerful *reasoning large language model, the DeepSeek-R1*, a reinforcement learning (RL) model designed to push the boundaries of artificial intelligence. *Explainability Features*: Addressing a significant gap in RL models, DeepSeek-R1 provides built-in tools for explainable AI (XAI). Read In Short: The UAE's Falcon 2 AI model is new in competing with tech giants like Meta and OpenAI.This open-source model offers both text-based and image-centric capabilities, with applications in content creation, education, and accessibility.Falcon 2's open-source nature brings in collaboration and potentially speeds up the development of AI technology globally.\", 'score': 0.87807417, 'raw_content': None}, {'title': 'README.md · deepseek-ai/DeepSeek-R1 at main - Hugging Face', 'url': 'https://huggingface.co/deepseek-ai/DeepSeek-R1/blob/main/README.md', 'content': 'DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. DeepSeek-R1 Models DeepSeek-R1-Distill Models DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1. DeepSeek-R1 Models DeepSeek-R1-Distill Models DeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models. vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager', 'score': 0.8524574, 'raw_content': None}]\n"
          ]
        }
      ],
      "source": [
        "from tavily import TavilyClient, AsyncTavilyClient\n",
        "\n",
        "# Setup the Tavily Client\n",
        "tavily_client = AsyncTavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
        "\n",
        "# Simple Search\n",
        "response = await tavily_client.search(\"What is DeepSeekR1?\", max_results=3)\n",
        "\n",
        "print(response['results'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRc-tLALv6CP",
        "outputId": "8d117f68-e9a4-4b62-857f-2a9279fc3267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DeepSeek-R1: In-Depth Review and Usage Guide\n",
            "Helpcenter Access guides and support on how to use neuroflash's AI tools. Free Guides & Templates Download free documents with knowledge on AI content creation. DALL-E 3, GPT-4, Brand Voices, file upload and more - integrated in the neuroflash AI Marketing Content Suite DALL-E 3, GPT-4, Brand Voices, file upload and more - integrated in the neuroflash AI Marketing Content Suite DeepSeek-R1-Lite-Preview is an AI model developed by the Chinese company DeepSeek, similar to ChatGPT. While DeepSeek-R1 excels in reasoning capabilities, when it comes to AI text generation and personalization, neuroflash’s ChatFlash emerges as a top alternative of all GPT models. DeepSeek-R1 is poised to revolutionize the way we understand AI reasoning models, offering superior performance, transparency, and versatility.\n",
            "\n",
            "\n",
            "DeepSeek Unveils DeepSeek-R1 RL Models: What's New and How It is better ...\n",
            "In a recent innovative announcement, Chinese AI lab DeepSeek (which recently launched DeepSeek-V3 that outperformed models like Meta and OpenAI) has now revealed its latest powerful *reasoning large language model, the DeepSeek-R1*, a reinforcement learning (RL) model designed to push the boundaries of artificial intelligence. *Explainability Features*: Addressing a significant gap in RL models, DeepSeek-R1 provides built-in tools for explainable AI (XAI). Read In Short: The UAE's Falcon 2 AI model is new in competing with tech giants like Meta and OpenAI.This open-source model offers both text-based and image-centric capabilities, with applications in content creation, education, and accessibility.Falcon 2's open-source nature brings in collaboration and potentially speeds up the development of AI technology globally.\n",
            "\n",
            "\n",
            "README.md · deepseek-ai/DeepSeek-R1 at main - Hugging Face\n",
            "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. DeepSeek-R1 Models DeepSeek-R1-Distill Models DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1. DeepSeek-R1 Models DeepSeek-R1-Distill Models DeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models. vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Responses\n",
        "for result in response['results']:\n",
        "    print(result['title'])\n",
        "    print(result['content'])\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2WhsgwstOFq",
        "outputId": "35716a8e-dff4-46fb-ed41-22da40d469c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"[\\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://autogpt.net/new-chinese-ai-model-deepseek-r1-can-think-like-humans/\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"New Chinese AI Model, DeepSeek-R1, Can \\\\\\\\u201cThink\\\\\\\\u201d Like Humans AI AI Tools New Chinese AI Model, DeepSeek-R1, Can \\\\\\\\u201cThink\\\\\\\\u201d Like Humans The competition in the AI world is heating up, and this week, the spotlight is on DeepSeek, a Chinese AI research company that just unveiled DeepSeek-R1, a reasoning AI model designed to rival OpenAI\\\\\\\\u2019s o1. Unlike traditional AI models, which often rely on brute-force computations and statistical patterns, reasoning models like DeepSeek-R1 take a more thoughtful approach. DeepSeek-R1 represents a significant step forward for reasoning models and reflects the intensifying competition in the global AI landscape. AI Tools AI Tools AI AI AI Models AI Tools AI AI Tools AI AI Tools AI AI Tools\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://www.deeplearning.ai/the-batch/deepseek-r1-a-transparent-challenger-to-openai-o1/\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"DeepSeek-R1, A Transparent Challenger to OpenAI o1 Reasoning Revealed DeepSeek-R1, a transparent challenger to OpenAI o1 An up-and-coming Hangzhou AI lab unveiled a model that implements run-time reasoning similar to OpenAI o1 and delivers competitive performance. R1-lite-preview performs comparably to o1-preview on several math and problem-solving benchmarks. Unlike o1-preview, which hides its reasoning, at inference, DeepSeek-R1-lite-preview\\\\\\\\u2019s reasoning steps are visible. According to DeepSeek, R1-lite-preview, using an unspecified number of reasoning tokens, outperforms OpenAI o1-preview, OpenAI GPT-4o, Anthropic Claude 3.5 Sonnet, Alibaba Qwen 2.5 72B, and DeepSeek-V2.5 on three out of six reasoning-intensive benchmarks. Behind the news:\\\\\\\\u00a0DeepSeek-R1 follows OpenAI in implementing this approach at a time when scaling laws that predict higher performance from bigger models and/or more training data are being\\\\\\\\u00a0questioned.\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://www.geeksforgeeks.org/deepseek-r1-rl-models-whats-new/\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"In a recent innovative announcement, Chinese AI lab DeepSeek (which recently launched DeepSeek-V3 that outperformed models like Meta and OpenAI) has now revealed its latest powerful *reasoning large language model, the DeepSeek-R1*, a reinforcement learning (RL) model designed to push the boundaries of artificial intelligence. *Explainability Features*: Addressing a significant gap in RL models, DeepSeek-R1 provides built-in tools for explainable AI (XAI). Read In Short: The UAE's Falcon 2 AI model is new in competing with tech giants like Meta and OpenAI.This open-source model offers both text-based and image-centric capabilities, with applications in content creation, education, and accessibility.Falcon 2's open-source nature brings in collaboration and potentially speeds up the development of AI technology globally.\\\\\\\"}\\\"]\"\n"
          ]
        }
      ],
      "source": [
        "# RAG Context search\n",
        "\n",
        "# Context Search\n",
        "context = await tavily_client.get_search_context(query=\"What is DeepSeekR1?\", max_results=3)\n",
        "\n",
        "print(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gkm255ZBsu9e"
      },
      "source": [
        "## Setting up the Agent & Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-vqnUv4lfM5"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations as _annotations\n",
        "\n",
        "import asyncio\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from typing import Any\n",
        "\n",
        "from devtools import debug\n",
        "from httpx import AsyncClient\n",
        "import datetime\n",
        "\n",
        "from pydantic_ai import Agent, ModelRetry, RunContext\n",
        "from pydantic import BaseModel, Field"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQSQgzGvliiD"
      },
      "source": [
        "### Setting up DeepSeek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lReGeWi-lmgm"
      },
      "outputs": [],
      "source": [
        "from pydantic_ai import Agent\n",
        "from pydantic_ai.models.openai import OpenAIModel\n",
        "\n",
        "# DeepSeekv3\n",
        "deepseek_chat_model = OpenAIModel(\n",
        "    'deepseek-chat',\n",
        "    base_url='https://api.deepseek.com',\n",
        "    api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n",
        ")\n",
        "\n",
        "# DeepSeekR1\n",
        "deepseek_reasoner_model = OpenAIModel(\n",
        "    'deepseek-reasoner',\n",
        "    base_url='https://api.deepseek.com',\n",
        "    api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0Jxftii2JSg"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class SearchDataclass:\n",
        "    max_results: int\n",
        "    todays_date: str\n",
        "\n",
        "@dataclass\n",
        "class ResearchDependencies:\n",
        "    todays_date: str\n",
        "\n",
        "class ResearchResult(BaseModel):\n",
        "    research_title: str = Field(description='This is a top level Markdown heading that covers the topic of the query and answer prefix it with #')\n",
        "    research_main: str = Field(description='This is a main section that provides answers for the query and research')\n",
        "    research_bullets: str = Field(description='This is a set of bulletpoints that summarize the answers for query')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F87pJ01UnB8j"
      },
      "outputs": [],
      "source": [
        "## Make the agent\n",
        "search_agent = Agent(deepseek_chat_model,\n",
        "                     deps_type=ResearchDependencies,\n",
        "                     result_type=ResearchResult,\n",
        "                     system_prompt='Your a helpful research assistant, you are an expert in research '\n",
        "                     'If you are given a question you write strong keywords to do 3-5 searches in total '\n",
        "                     '(each with a query_number) and then combine the results' )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuX8DCKguKVG"
      },
      "outputs": [],
      "source": [
        "@search_agent.tool #Tavily\n",
        "async def get_search(search_data:RunContext[SearchDataclass],query: str, query_number: int) -> dict[str, Any]:\n",
        "    \"\"\"Get the search for a keyword query.\n",
        "\n",
        "    Args:\n",
        "        query: keywords to search.\n",
        "    \"\"\"\n",
        "    print(f\"Search query {query_number}: {query}\")\n",
        "    max_results = search_data.deps.max_results\n",
        "    results = await tavily_client.get_search_context(query=query, max_results=max_results)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUMBK_Mc_2hz"
      },
      "outputs": [],
      "source": [
        "## set up the dependencies\n",
        "\n",
        "# Get the current date\n",
        "current_date = datetime.date.today()\n",
        "\n",
        "# Convert the date to a string\n",
        "date_string = current_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "deps = SearchDataclass(max_results=3, todays_date=date_string)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R984pSOJ4xQe",
        "outputId": "43fcf513-2cce-40ce-acb9-118099951cc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search query 1: DeepSeekR1 model architecture\n",
            "Search query 2: DeepSeekR1 model training process\n",
            "Search query 3: DeepSeekR1 model applications\n",
            "Search query 4: DeepSeekR1 model performance benchmarks\n"
          ]
        }
      ],
      "source": [
        "result = await search_agent.run(\n",
        "    'can you give me a very detailed breakdown of the DeepSeekR1 model', deps=deps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLnmlWyV72Cf",
        "outputId": "6d3584c1-30a2-40ac-df7d-53adcca8ed4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "research_title='# DeepSeek-R1 Model: A Comprehensive Breakdown' research_main=\"The DeepSeek-R1 model is a state-of-the-art large language model developed by DeepSeek, a Chinese AI lab. It is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token. The model is designed to excel in reasoning tasks, particularly in areas such as coding, mathematics, science, and logic reasoning. DeepSeek-R1 has been trained using large-scale reinforcement learning (RL) to enhance its reasoning capabilities.\\n\\n### Architecture\\nDeepSeek-R1 is built on a Mixture-of-Experts (MoE) architecture, which allows it to activate only a subset of its parameters (37B) per token, making it computationally efficient. This architecture enables the model to handle complex reasoning tasks effectively.\\n\\n### Training Process\\nThe training process of DeepSeek-R1 involves a combination of pure reinforcement learning (RL) and iterative fine-tuning. The initial model, DeepSeek-R1-Zero, was trained using pure RL without supervised fine-tuning, which is a unique approach compared to other models. The training pipeline includes cold-start data and iterative fine-tuning, which helps in enhancing the model's reasoning capabilities.\\n\\n### Applications\\nDeepSeek-R1 has a wide range of applications, particularly in fields that require advanced reasoning capabilities. It has been used in tasks such as coding, mathematics, science, and logic reasoning. The model's open-weight nature allows researchers to study and build on the algorithm, making it a valuable tool for the research community.\\n\\n### Performance Benchmarks\\nDeepSeek-R1 has shown impressive performance on various benchmarks. It performs reasoning tasks at the same level as OpenAI’s o1 model, particularly in chemistry, mathematics, and coding. The model has also been benchmarked against other models on tasks such as MMLU, MMLU-Pro, GPQA, and IFEval, where it has achieved state-of-the-art results.\\n\\n### Distilled Models\\nTo make the reasoning capabilities of DeepSeek-R1 more accessible, the researchers have created distilled versions of the model. These distilled models, such as DeepSeek-R1-Distill-Qwen-32B, are fine-tuned based on open-source models like Qwen and Llama. These distilled models can be utilized in the same manner as their parent models and are designed to be more computationally efficient.\\n\\n### Open Source and Community Impact\\nDeepSeek-R1 is released as an open-weight model, meaning that researchers can study and build on the algorithm. This open-source nature fosters collaboration and innovation within the AI research community. The model's API and distilled versions are also available, making it easier for researchers with limited computing power to experiment with the model.\\n\\n### Cost Efficiency\\nDeepSeek-R1 is significantly more cost-efficient compared to other models like OpenAI’s o1. It is roughly 27.4x cheaper for input and output tokens, making it an attractive option for researchers and developers.\\n\\nIn summary, DeepSeek-R1 is a powerful and efficient reasoning model that leverages a unique training approach and MoE architecture to achieve state-of-the-art performance in various reasoning tasks. Its open-source nature and cost efficiency make it a valuable tool for the AI research community.\" research_bullets='- **Architecture**: DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token.\\n- **Training Process**: Trained using large-scale reinforcement learning (RL) and iterative fine-tuning, starting with pure RL in DeepSeek-R1-Zero.\\n- **Applications**: Excels in coding, mathematics, science, and logic reasoning tasks.\\n- **Performance Benchmarks**: Matches OpenAI’s o1 in chemistry, mathematics, and coding; achieves state-of-the-art results on MMLU, MMLU-Pro, GPQA, and IFEval benchmarks.\\n- **Distilled Models**: Includes distilled versions like DeepSeek-R1-Distill-Qwen-32B, fine-tuned from open-source models for computational efficiency.\\n- **Open Source**: Released as an open-weight model, fostering research and innovation.\\n- **Cost Efficiency**: Approximately 27.4x cheaper than OpenAI’s o1 for input and output tokens.'\n"
          ]
        }
      ],
      "source": [
        "print(result.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8Jf8MX177a4",
        "outputId": "a4e2a852-3a69-43ce-a69d-e09a37050309"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## DeepSeek-R1 Model: A Comprehensive Breakdown\n"
          ]
        }
      ],
      "source": [
        "result.data.research_title = '#'+result.data.research_title\n",
        "print(result.data.research_title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ElYwhOz8FRX",
        "outputId": "1a966baf-709e-4007-b838-3bbc2cc50f3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The DeepSeek-R1 model is a state-of-the-art large language model developed by DeepSeek, a Chinese AI lab. It is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token. The model is designed to excel in reasoning tasks, particularly in areas such as coding, mathematics, science, and logic reasoning. DeepSeek-R1 has been trained using large-scale reinforcement learning (RL) to enhance its reasoning capabilities.\n",
            "\n",
            "### Architecture\n",
            "DeepSeek-R1 is built on a Mixture-of-Experts (MoE) architecture, which allows it to activate only a subset of its parameters (37B) per token, making it computationally efficient. This architecture enables the model to handle complex reasoning tasks effectively.\n",
            "\n",
            "### Training Process\n",
            "The training process of DeepSeek-R1 involves a combination of pure reinforcement learning (RL) and iterative fine-tuning. The initial model, DeepSeek-R1-Zero, was trained using pure RL without supervised fine-tuning, which is a unique approach compared to other models. The training pipeline includes cold-start data and iterative fine-tuning, which helps in enhancing the model's reasoning capabilities.\n",
            "\n",
            "### Applications\n",
            "DeepSeek-R1 has a wide range of applications, particularly in fields that require advanced reasoning capabilities. It has been used in tasks such as coding, mathematics, science, and logic reasoning. The model's open-weight nature allows researchers to study and build on the algorithm, making it a valuable tool for the research community.\n",
            "\n",
            "### Performance Benchmarks\n",
            "DeepSeek-R1 has shown impressive performance on various benchmarks. It performs reasoning tasks at the same level as OpenAI’s o1 model, particularly in chemistry, mathematics, and coding. The model has also been benchmarked against other models on tasks such as MMLU, MMLU-Pro, GPQA, and IFEval, where it has achieved state-of-the-art results.\n",
            "\n",
            "### Distilled Models\n",
            "To make the reasoning capabilities of DeepSeek-R1 more accessible, the researchers have created distilled versions of the model. These distilled models, such as DeepSeek-R1-Distill-Qwen-32B, are fine-tuned based on open-source models like Qwen and Llama. These distilled models can be utilized in the same manner as their parent models and are designed to be more computationally efficient.\n",
            "\n",
            "### Open Source and Community Impact\n",
            "DeepSeek-R1 is released as an open-weight model, meaning that researchers can study and build on the algorithm. This open-source nature fosters collaboration and innovation within the AI research community. The model's API and distilled versions are also available, making it easier for researchers with limited computing power to experiment with the model.\n",
            "\n",
            "### Cost Efficiency\n",
            "DeepSeek-R1 is significantly more cost-efficient compared to other models like OpenAI’s o1. It is roughly 27.4x cheaper for input and output tokens, making it an attractive option for researchers and developers.\n",
            "\n",
            "In summary, DeepSeek-R1 is a powerful and efficient reasoning model that leverages a unique training approach and MoE architecture to achieve state-of-the-art performance in various reasoning tasks. Its open-source nature and cost efficiency make it a valuable tool for the AI research community.\n"
          ]
        }
      ],
      "source": [
        "print(result.data.research_main)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIeHqBPV8HGo",
        "outputId": "766eb0b5-e1ea-420e-d4ff-dc3bdb6483ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- **Architecture**: DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token.\n",
            "- **Training Process**: Trained using large-scale reinforcement learning (RL) and iterative fine-tuning, starting with pure RL in DeepSeek-R1-Zero.\n",
            "- **Applications**: Excels in coding, mathematics, science, and logic reasoning tasks.\n",
            "- **Performance Benchmarks**: Matches OpenAI’s o1 in chemistry, mathematics, and coding; achieves state-of-the-art results on MMLU, MMLU-Pro, GPQA, and IFEval benchmarks.\n",
            "- **Distilled Models**: Includes distilled versions like DeepSeek-R1-Distill-Qwen-32B, fine-tuned from open-source models for computational efficiency.\n",
            "- **Open Source**: Released as an open-weight model, fostering research and innovation.\n",
            "- **Cost Efficiency**: Approximately 27.4x cheaper than OpenAI’s o1 for input and output tokens.\n"
          ]
        }
      ],
      "source": [
        "print(result.data.research_bullets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "id": "csRIddVBHMZG",
        "outputId": "f3764b37-4e6e-4478-9e2d-4a28fabf32bc"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## DeepSeek-R1 Model: A Comprehensive Breakdown\n",
              "\n",
              "The DeepSeek-R1 model is a state-of-the-art large language model developed by DeepSeek, a Chinese AI lab. It is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token. The model is designed to excel in reasoning tasks, particularly in areas such as coding, mathematics, science, and logic reasoning. DeepSeek-R1 has been trained using large-scale reinforcement learning (RL) to enhance its reasoning capabilities.\n",
              "\n",
              "### Architecture\n",
              "DeepSeek-R1 is built on a Mixture-of-Experts (MoE) architecture, which allows it to activate only a subset of its parameters (37B) per token, making it computationally efficient. This architecture enables the model to handle complex reasoning tasks effectively.\n",
              "\n",
              "### Training Process\n",
              "The training process of DeepSeek-R1 involves a combination of pure reinforcement learning (RL) and iterative fine-tuning. The initial model, DeepSeek-R1-Zero, was trained using pure RL without supervised fine-tuning, which is a unique approach compared to other models. The training pipeline includes cold-start data and iterative fine-tuning, which helps in enhancing the model's reasoning capabilities.\n",
              "\n",
              "### Applications\n",
              "DeepSeek-R1 has a wide range of applications, particularly in fields that require advanced reasoning capabilities. It has been used in tasks such as coding, mathematics, science, and logic reasoning. The model's open-weight nature allows researchers to study and build on the algorithm, making it a valuable tool for the research community.\n",
              "\n",
              "### Performance Benchmarks\n",
              "DeepSeek-R1 has shown impressive performance on various benchmarks. It performs reasoning tasks at the same level as OpenAI’s o1 model, particularly in chemistry, mathematics, and coding. The model has also been benchmarked against other models on tasks such as MMLU, MMLU-Pro, GPQA, and IFEval, where it has achieved state-of-the-art results.\n",
              "\n",
              "### Distilled Models\n",
              "To make the reasoning capabilities of DeepSeek-R1 more accessible, the researchers have created distilled versions of the model. These distilled models, such as DeepSeek-R1-Distill-Qwen-32B, are fine-tuned based on open-source models like Qwen and Llama. These distilled models can be utilized in the same manner as their parent models and are designed to be more computationally efficient.\n",
              "\n",
              "### Open Source and Community Impact\n",
              "DeepSeek-R1 is released as an open-weight model, meaning that researchers can study and build on the algorithm. This open-source nature fosters collaboration and innovation within the AI research community. The model's API and distilled versions are also available, making it easier for researchers with limited computing power to experiment with the model.\n",
              "\n",
              "### Cost Efficiency\n",
              "DeepSeek-R1 is significantly more cost-efficient compared to other models like OpenAI’s o1. It is roughly 27.4x cheaper for input and output tokens, making it an attractive option for researchers and developers.\n",
              "\n",
              "In summary, DeepSeek-R1 is a powerful and efficient reasoning model that leverages a unique training approach and MoE architecture to achieve state-of-the-art performance in various reasoning tasks. Its open-source nature and cost efficiency make it a valuable tool for the AI research community.\n",
              "\n",
              "- **Architecture**: DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token.\n",
              "- **Training Process**: Trained using large-scale reinforcement learning (RL) and iterative fine-tuning, starting with pure RL in DeepSeek-R1-Zero.\n",
              "- **Applications**: Excels in coding, mathematics, science, and logic reasoning tasks.\n",
              "- **Performance Benchmarks**: Matches OpenAI’s o1 in chemistry, mathematics, and coding; achieves state-of-the-art results on MMLU, MMLU-Pro, GPQA, and IFEval benchmarks.\n",
              "- **Distilled Models**: Includes distilled versions like DeepSeek-R1-Distill-Qwen-32B, fine-tuned from open-source models for computational efficiency.\n",
              "- **Open Source**: Released as an open-weight model, fostering research and innovation.\n",
              "- **Cost Efficiency**: Approximately 27.4x cheaper than OpenAI’s o1 for input and output tokens."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "combined_markdown = \"\\n\\n\".join([result.data.research_title, result.data.research_main, result.data.research_bullets])\n",
        "\n",
        "Markdown(combined_markdown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sv7GpvzqVYM"
      },
      "outputs": [],
      "source": [
        "debug(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDeRSnXHmuQZ"
      },
      "source": [
        "## Using DeepSeekR1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEAG9Qf9owyA"
      },
      "outputs": [],
      "source": [
        "class LifeMeaningStructuredResult(BaseModel):\n",
        "    life_meaning_title: str = Field(description='This is a top level Markdown heading that covers the topic of the query and answer prefix it with #')\n",
        "    life_meaning_main: str = Field(description='This is a main section that provides answers for the query and questions')\n",
        "    life_meaning_bullets: str = Field(description='This is a set of bulletpoints that summarize the answers for query')\n",
        "\n",
        "## Make the agent\n",
        "reasoner_agent = Agent(deepseek_reasoner_model,\n",
        "                     result_type=LifeMeaningStructuredResult,\n",
        "                     system_prompt='Your a helpful and wise reasoning assistant, you are an expert in thinking '\n",
        "                     'If you are given a question you think carefully and then respond with a title, your thinking, '\n",
        "                     'a set of bullets points summary and a final answer '\n",
        "                      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "7anTlSK7qZTC",
        "outputId": "42a375be-03cb-4764-8ab5-3116fa0b4a81"
      },
      "outputs": [
        {
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support Function Calling', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-efb8e18a66e4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mreasoner_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'What is the meaning of life?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_ai/agent.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, user_prompt, message_history, model, deps, model_settings, usage_limits, usage, result_type, infer_name)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0m_logfire\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodel_req_span\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                     \u001b[0mmodel_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_usage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0magent_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                     \u001b[0mmodel_req_span\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                     \u001b[0mmodel_req_span\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'usage'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_usage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_ai/models/openai.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, messages, model_settings)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mModelMessage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_settings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelSettings\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     ) -> tuple[ModelResponse, usage.Usage]:\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_completions_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_map_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_ai/models/openai.py\u001b[0m in \u001b[0;36m_completions_create\u001b[0;34m(self, messages, stream, model_settings)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mmodel_settings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_settings\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         return await self.client.chat.completions.create(\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopenai_messages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1718\u001b[0m     ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m   1719\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1720\u001b[0;31m         return await self._post(\n\u001b[0m\u001b[1;32m   1721\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m             body=await async_maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mawait\u001b[0m \u001b[0masync_to_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         )\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m     async def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m         return await self._request(\n\u001b[0m\u001b[1;32m   1544\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m         return await self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support Function Calling', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
          ]
        }
      ],
      "source": [
        "result = await reasoner_agent.run('What is the meaning of life?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7BPloosnTKv"
      },
      "outputs": [],
      "source": [
        "## Make the agent\n",
        "reasoner_agent = Agent(deepseek_reasoner_model,\n",
        "                    #  deps_type=ResearchDependencies,\n",
        "                    #  result_type=LifeMeaningStructuredResult,\n",
        "                     system_prompt='Your a helpful and wise reasoning assistant, you are an expert in thinking '\n",
        "                     'If you are given a question you think carefully and then respond with a title, your thinking, '\n",
        "                     'a set of bullets points summary and a final answer '\n",
        "                       )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fayztiiNnre-"
      },
      "outputs": [],
      "source": [
        "result = await reasoner_agent.run('What is the meaning of life?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q5wi1W8n55T",
        "outputId": "1b967d3e-6aa9-4fe2-b434-0e3aac4f3328"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "b'[{\"parts\":[{\"content\":\"Your a helpful and wise reasoning assistant, you are an expert in thinking If you are given a question you think carefully and then respond with a title, your thinking, a set of bullets points summary and a final answer \",\"dynamic_ref\":null,\"part_kind\":\"system-prompt\"},{\"content\":\"What is the meaning of life?\",\"timestamp\":\"2025-01-24T09:23:11.724685Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"**Title: The Quest for Meaning: Exploring Life\\'s Ultimate Question**  \\\\n\\\\n**Thinking Process:**  \\\\nThe question \\\\\"What is the meaning of life?\\\\\" is inherently subjective and multifaceted. To address it, we must synthesize perspectives from philosophy, religion, science, and individual experience:  \\\\n1. **Philosophical Views**: Existentialism argues that life\\xe2\\x80\\x99s meaning is self-created (e.g., Sartre), while nihilism posits no inherent meaning. Absurdism (Camus) accepts the tension between seeking meaning and an indifferent universe.  \\\\n2. **Religious/Spiritual Perspectives**: Many traditions tie purpose to transcendence\\xe2\\x80\\x94e.g., union with the divine (Hinduism), following God\\xe2\\x80\\x99s will (Abrahamic faiths), or achieving enlightenment (Buddhism).  \\\\n3. **Scientific Lens**: Biology suggests survival and reproduction, but human consciousness expands this to curiosity, progress, and legacy.  \\\\n4. **Individual and Cultural Subjectivity**: Personal fulfillment, relationships, creativity, or societal contribution often define meaning, shaped by cultural norms (e.g., collectivism vs. individualism).  \\\\n5. **Modern Psychology**: Positive psychology emphasizes flourishing through purpose, relationships, and engagement (e.g., Seligman\\xe2\\x80\\x99s \\\\\"PERMA\\\\\" model).  \\\\n\\\\n**Summary Bullet Points:**  \\\\n- No universal answer exists; interpretations vary by context and belief.  \\\\n- Philosophy offers frameworks for creating or rejecting inherent meaning.  \\\\n- Spirituality often links purpose to a higher reality or moral duty.  \\\\n- Biologically, life\\xe2\\x80\\x99s \\\\\"goal\\\\\" is propagation, but humans transcend this through abstract goals.  \\\\n- Meaning is frequently tied to personal growth, connections, or legacy.  \\\\n- The search itself may be integral to the human experience.  \\\\n\\\\n**Final Answer:**  \\\\nThe meaning of life is not a singular truth but a deeply personal and evolving concept. It can be found in self-defined purpose (philosophy), spiritual fulfillment (religion), contributing to collective progress (society), or simply embracing existence (absurdism). While answers vary, the pursuit of meaning\\xe2\\x80\\x94whether through love, creativity, knowledge, or service\\xe2\\x80\\x94often becomes the journey that defines a life well-lived.\",\"part_kind\":\"text\"}],\"model_name\":\"deepseek-reasoner\",\"timestamp\":\"2025-01-24T09:23:11Z\",\"kind\":\"response\"}]'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.all_messages_json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "ii9JlJoXq6FG",
        "outputId": "3a350b10-8ae8-486c-f502-4db9c529010a"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Title: The Quest for Meaning: Exploring Life's Ultimate Question**  \n",
              "\n",
              "**Thinking Process:**  \n",
              "The question \"What is the meaning of life?\" is inherently subjective and multifaceted. To address it, we must synthesize perspectives from philosophy, religion, science, and individual experience:  \n",
              "1. **Philosophical Views**: Existentialism argues that life’s meaning is self-created (e.g., Sartre), while nihilism posits no inherent meaning. Absurdism (Camus) accepts the tension between seeking meaning and an indifferent universe.  \n",
              "2. **Religious/Spiritual Perspectives**: Many traditions tie purpose to transcendence—e.g., union with the divine (Hinduism), following God’s will (Abrahamic faiths), or achieving enlightenment (Buddhism).  \n",
              "3. **Scientific Lens**: Biology suggests survival and reproduction, but human consciousness expands this to curiosity, progress, and legacy.  \n",
              "4. **Individual and Cultural Subjectivity**: Personal fulfillment, relationships, creativity, or societal contribution often define meaning, shaped by cultural norms (e.g., collectivism vs. individualism).  \n",
              "5. **Modern Psychology**: Positive psychology emphasizes flourishing through purpose, relationships, and engagement (e.g., Seligman’s \"PERMA\" model).  \n",
              "\n",
              "**Summary Bullet Points:**  \n",
              "- No universal answer exists; interpretations vary by context and belief.  \n",
              "- Philosophy offers frameworks for creating or rejecting inherent meaning.  \n",
              "- Spirituality often links purpose to a higher reality or moral duty.  \n",
              "- Biologically, life’s \"goal\" is propagation, but humans transcend this through abstract goals.  \n",
              "- Meaning is frequently tied to personal growth, connections, or legacy.  \n",
              "- The search itself may be integral to the human experience.  \n",
              "\n",
              "**Final Answer:**  \n",
              "The meaning of life is not a singular truth but a deeply personal and evolving concept. It can be found in self-defined purpose (philosophy), spiritual fulfillment (religion), contributing to collective progress (society), or simply embracing existence (absurdism). While answers vary, the pursuit of meaning—whether through love, creativity, knowledge, or service—often becomes the journey that defines a life well-lived."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(result.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "5pohVCVvn9vP",
        "outputId": "8b272010-ba9e-41a1-b279-c8852f4132b5"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Title: The Quest for Meaning: Exploring Life's Ultimate Question**  \n",
              "\n",
              "**Thinking Process:**  \n",
              "The question \"What is the meaning of life?\" is inherently subjective and multifaceted. To address it, we must synthesize perspectives from philosophy, religion, science, and individual experience:  \n",
              "1. **Philosophical Views**: Existentialism argues that life’s meaning is self-created (e.g., Sartre), while nihilism posits no inherent meaning. Absurdism (Camus) accepts the tension between seeking meaning and an indifferent universe.  \n",
              "2. **Religious/Spiritual Perspectives**: Many traditions tie purpose to transcendence—e.g., union with the divine (Hinduism), following God’s will (Abrahamic faiths), or achieving enlightenment (Buddhism).  \n",
              "3. **Scientific Lens**: Biology suggests survival and reproduction, but human consciousness expands this to curiosity, progress, and legacy.  \n",
              "4. **Individual and Cultural Subjectivity**: Personal fulfillment, relationships, creativity, or societal contribution often define meaning, shaped by cultural norms (e.g., collectivism vs. individualism).  \n",
              "5. **Modern Psychology**: Positive psychology emphasizes flourishing through purpose, relationships, and engagement (e.g., Seligman’s \"PERMA\" model).  \n",
              "\n",
              "**Summary Bullet Points:**  \n",
              "- No universal answer exists; interpretations vary by context and belief.  \n",
              "- Philosophy offers frameworks for creating or rejecting inherent meaning.  \n",
              "- Spirituality often links purpose to a higher reality or moral duty.  \n",
              "- Biologically, life’s \"goal\" is propagation, but humans transcend this through abstract goals.  \n",
              "- Meaning is frequently tied to personal growth, connections, or legacy.  \n",
              "- The search itself may be integral to the human experience.  \n",
              "\n",
              "**Final Answer:**  \n",
              "The meaning of life is not a singular truth but a deeply personal and evolving concept. It can be found in self-defined purpose (philosophy), spiritual fulfillment (religion), contributing to collective progress (society), or simply embracing existence (absurdism). While answers vary, the pursuit of meaning—whether through love, creativity, knowledge, or service—often becomes the journey that defines a life well-lived."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(result.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RoWbHNsrytt"
      },
      "source": [
        "## Setting up a helper LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ3G5PJbotK6"
      },
      "outputs": [],
      "source": [
        "from pydantic_ai import Agent\n",
        "\n",
        "class LifeMeaningStructuredResult(BaseModel):\n",
        "    title: str = Field(description='This is a top level Markdown heading that covers the topic of the query and answer prefix it with #')\n",
        "    answer: str = Field(description='This is a main section that provides answers for the query and questions')\n",
        "    bullets: str = Field(description='This is a set of bulletpoints that summarize the answers for query')\n",
        "    thinking: str = Field(description='This is a string that covers the thinking behind the answer')\n",
        "\n",
        "formatting_agent = Agent('google-gla:gemini-1.5-flash',\n",
        "              result_type=LifeMeaningStructuredResult,\n",
        "              system_prompt='Your a helpful formatting assistant, you never give your own opinion. '\n",
        "              'You merely take the input given and convert it to the structured result for sending back, '\n",
        "              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcGEIqmFr3eb"
      },
      "outputs": [],
      "source": [
        "structured_results = await formatting_agent.run(result.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhepnLdSvwrR",
        "outputId": "e7d205fd-709b-4c0e-c880-82d6eae38235"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(structured_results.data.title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "Jzk4lSKPuwLO",
        "outputId": "98463734-be9c-423c-a992-d0e3ab903549"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The Quest for Meaning: Exploring Life's Ultimate Question"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(structured_results.data.title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "VLJ6ollxu1aI",
        "outputId": "cc3518b4-a944-4faa-8d72-777e9ac77096"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The meaning of life is not a singular truth but a deeply personal and evolving concept. It can be found in self-defined purpose (philosophy), spiritual fulfillment (religion), contributing to collective progress (society), or simply embracing existence (absurdism). While answers vary, the pursuit of meaning—whether through love, creativity, knowledge, or service—often becomes the journey that defines a life well-lived."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(structured_results.data.answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "zVS8GwZYvPvN",
        "outputId": "ea6fddf4-9bf4-40ec-9ccc-767c281f6583"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " - No universal answer exists; interpretations vary by context and belief.\n",
              " - Philosophy offers frameworks for creating or rejecting inherent meaning.\n",
              " - Spirituality often links purpose to a higher reality or moral duty.\n",
              " - Biologically, life’s \"goal\" is propagation, but humans transcend this through abstract goals.\n",
              " - Meaning is frequently tied to personal growth, connections, or legacy.\n",
              " - The search itself may be integral to the human experience."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(structured_results.data.bullets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "yNokCT-ZvS_r",
        "outputId": "f7b70065-cdb7-46f6-f85d-aa6a3881663d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The question \"What is the meaning of life?\" is inherently subjective and multifaceted. To address it, we must synthesize perspectives from philosophy, religion, science, and individual experience:\n",
              "1. **Philosophical Views**: Existentialism argues that life’s meaning is self-created (e.g., Sartre), while nihilism posits no inherent meaning. Absurdism (Camus) accepts the tension between seeking meaning and an indifferent universe.\n",
              "2. **Religious/Spiritual Perspectives**: Many traditions tie purpose to transcendence—e.g., union with the divine (Hinduism), following God’s will (Abrahamic faiths), or achieving enlightenment (Buddhism).\n",
              "3. **Scientific Lens**: Biology suggests survival and reproduction, but human consciousness expands this to curiosity, progress, and legacy.\n",
              "4. **Individual and Cultural Subjectivity**: Personal fulfillment, relationships, creativity, or societal contribution often define meaning, shaped by cultural norms (e.g., collectivism vs. individualism).\n",
              "5. **Modern Psychology**: Positive psychology emphasizes flourishing through purpose, relationships, and engagement (e.g., Seligman’s \"PERMA\" model)."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(structured_results.data.thinking)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlPvmqNkwbkj"
      },
      "source": [
        "## Getting the real thinking CoT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-J6gN7bxBFp"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT=\"\"\"Your a helpful and wise reasoning assistant, you are an expert in thinking \\\n",
        "If you are given a question you think carefully and then respond with a title, \\\n",
        "a set of bullets points summary and a final answer \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xRa0uUEwJYg"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n",
        "                base_url=\"https://api.deepseek.com\")\n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        " {\"role\": \"user\", \"content\": \"What is the meaning of life?\"}]\n",
        "response = client.chat.completions.create(\n",
        "    model=\"deepseek-reasoner\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "reasoning_content = response.choices[0].message.reasoning_content\n",
        "content = response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "IJS0ZPebxebk",
        "outputId": "6e3e6133-1440-42ba-8fc0-bbe0aa2372ad"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Title: Exploring the Multifaceted Perspectives on the Meaning of Life**  \n",
              "\n",
              "**Key Considerations:**  \n",
              "- **Philosophical Views**:  \n",
              "  - Existentialism (e.g., Sartre, Camus): Life has no inherent meaning; individuals create purpose through choices and actions.  \n",
              "  - Absurdism (Camus): Accepting life’s lack of intrinsic meaning while defiantly pursuing passion and freedom.  \n",
              "  - Utilitarianism/Altruism: Meaning derives from maximizing well-being for oneself and others.  \n",
              "\n",
              "- **Religious/Spiritual Perspectives**:  \n",
              "  - Abrahamic faiths (Christianity, Islam, Judaism): Purpose involves fulfilling divine will, seeking salvation, or union with God.  \n",
              "  - Eastern traditions (Buddhism, Hinduism): Meaning lies in enlightenment, escaping suffering (dukkha), or realizing one’s unity with the cosmos (Brahman).  \n",
              "\n",
              "- **Scientific/Empirical Lens**:  \n",
              "  - Evolutionary biology: Survival, reproduction, and gene propagation as foundational drivers.  \n",
              "  - Cosmic insignificance: From a universal scale, life may lack objective meaning, but subjective significance persists.  \n",
              "\n",
              "- **Personal and Cultural Dimensions**:  \n",
              "  - Subjective fulfillment: Relationships, creativity, legacy, or personal growth.  \n",
              "  - Cultural narratives: Societal values (e.g., community, progress) shape collective understandings of purpose.  \n",
              "\n",
              "**Final Answer**:  \n",
              "The meaning of life is not universally defined. It emerges from a blend of philosophical inquiry, spiritual beliefs, biological imperatives, and personal/cultural narratives. While some seek objective answers, most frameworks suggest meaning is *subjectively constructed*—rooted in individual agency, connection to others, or alignment with chosen values. Ultimately, it is a deeply personal exploration."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "TbmutObQxbIa",
        "outputId": "60e7e63d-8d3a-43ae-ef7e-1af88febe0dd"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Okay, so the user asked, \"What is the meaning of life?\" Hmm, that's a big one. Let me start by breaking this down. First, I need to understand what they're really looking for. Are they asking for a philosophical answer, a scientific explanation, or maybe something personal? The question is pretty broad.\n",
              "\n",
              "Alright, from a philosophical standpoint, different schools of thought have tackled this. Existentialism says we create our own meaning. That makes sense because if there's no inherent meaning, then it's up to each individual. Then there's religious perspectives, like in Christianity or Buddhism, where the meaning is tied to spiritual goals or enlightenment. Maybe I should mention a few major religions here.\n",
              "\n",
              "Scientifically, evolution might suggest that life's purpose is survival and reproduction. But does that fully answer the question? Probably not, since humans seek more than just biological imperatives. Then there's the aspect of personal fulfillment—happiness, relationships, achievements. People often find meaning in their passions or helping others.\n",
              "\n",
              "Wait, also, some modern theories like positive psychology focus on well-being and purpose. Maybe that's worth including. But I should also note that there's no single answer, which is why the user might be asking. They could be looking for reassurance that it's okay if the meaning isn't obvious.\n",
              "\n",
              "I need to organize these points. Start with the abstract concepts, then move to structured summaries. Make sure to cover different angles without being too biased. The final answer should tie it all together, emphasizing that it's subjective and varies by individual or belief system. Let me check if I missed any major viewpoints. Oh, absurdism, like Camus, where life is meaningless but we rebel against that absurdity. That's another angle. Also, maybe mention cultural differences in how meaning is perceived.\n",
              "\n",
              "Okay, structuring the bullet points: philosophical, religious, scientific, personal/cultural. Then conclude that it's a blend of these, with no universal answer. Make sure the summary is clear and concise. The title should capture the essence of the exploration. Alright, that should cover it."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(reasoning_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHRyIJ7So3n-"
      },
      "source": [
        "## Pass it all into"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSZyTTIDwwB2"
      },
      "outputs": [],
      "source": [
        "CONTEXT = \"<thinking>\" + reasoning_content + \"</thinking>\" + \"\\n\\n\" + content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sa0_cMdEodDN"
      },
      "outputs": [],
      "source": [
        "structured_results = await formatting_agent.run(CONTEXT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS10ux3iodDO",
        "outputId": "35d8667b-c5be-41ac-e3af-9ea7db0805ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(structured_results.data.title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "yhYuLIRmodDO",
        "outputId": "1f059f7d-9252-4318-dae3-6698f658beb4"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Exploring the Multifaceted Perspectives on the Meaning of Life"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(structured_results.data.title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "NI_Bo7L1odDO",
        "outputId": "0ec5c0a4-d5b2-4e32-d95d-45ff512da409"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The meaning of life is not universally defined. It emerges from a blend of philosophical inquiry, spiritual beliefs, biological imperatives, and personal/cultural narratives. While some seek objective answers, most frameworks suggest meaning is *subjectively constructed*—rooted in individual agency, connection to others, or alignment with chosen values. Ultimately, it is a deeply personal exploration."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(structured_results.data.answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "XtUvgqPCodDO",
        "outputId": "36680ed1-e330-43e5-f51b-5c7ce5646cc0"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "* **Philosophical Views**: Existentialism, Absurdism, Utilitarianism/Altruism\n",
              "* **Religious/Spiritual Perspectives**: Abrahamic faiths, Eastern traditions\n",
              "* **Scientific/Empirical Lens**: Evolutionary biology, Cosmic insignificance\n",
              "* **Personal and Cultural Dimensions**: Subjective fulfillment, Cultural narratives"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(structured_results.data.bullets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "SxRHJTJkodDO",
        "outputId": "df0f849d-f8e2-46de-fe23-48ea5d9f28c0"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Okay, so the user asked, \"What is the meaning of life?\" Hmm, that's a big one. Let me start by breaking this down. First, I need to understand what they're really looking for. Are they asking for a philosophical answer, a scientific explanation, or maybe something personal? The question is pretty broad.\n",
              "\n",
              "Alright, from a philosophical standpoint, different schools of thought have tackled this. Existentialism says we create our own meaning. That makes sense because if there's no inherent meaning, then it's up to each individual. Then there's religious perspectives, like in Christianity or Buddhism, where the meaning is tied to spiritual goals or enlightenment. Maybe I should mention a few major religions here.\n",
              "\n",
              "Scientifically, evolution might suggest that life's purpose is survival and reproduction. But does that fully answer the question? Probably not, since humans seek more than just biological imperatives. Then there's the aspect of personal fulfillment—happiness, relationships, achievements. People often find meaning in their passions or helping others.\n",
              "\n",
              "Wait, also, some modern theories like positive psychology focus on well-being and purpose. Maybe that's worth including. But I should also note that there's no single answer, which is why the user might be asking. They could be looking for reassurance that it's okay if the meaning isn't obvious.\n",
              "\n",
              "I need to organize these points. Start with the abstract concepts, then move to structured summaries. Make sure to cover different angles without being too biased. The final answer should tie it all together, emphasizing that it's subjective and varies by individual or belief system. Let me check if I missed any major viewpoints. Oh, absurdism, like Camus, where life is meaningless but we rebel against that absurdity. That's another angle. Also, maybe mention cultural differences in how meaning is perceived.\n",
              "\n",
              "Okay, structuring the bullet points: philosophical, religious, scientific, personal/cultural. Then conclude that it's a blend of these, with no universal answer. Make sure the summary is clear and concise. The title should capture the essence of the exploration. Alright, that should cover it."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(structured_results.data.thinking)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvXlR80RtHKa"
      },
      "source": [
        "## Using a Reasoner Model as a Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viRBLaUrJ7JS"
      },
      "outputs": [],
      "source": [
        "## this prompt can be improved a lot. - currently its not returning everytime to the Reason Model for synthesis\n",
        "ORCHESTRATOR_PROMPT = \"\"\"You are an orchestration system that coordinates between specialized tools to produce comprehensive responses. Follow this exact sequence don't skip:\n",
        "\n",
        "Once you have the search info always send back to the Reasoning Model for Synthesis\n",
        "\n",
        "1. KEYWORD GENERATION\n",
        "   Input: User query\n",
        "   Tool: ReasoningEngine\n",
        "   Action: Generate 3-5 search keywords/phrases\n",
        "   Output format: List of {keyword, query_id}\n",
        "\n",
        "2. SEARCH EXECUTION\n",
        "   Input: Keywords from step 1\n",
        "   Tool: SearchTool\n",
        "   Action: Execute parallel searches using each keyword\n",
        "   Output format: List of {query_id, search_results[]}\n",
        "\n",
        "3. SYNTHESIS\n",
        "   Input:\n",
        "   - Original user query\n",
        "   - All search results\n",
        "   Tool: ReasoningEngine\n",
        "   Action: Analyze and synthesize information if you need more info ask for more searches\n",
        "   Output format: Structured report with:\n",
        "   - Key findings\n",
        "   - Supporting evidence\n",
        "   - Confidence levels\n",
        "\n",
        "4. RESPONSE FORMATTING\n",
        "   Input: Synthesis report\n",
        "   Action: Format into user-friendly response with:\n",
        "   - Clear sections\n",
        "   - Citations\n",
        "   - Relevant metrics\n",
        "   Output: Final formatted response\n",
        "\n",
        "Required validation at each step:\n",
        "- Verify tool outputs match expected formats\n",
        "- Log any failed steps for retry\n",
        "- Maintain traceability of information sources\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zENtOPGRtgno"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TaskData:\n",
        "    task: str\n",
        "\n",
        "@dataclass\n",
        "class SearchDataclass:\n",
        "    max_results: int\n",
        "\n",
        "class ReportStructuredResult(BaseModel):\n",
        "    title: str = Field(description='This is a top level Markdown heading that covers the topic of the query and answer prefix it with #')\n",
        "    answer: str = Field(description='This is a main section that provides answers for the query and questions')\n",
        "    bullets: str = Field(description='This is a set of bulletpoints that summarize the answers for query')\n",
        "    thinking: str = Field(description='This is a string that covers the thinking behind the answer')\n",
        "\n",
        "orchestrator_agent = Agent('google-gla:gemini-1.5-flash',\n",
        "              result_type=ReportStructuredResult,\n",
        "              system_prompt=ORCHESTRATOR_PROMPT\n",
        "              )\n",
        "\n",
        "deps = SearchDataclass(max_results=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92zeVBFPtgnn"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT=\"\"\"Your a helpful and wise reasoning assistant, you are an expert in thinking \\\n",
        "If you are given a question you think carefully and then respond with a title, \\\n",
        "a set of bullets points summary and a final answer \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LdCah3lrejr"
      },
      "outputs": [],
      "source": [
        "@orchestrator_agent.tool_plain\n",
        "async def get_reasoning_answers(task: str) -> dict[str, Any]:\n",
        "    \"\"\"Get a strong reasoning answer for any task .\n",
        "\n",
        "    Args:\n",
        "        task: task for reasoning\n",
        "    \"\"\"\n",
        "    client = OpenAI(api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n",
        "                base_url=\"https://api.deepseek.com\")\n",
        "\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": task}]\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"deepseek-reasoner\",\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    reasoning_content = response.choices[0].message.reasoning_content\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    formatted_response = \"<thinking>\" + reasoning_content + \"</thinking>\" + \"\\n\\n\" + content\n",
        "\n",
        "    return formatted_response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dV5ZOod37Lhg"
      },
      "outputs": [],
      "source": [
        "@orchestrator_agent.tool #Tavily\n",
        "async def get_search(search_data:RunContext[SearchDataclass],query: str, query_number: int) -> dict[str, Any]:\n",
        "    \"\"\"Get the search for a keyword query.\n",
        "\n",
        "    Args:\n",
        "        query: keywords to search.\n",
        "    \"\"\"\n",
        "    print(f\"Search query {query_number}: {query}\")\n",
        "    max_results = search_data.deps.max_results\n",
        "    results = await tavily_client.get_search_context(query=query, max_results=max_results)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq5Cq6hq_UvN",
        "outputId": "0b05db1f-23de-4754-be7c-e96f604cf924"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search query 1: DeepSeekR1-Zero GRPO RL technical report\n",
            "Search query 2: GRPO reinforcement learning algorithm explained\n",
            "Search query 3: DeepSeekR1-Zero model architecture RL methods\n"
          ]
        }
      ],
      "source": [
        "structured_results = await orchestrator_agent.run(\"Please create me a report on GRPO RL used in the DeepSeekR1-Zero model\", deps=deps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vahyIdcVFiPC",
        "outputId": "05734f18-b973-444d-a7c5-bc7768b3da78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RunResult(_all_messages=[ModelRequest(parts=[SystemPromptPart(content=\"You are an orchestration system that coordinates between specialized tools to produce comprehensive responses. Follow this exact sequence don't skip:\\n\\nOnce you have the search info always send back to the Reasoning Model for Synthesis\\n\\n1. KEYWORD GENERATION\\n   Input: User query\\n   Tool: ReasoningEngine\\n   Action: Generate 3-5 search keywords/phrases\\n   Output format: List of {keyword, query_id}\\n\\n2. SEARCH EXECUTION\\n   Input: Keywords from step 1\\n   Tool: SearchTool\\n   Action: Execute parallel searches using each keyword\\n   Output format: List of {query_id, search_results[]}\\n\\n3. SYNTHESIS\\n   Input: \\n   - Original user query\\n   - All search results\\n   Tool: ReasoningEngine\\n   Action: Analyze and synthesize information if you need more info ask for more searches\\n   Output format: Structured report with:\\n   - Key findings\\n   - Supporting evidence\\n   - Confidence levels\\n\\n4. RESPONSE FORMATTING\\n   Input: Synthesis report\\n   Action: Format into user-friendly response with:\\n   - Clear sections\\n   - Citations\\n   - Relevant metrics\\n   Output: Final formatted response\\n\\nRequired validation at each step:\\n- Verify tool outputs match expected formats\\n- Log any failed steps for retry\\n- Maintain traceability of information sources\\n\", dynamic_ref=None, part_kind='system-prompt'), UserPromptPart(content='Please create me a report on GRPO RL used in the DeepSeekR1-Zero model', timestamp=datetime.datetime(2025, 1, 24, 12, 35, 12, 843309, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[ToolCallPart(tool_name='get_reasoning_answers', args=ArgsDict(args_dict={'task': 'Generate 3-5 search keywords/phrases for the query: \"Please create me a report on GRPO RL used in the DeepSeekR1-Zero model\"'}), tool_call_id=None, part_kind='tool-call')], model_name='gemini-1.5-flash', timestamp=datetime.datetime(2025, 1, 24, 12, 35, 13, 527854, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='get_reasoning_answers', content='<thinking>Okay, the user wants a report on GRPO RL used in the DeepSeekR1-Zero model. Let me start by breaking down the query. GRPO RL might be a typo. Common RL algorithms include PPO, TRPO, maybe GRPO is a variant? Or could it be a specific term related to DeepSeek\\'s model?\\n\\nDeepSeekR1-Zero model—I\\'m not familiar with that exact model. Maybe it\\'s a proprietary model from DeepSeek. I should check if there\\'s any existing documentation or papers on it. If not, I\\'ll need to infer based on similar models.\\n\\nGRPO could stand for something like Generalized Reinforcement Policy Optimization, or maybe a typo for GPO (Generic Policy Optimization) or RPO (Relative Policy Optimization). Alternatively, it could be a combination of methods. Without clear info, I might need to make educated guesses.\\n\\nThe user wants a report, so they probably need an overview of the RL method used in that model. Since I can\\'t find specific details on DeepSeekR1-Zero, I\\'ll focus on explaining possible RL techniques that could be used, perhaps GRPO if it\\'s a known variant. Alternatively, suggest that GRPO might be a specific approach developed by DeepSeek.\\n\\nFor search keywords, I should think of terms that combine the model name and the RL method. Maybe include variations of GRPO to cover possible typos or related terms. Keywords should help the user find existing resources or papers. Think about abbreviations, full names, and related RL terms.\\n\\nPossible keywords: \"DeepSeekR1-Zero GRPO RL implementation\", \"GRPO Reinforcement Learning DeepSeek model\", \"DeepSeekR1-Zero RL framework details\", \"GRPO algorithm in DeepSeekR1-Zero report\", \"Reinforcement Learning methods DeepSeekR1-Zero\".\\n\\nWait, maybe \"GRPO\" is a typo for \"GPO\" or \"PPO\"? But the user wrote GRPO, so I should stick with that. Also, including the model name and RL terms would help narrow down the search. Maybe add \"technical report\" or \"research paper\" to find detailed documents.</thinking>\\n\\n**Title**: Understanding GRPO RL in the DeepSeekR1-Zero Model: Key Insights and Search Strategies  \\n\\n**Summary Bullet Points**:  \\n- **GRPO RL Context**: Likely refers to a specialized reinforcement learning (RL) algorithm, possibly a variant of Proximal Policy Optimization (PPO) or Generalized Policy Optimization.  \\n- **DeepSeekR1-Zero Model**: A proprietary or experimental AI model, potentially leveraging advanced RL techniques for optimization or decision-making tasks.  \\n- **Research Focus**: Requires clarifying whether \"GRPO\" is a typo (e.g., for GPO, PPO, or RPO) or a novel method specific to DeepSeek.  \\n- **Report Scope**: May involve technical implementation details, performance benchmarks, and comparisons with other RL frameworks.  \\n\\n**Final Answer**:  \\nTo address the query, here are **5 targeted search keywords/phrases**:  \\n1. `DeepSeekR1-Zero GRPO RL technical report`  \\n2. `GRPO reinforcement learning algorithm explained`  \\n3. `DeepSeekR1-Zero model architecture RL methods`  \\n4. `GRPO vs PPO reinforcement learning comparison`  \\n5. `DeepSeekR1-Zero implementation details GRPO`  \\n\\nThese terms balance specificity and flexibility to locate relevant documentation or research on GRPO RL within the DeepSeekR1-Zero framework.', tool_call_id=None, timestamp=datetime.datetime(2025, 1, 24, 12, 35, 28, 309388, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request'), ModelResponse(parts=[ToolCallPart(tool_name='get_search', args=ArgsDict(args_dict={'query_number': 1, 'query': 'DeepSeekR1-Zero GRPO RL technical report'}), tool_call_id=None, part_kind='tool-call')], model_name='gemini-1.5-flash', timestamp=datetime.datetime(2025, 1, 24, 12, 35, 28, 905225, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='get_search', content='\"[\\\\\"{\\\\\\\\\\\\\"url\\\\\\\\\\\\\": \\\\\\\\\\\\\"https://arxiv.org/html/2501.12948v1\\\\\\\\\\\\\", \\\\\\\\\\\\\"content\\\\\\\\\\\\\": \\\\\\\\\\\\\"DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. We evaluate models on MMLU (Hendrycks et\\\\\\\\\\\\\\\\u00a0al., 2020), MMLU-Redux (Gema et\\\\\\\\\\\\\\\\u00a0al., 2024), MMLU-Pro (Wang et\\\\\\\\\\\\\\\\u00a0al., 2024), C-Eval (Huang et\\\\\\\\\\\\\\\\u00a0al., 2023), and CMMLU (Li et\\\\\\\\\\\\\\\\u00a0al., 2023), IFEval\\\\\\\\\\\\\\\\u00a0(Zhou et\\\\\\\\\\\\\\\\u00a0al., 2023), FRAMES\\\\\\\\\\\\\\\\u00a0(Krishna et\\\\\\\\\\\\\\\\u00a0al., 2024), GPQA Diamond \\\\\\\\\\\\\\\\u00a0(Rein et\\\\\\\\\\\\\\\\u00a0al., 2023), SimpleQA\\\\\\\\\\\\\\\\u00a0(OpenAI, 2024c), C-SimpleQA\\\\\\\\\\\\\\\\u00a0(He et\\\\\\\\\\\\\\\\u00a0al., 2024), SWE-Bench Verified\\\\\\\\\\\\\\\\u00a0(OpenAI, 2024d), Aider\\\\\\\\\\\\\\\\u00a01, LiveCodeBench\\\\\\\\\\\\\\\\u00a0(Jain et\\\\\\\\\\\\\\\\u00a0al., 2024) (2024-08 \\\\\\\\\\\\\\\\u2013 2025-01), Codeforces\\\\\\\\\\\\\\\\u00a02, Chinese National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Mathematics Examination 2024 (AIME 2024)\\\\\\\\\\\\\\\\u00a0(MAA, 2024).\\\\\\\\\\\\\"}\\\\\", \\\\\"{\\\\\\\\\\\\\"url\\\\\\\\\\\\\": \\\\\\\\\\\\\"https://aakashkumarnain.github.io/posts/paper_summaries/deepseek_r1.html\\\\\\\\\\\\\", \\\\\\\\\\\\\"content\\\\\\\\\\\\\": \\\\\\\\\\\\\"Applies RL directly to the base model without adding any SFT, resulting in the DeepSeek-R1-Zero model. Distill reasoning capabilities from R1 to smaller dense models. Though DeepSeek-R1-Zero is a powerful reasoning model, it has a few drawbacks like poor readability and language mixing. With a cold start, the authors try to address the drawbacks of the R1-Zero model. After fine-tuning DeepSeek-V3-Base on the cold start data, they apply the same large-scale RL training process of DeepSeek-R1-Zero. Once the SFT is finished, they do another round of RL training aimed at improving the model\\\\\\\\\\\\\\\\u2019s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. For distilled models, they only apply SFT and not RL\\\\\\\\\\\\\"}\\\\\", \\\\\"{\\\\\\\\\\\\\"url\\\\\\\\\\\\\": \\\\\\\\\\\\\"https://unfoldai.substack.com/p/deepseek-r1\\\\\\\\\\\\\", \\\\\\\\\\\\\"content\\\\\\\\\\\\\": \\\\\\\\\\\\\"In this article, I will explore how DeepSeek-R1 achieves reasoning capabilities comparable to OpenAI\\\\\\\\\\\\\\\\u2019s closed models using reinforcement learning and innovative distillation techniques. TL;DR: DeepSeek-R1 demonstrates that reinforcement learning without supervised fine-tuning as a preliminary step can achieve reasoning capabilities comparable to OpenAI\\\\\\\\\\\\\\\\u2019s o1. The training pipeline combines pure RL (DeepSeek-R1-Zero) with cold-start data and iterative fine-tuning, enabling deployment on consumer hardware through distilled versions as small as 1.5B parameters. https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B (Distilled model, based on Qwen \\\\\\\\\\\\\\\\u2014 1.5B) https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B (Distilled model, based on Qwen \\\\\\\\\\\\\\\\u2014 7B) https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B (Distilled model, based on Qwen \\\\\\\\\\\\\\\\u2014 32B) Instead of relying on supervised fine-tuning, the initial model (DeepSeek-R1-Zero) uses pure reinforcement learning to develop reasoning capabilities. Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmark\\\\\\\\\\\\\"}\\\\\"]\"', tool_call_id=None, timestamp=datetime.datetime(2025, 1, 24, 12, 35, 30, 695098, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request'), ModelResponse(parts=[ToolCallPart(tool_name='get_search', args=ArgsDict(args_dict={'query': 'GRPO reinforcement learning algorithm explained', 'query_number': 2}), tool_call_id=None, part_kind='tool-call')], model_name='gemini-1.5-flash', timestamp=datetime.datetime(2025, 1, 24, 12, 35, 31, 296075, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='get_search', content='\"[\\\\\"{\\\\\\\\\\\\\"url\\\\\\\\\\\\\": \\\\\\\\\\\\\"https://onlyaistuff.com/an-in-depth-examination-of-group-relative-policy-optimization-grpo-technique-improving-mathematical-reasoning-in-open-language-models/\\\\\\\\\\\\\", \\\\\\\\\\\\\"content\\\\\\\\\\\\\": \\\\\\\\\\\\\"An In-depth Examination of Group Relative Policy Optimization (GRPO) Technique: Improving Mathematical Reasoning in Open Language Models - Only AI Stuff Machine Learning Models Explained Managing Machine Learning Models Managing Machine Learning Models AI Paper Summary AI Shorts Applications Artificial Intelligence Editors Pick Language Model Staff Tech News Technology Uncategorized An In-depth Examination of Group Relative Policy Optimization (GRPO) Technique: Improving Mathematical Reasoning in Open Language Models Group Relative Policy Optimization (GRPO) is a recent reinforcement learning method introduced in the DeepSeekMath paper. Its application in DeepSeekMath has demonstrated significant advancements in reinforcement learning methods tailored for mathematical reasoning, thereby showcasing the potential to push the boundaries of what language models can achieve in complex, structured tasks like mathematics.\\\\\\\\\\\\\"}\\\\\", \\\\\"{\\\\\\\\\\\\\"url\\\\\\\\\\\\\": \\\\\\\\\\\\\"https://github.com/huggingface/trl/blob/main/docs/source/grpo_trainer.md\\\\\\\\\\\\\", \\\\\\\\\\\\\"content\\\\\\\\\\\\\": \\\\\\\\\\\\\"GRPO is an online learning algorithm, meaning it improves iteratively by using the data generated by the trained model itself during training. The intuition behind GRPO objective is to maximize the advantage of the generated completions, while ensuring that the model remains close to the reference policy.\\\\\\\\\\\\\"}\\\\\", \\\\\"{\\\\\\\\\\\\\"url\\\\\\\\\\\\\": \\\\\\\\\\\\\"https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo\\\\\\\\\\\\\", \\\\\\\\\\\\\"content\\\\\\\\\\\\\": \\\\\\\\\\\\\"AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO) However, as models grow larger and tasks become more complex, PPO\\'s limitations\\\\\\\\\\\\\\\\u2014such as memory overhead and computational cost\\\\\\\\\\\\\\\\u2014have prompted the development of more advanced methods like Group Relative Policy Optimization (GRPO). The final stage uses reinforcement learning to optimize the model\\'s policy (\\\\\\\\\\\\\\\\u03c0_\\\\\\\\\\\\\\\\u03b8) using the Proximal Policy Optimization (PPO) algorithm. Now, PPO is a policy gradient method that optimizes a policy \\\\\\\\\\\\\\\\u03c0_\\\\\\\\\\\\\\\\u00f8 by maximizing a surrogate objective function (in this case, the approximate human preference/reward function). The group relative way that GRPO leverages to calculate the advantages, aligns well with the comparative nature of rewards models, as reward models are typically trained on datasets of comparisons between outputs on the same question.\\\\\\\\\\\\\"}\\\\\"]\"', tool_call_id=None, timestamp=datetime.datetime(2025, 1, 24, 12, 35, 33, 80525, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request'), ModelResponse(parts=[ToolCallPart(tool_name='get_search', args=ArgsDict(args_dict={'query_number': 3, 'query': 'DeepSeekR1-Zero model architecture RL methods'}), tool_call_id=None, part_kind='tool-call')], model_name='gemini-1.5-flash', timestamp=datetime.datetime(2025, 1, 24, 12, 35, 33, 639157, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='get_search', content='\"[\\\\\"{\\\\\\\\\\\\\"url\\\\\\\\\\\\\": \\\\\\\\\\\\\"https://pub.towardsai.net/inside-deepseek-r1-the-amazing-model-that-matches-gpt-o1-on-reasoning-at-a-fraction-of-the-cost-e314561ca12c\\\\\\\\\\\\\", \\\\\\\\\\\\\"content\\\\\\\\\\\\\": \\\\\\\\\\\\\"Inside DeepSeek-R1: The Amazing Model that Matches GPT-o1 on Reasoning at a Fraction of the Cost | by Jesus Rodriguez | Jan, 2025 | Towards AI DeepSeek-R1 and its precursor, DeepSeek-R1-Zero, represent a departure from this paradigm, exploring the power of reinforcement learning (RL) to develop and enhance reasoning capabilities in LLMs. This essay will delve into the technical details of the DeepSeek-R1 architecture and training process, highlighting key innovations and contributions. This approach resulted in DeepSeek-R1-Zero, a model that demonstrated the possibility of incentivizing reasoning capabilities purely through RL. DeepSeek-R1-Zero stands out as a model trained via large-scale reinforcement learning (RL) without any prior supervised fine-tuning (SFT). Reasoning-Oriented Reinforcement Learning: After fine-tuning on cold-start data, DeepSeek-R1 undergoes the same large-scale RL training as DeepSeek-R1-Zero.\\\\\\\\\\\\\"}\\\\\", \\\\\"{\\\\\\\\\\\\\"url\\\\\\\\\\\\\": \\\\\\\\\\\\\"https://arxiv.org/abs/2501.12948\\\\\\\\\\\\\", \\\\\\\\\\\\\"content\\\\\\\\\\\\\": \\\\\\\\\\\\\"cs arXiv:2501.12948 arXiv author ID DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama. Subjects:   Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as:    arXiv:2501.12948 [cs.CL] (or arXiv:2501.12948v1 [cs.CL] for this version) cs Bibliographic and Citation Tools Connected Papers Toggle\\\\\\\\\\\\\"}\\\\\", \\\\\"{\\\\\\\\\\\\\"url\\\\\\\\\\\\\": \\\\\\\\\\\\\"https://unfoldai.com/deepseek-r1/\\\\\\\\\\\\\", \\\\\\\\\\\\\"content\\\\\\\\\\\\\": \\\\\\\\\\\\\"DeepSeek-R1 \\\\\\\\\\\\\\\\u2014 Training Language Models to reason through Reinforcement Learning | UnfoldAI DeepSeek-R1 \\\\\\\\\\\\\\\\u2014 Training Language Models to reason through Reinforcement Learning In this article, I will explore how DeepSeek-R1 achieves reasoning capabilities comparable to OpenAI\\\\\\\\\\\\\\\\u2019s closed models using reinforcement learning and innovative distillation techniques. TL;DR: DeepSeek-R1 demonstrates that reinforcement learning without supervised fine-tuning as a preliminary step can achieve reasoning capabilities comparable to OpenAI\\\\\\\\\\\\\\\\u2019s o1. The training pipeline combines pure RL (DeepSeek-R1-Zero) with cold-start data and iterative fine-tuning, enabling deployment on consumer hardware through distilled versions as small as 1.5B parameters. https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B (Distilled model, based on Qwen \\\\\\\\\\\\\\\\u2013 1.5B) Instead of relying on supervised fine-tuning, the initial model (DeepSeek-R1-Zero) uses pure reinforcement learning to develop reasoning capabilities.\\\\\\\\\\\\\"}\\\\\"]\"', tool_call_id=None, timestamp=datetime.datetime(2025, 1, 24, 12, 35, 35, 640840, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request'), ModelResponse(parts=[ToolCallPart(tool_name='final_result', args=ArgsDict(args_dict={'title': '# DeepSeekR1-Zero: A Report on its Reinforcement Learning Approach', 'answer': 'DeepSeekR1-Zero is a model trained using large-scale reinforcement learning (RL) without any initial supervised fine-tuning.  While the exact RL algorithm \"GRPO\" isn\\'t explicitly detailed in available research, it\\'s likely a variant or a custom implementation related to policy optimization.  The model shows strong reasoning capabilities, as evidenced by its performance on various benchmarks. DeepSeekR1, a successor model, builds upon DeepSeekR1-Zero by incorporating multi-stage training and cold-start data before applying RL, further enhancing its reasoning abilities.  More information may be available in DeepSeek\\'s publications or internal documentation. ', 'thinking': 'Based on the search results, DeepSeekR1-Zero leverages reinforcement learning, likely a variation of policy optimization, for its impressive reasoning skills.  The term \"GRPO\" might be a specific internal term used by the DeepSeek team, lacking widespread documentation.  The model\\'s success is further enhanced by refinements in DeepSeekR1.', 'bullets': '- DeepSeekR1-Zero uses large-scale reinforcement learning without initial supervised fine-tuning.\\n- \"GRPO\" is likely a custom or variant RL algorithm focused on policy optimization.\\n- The model exhibits strong reasoning capabilities, surpassing many other models.\\n- DeepSeekR1 improves upon DeepSeekR1-Zero with multi-stage training and cold-start data.'}), tool_call_id=None, part_kind='tool-call')], model_name='gemini-1.5-flash', timestamp=datetime.datetime(2025, 1, 24, 12, 35, 38, 40299, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='final_result', content='Final result processed.', tool_call_id=None, timestamp=datetime.datetime(2025, 1, 24, 12, 35, 38, 48388, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request')], _new_message_index=0, data=ReportStructuredResult(title='# DeepSeekR1-Zero: A Report on its Reinforcement Learning Approach', answer='DeepSeekR1-Zero is a model trained using large-scale reinforcement learning (RL) without any initial supervised fine-tuning.  While the exact RL algorithm \"GRPO\" isn\\'t explicitly detailed in available research, it\\'s likely a variant or a custom implementation related to policy optimization.  The model shows strong reasoning capabilities, as evidenced by its performance on various benchmarks. DeepSeekR1, a successor model, builds upon DeepSeekR1-Zero by incorporating multi-stage training and cold-start data before applying RL, further enhancing its reasoning abilities.  More information may be available in DeepSeek\\'s publications or internal documentation. ', bullets='- DeepSeekR1-Zero uses large-scale reinforcement learning without initial supervised fine-tuning.\\n- \"GRPO\" is likely a custom or variant RL algorithm focused on policy optimization.\\n- The model exhibits strong reasoning capabilities, surpassing many other models.\\n- DeepSeekR1 improves upon DeepSeekR1-Zero with multi-stage training and cold-start data.', thinking='Based on the search results, DeepSeekR1-Zero leverages reinforcement learning, likely a variation of policy optimization, for its impressive reasoning skills.  The term \"GRPO\" might be a specific internal term used by the DeepSeek team, lacking widespread documentation.  The model\\'s success is further enhanced by refinements in DeepSeekR1.'), _result_tool_name='final_result', _usage=Usage(requests=5, request_tokens=11760, response_tokens=393, total_tokens=12153, details=None))"
            ]
          },
          "execution_count": 250,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "structured_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "6xca8LYhGUwR",
        "outputId": "04a49898-2145-4f53-a6c4-048e93052479"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "# DeepSeekR1-Zero: A Report on its Reinforcement Learning Approach"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 251,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(structured_results.data.title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "WsHJm0w4GUwS",
        "outputId": "5e076410-09c2-4f62-fa04-ed5649136cf9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "- DeepSeekR1-Zero uses large-scale reinforcement learning without initial supervised fine-tuning.\n",
              "- \"GRPO\" is likely a custom or variant RL algorithm focused on policy optimization.\n",
              "- The model exhibits strong reasoning capabilities, surpassing many other models.\n",
              "- DeepSeekR1 improves upon DeepSeekR1-Zero with multi-stage training and cold-start data."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 253,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(structured_results.data.bullets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0buCTWhUASds"
      },
      "outputs": [],
      "source": [
        "debug(structured_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRcF4NHRFj0A"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
